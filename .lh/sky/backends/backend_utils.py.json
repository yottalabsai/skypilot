{
    "sourceFile": "sky/backends/backend_utils.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1768543310494,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1768543310494,
            "name": "Commit-0",
            "content": "\"\"\"Util constants/functions for the backends.\"\"\"\nimport asyncio\nfrom datetime import datetime\nimport enum\nimport fnmatch\nimport hashlib\nimport os\nimport pathlib\nimport pprint\nimport queue as queue_lib\nimport re\nimport shlex\nimport subprocess\nimport sys\nimport tempfile\nimport threading\nimport time\nimport typing\nfrom typing import (Any, Callable, Dict, Iterator, List, Optional, Sequence,\n                    Set, Tuple, TypeVar, Union)\nimport uuid\n\nimport aiohttp\nfrom aiohttp import ClientTimeout\nfrom aiohttp import TCPConnector\nimport colorama\nfrom packaging import version\nfrom typing_extensions import Literal\n\nimport sky\nfrom sky import authentication as auth\nfrom sky import backends\nfrom sky import check as sky_check\nfrom sky import clouds\nfrom sky import exceptions\nfrom sky import global_user_state\nfrom sky import logs\nfrom sky import provision as provision_lib\nfrom sky import sky_logging\nfrom sky import skypilot_config\nfrom sky.adaptors import common as adaptors_common\nfrom sky.jobs import utils as managed_job_utils\nfrom sky.provision import common as provision_common\nfrom sky.provision import instance_setup\nfrom sky.provision.kubernetes import utils as kubernetes_utils\nfrom sky.serve import serve_utils\nfrom sky.server.requests import requests as requests_lib\nfrom sky.skylet import autostop_lib\nfrom sky.skylet import constants\nfrom sky.usage import usage_lib\nfrom sky.utils import auth_utils\nfrom sky.utils import cluster_utils\nfrom sky.utils import command_runner\nfrom sky.utils import common\nfrom sky.utils import common_utils\nfrom sky.utils import context as context_lib\nfrom sky.utils import context_utils\nfrom sky.utils import controller_utils\nfrom sky.utils import env_options\nfrom sky.utils import locks\nfrom sky.utils import registry\nfrom sky.utils import resources_utils\nfrom sky.utils import rich_utils\nfrom sky.utils import schemas\nfrom sky.utils import status_lib\nfrom sky.utils import subprocess_utils\nfrom sky.utils import tempstore\nfrom sky.utils import timeline\nfrom sky.utils import ux_utils\nfrom sky.utils import volume as volume_utils\nfrom sky.utils import yaml_utils\nfrom sky.utils.plugin_extensions import ExternalFailureSource\nfrom sky.workspaces import core as workspaces_core\n\nif typing.TYPE_CHECKING:\n    import grpc\n    import requests\n    from requests import adapters\n    from requests.packages.urllib3.util import retry as retry_lib\n    import rich.progress as rich_progress\n    import yaml\n\n    from sky import resources as resources_lib\n    from sky import task as task_lib\n    from sky.backends import cloud_vm_ray_backend\n    from sky.backends import local_docker_backend\nelse:\n    yaml = adaptors_common.LazyImport('yaml')\n    requests = adaptors_common.LazyImport('requests')\n    rich_progress = adaptors_common.LazyImport('rich.progress')\n    adapters = adaptors_common.LazyImport('requests.adapters')\n    retry_lib = adaptors_common.LazyImport(\n        'requests.packages.urllib3.util.retry')\n    # To avoid requiring grpcio to be installed on the client side.\n    grpc = adaptors_common.LazyImport('grpc')\n\nlogger = sky_logging.init_logger(__name__)\n\n# NOTE: keep in sync with the cluster template 'file_mounts'.\nSKY_REMOTE_APP_DIR = '~/.sky/sky_app'\n# Exclude subnet mask from IP address regex.\nIP_ADDR_REGEX = r'\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}(?!/\\d{1,2})\\b'\nSKY_REMOTE_PATH = '~/.sky/wheels'\n\n# Do not use /tmp because it gets cleared on VM restart.\n_SKY_REMOTE_FILE_MOUNTS_DIR = '~/.sky/file_mounts/'\n\n_LAUNCHED_HEAD_PATTERN = re.compile(r'(\\d+) ray[._]head[._]default')\n_LAUNCHED_LOCAL_WORKER_PATTERN = re.compile(r'(\\d+) node_')\n_LAUNCHED_WORKER_PATTERN = re.compile(r'(\\d+) ray[._]worker[._]default')\n_LAUNCHED_RESERVED_WORKER_PATTERN = re.compile(\n    r'(\\d+) ray[._]worker[._]reserved')\n# Intentionally not using prefix 'rf' for the string format because yapf have a\n# bug with python=3.6.\n# 10.133.0.5: ray.worker.default,\n_LAUNCHING_IP_PATTERN = re.compile(\n    r'({}): ray[._]worker[._](?:default|reserved)'.format(IP_ADDR_REGEX))\nSSH_CONNECTION_ERROR_PATTERN = re.compile(\n    r'^ssh:.*(timed out|connection refused)$', re.IGNORECASE)\n_SSH_CONNECTION_TIMED_OUT_PATTERN = re.compile(r'^ssh:.*timed out$',\n                                               re.IGNORECASE)\nK8S_PODS_NOT_FOUND_PATTERN = re.compile(r'.*(NotFound|pods .* not found).*',\n                                        re.IGNORECASE)\n_RAY_CLUSTER_NOT_FOUND_MESSAGE = 'Ray cluster is not found'\nWAIT_HEAD_NODE_IP_MAX_ATTEMPTS = 3\n\n# We check network connection by going through _TEST_IP_LIST. We may need to\n# check multiple IPs because some IPs may be blocked on certain networks.\n# Fixed IP addresses are used to avoid DNS lookup blocking the check, for\n# machine with no internet connection.\n# Refer to: https://stackoverflow.com/questions/3764291/how-can-i-see-if-theres-an-available-and-active-network-connection-in-python # pylint: disable=line-too-long\n_TEST_IP_LIST = ['https://8.8.8.8', 'https://1.1.1.1']\n\n# Allow each CPU thread take 2 tasks.\n# Note: This value cannot be too small, otherwise OOM issue may occur.\nDEFAULT_TASK_CPU_DEMAND = 0.5\n\nCLUSTER_STATUS_LOCK_TIMEOUT_SECONDS = 20\n\n# Time that must elapse since the last status check before we should re-check if\n# the cluster has been terminated or autostopped.\n_CLUSTER_STATUS_CACHE_DURATION_SECONDS = 2\n\nCLUSTER_FILE_MOUNTS_LOCK_TIMEOUT_SECONDS = 10\nWORKSPACE_LOCK_TIMEOUT_SECONDS = 10\nCLUSTER_TUNNEL_LOCK_TIMEOUT_SECONDS = 10.0\n\n# Remote dir that holds our runtime files.\n_REMOTE_RUNTIME_FILES_DIR = '~/.sky/.runtime_files'\n\n# The maximum size of a command line arguments is 128 KB, i.e. the command\n# executed with /bin/sh should be less than 128KB.\n# https://github.com/torvalds/linux/blob/master/include/uapi/linux/binfmts.h\n#\n# If a user have very long run or setup commands, the generated command may\n# exceed the limit, as we directly include scripts in job submission commands.\n# If the command is too long, we instead write it to a file, rsync and execute\n# it.\n#\n# We use 100KB as a threshold to be safe for other arguments that\n# might be added during ssh.\n_MAX_INLINE_SCRIPT_LENGTH = 100 * 1024\n\n_ENDPOINTS_RETRY_MESSAGE = ('If the cluster was recently started, '\n                            'please retry after a while.')\n\n# If a cluster is less than LAUNCH_DOUBLE_CHECK_WINDOW seconds old, and we don't\n# see any instances in the cloud, the instances might be in the process of\n# being created. We will wait LAUNCH_DOUBLE_CHECK_DELAY seconds and then double\n# check to make sure there are still no instances. LAUNCH_DOUBLE_CHECK_DELAY\n# should be set longer than the delay between (sending the create instance\n# request) and (the instances appearing on the cloud).\n# See https://github.com/skypilot-org/skypilot/issues/4431.\n_LAUNCH_DOUBLE_CHECK_WINDOW = 60\n_LAUNCH_DOUBLE_CHECK_DELAY = 1\n\n# Include the fields that will be used for generating tags that distinguishes\n# the cluster in ray, to avoid the stopped cluster being discarded due to\n# updates in the yaml template.\n# Some notes on the fields:\n# - 'provider' fields will be used for bootstrapping and insert more new items\n#   in 'node_config'.\n# - keeping the auth is not enough becuase the content of the key file will be\n#   used for calculating the hash.\n# TODO(zhwu): Keep in sync with the fields used in https://github.com/ray-project/ray/blob/e4ce38d001dbbe09cd21c497fedd03d692b2be3e/python/ray/autoscaler/_private/commands.py#L687-L701\n_RAY_YAML_KEYS_TO_RESTORE_FOR_BACK_COMPATIBILITY = {\n    'cluster_name', 'provider', 'auth', 'node_config', 'docker'\n}\n# For these keys, don't use the old yaml's version and instead use the new yaml's.\n#  - zone: The zone field of the old yaml may be '1a,1b,1c' (AWS) while the actual\n#    zone of the launched cluster is '1a'. If we restore, then on capacity errors\n#    it's possible to failover to 1b, which leaves a leaked instance in 1a. Here,\n#    we use the new yaml's zone field, which is guaranteed to be the existing zone\n#    '1a'.\n# - docker_login_config: The docker_login_config field of the old yaml may be\n#   outdated or wrong. Users may want to fix the login config if a cluster fails\n#   to launch due to the login config.\n# - UserData: The UserData field of the old yaml may be outdated, and we want to\n#   use the new yaml's UserData field, which contains the authorized key setup as\n#   well as the disabling of the auto-update with apt-get.\n# - firewall_rule: This is a newly added section for gcp in provider section.\n# - security_group: In #2485 we introduces the changed of security group, so we\n#   should take the latest security group name.\n_RAY_YAML_KEYS_TO_RESTORE_EXCEPTIONS = [\n    ('provider', 'availability_zone'),\n    # Clouds with new provisioner has docker_login_config in the\n    # docker field, instead of the provider field.\n    ('docker', 'docker_login_config'),\n    ('docker', 'run_options'),\n    # Other clouds\n    ('provider', 'docker_login_config'),\n    ('provider', 'firewall_rule'),\n    # TPU node launched before #2943 does not have the `provider.tpu_node` set,\n    # and our latest code need this field to be set to distinguish the node, so\n    # we need to take this field from the new yaml.\n    ('provider', 'tpu_node'),\n    ('provider', 'security_group', 'GroupName'),\n    ('available_node_types', 'ray.head.default', 'node_config',\n     'IamInstanceProfile'),\n    ('available_node_types', 'ray.head.default', 'node_config', 'UserData'),\n    ('available_node_types', 'ray.head.default', 'node_config',\n     'azure_arm_parameters', 'cloudInitSetupCommands'),\n    ('available_node_types', 'ray_head_default', 'node_config', 'pvc_spec'),\n    ('available_node_types', 'ray_head_default', 'node_config',\n     'deployment_spec'),\n]\n# These keys are expected to change when provisioning on an existing cluster,\n# but they don't actually represent a change that requires re-provisioning the\n# cluster.  If the cluster yaml is the same except for these keys, we can safely\n# skip reprovisioning. See _deterministic_cluster_yaml_hash.\n_RAY_YAML_KEYS_TO_REMOVE_FOR_HASH = [\n    # On first launch, availability_zones will include all possible zones. Once\n    # the cluster exists, it will only include the zone that the cluster is\n    # actually in.\n    ('provider', 'availability_zone'),\n]\n\n_ACK_MESSAGE = 'ack'\n_FORWARDING_FROM_MESSAGE = 'Forwarding from'\n\n\ndef is_command_length_over_limit(command: str) -> bool:\n    \"\"\"Check if the length of the command exceeds the limit.\n\n    We calculate the length of the command after quoting the command twice as\n    when it is executed by the CommandRunner, the command will be quoted twice\n    to ensure the correctness, which will add significant length to the command.\n    \"\"\"\n\n    quoted_length = len(shlex.quote(shlex.quote(command)))\n    return quoted_length > _MAX_INLINE_SCRIPT_LENGTH\n\n\ndef is_ip(s: str) -> bool:\n    \"\"\"Returns whether this string matches IP_ADDR_REGEX.\"\"\"\n    return len(re.findall(IP_ADDR_REGEX, s)) == 1\n\n\ndef _get_yaml_path_from_cluster_name(cluster_name: str,\n                                     prefix: str = constants.SKY_USER_FILE_PATH\n                                    ) -> str:\n    output_path = pathlib.Path(\n        prefix).expanduser().resolve() / f'{cluster_name}.yml'\n    os.makedirs(output_path.parents[0], exist_ok=True)\n    return str(output_path)\n\n\n# Add retry for the file mounts optimization, as the underlying cp command may\n# experience transient errors, #4758.\n@common_utils.retry\ndef _optimize_file_mounts(tmp_yaml_path: str) -> None:\n    \"\"\"Optimize file mounts in the given ray yaml file.\n\n    Runtime files handling:\n    List of runtime files to be uploaded to cluster:\n      - yaml config (for autostopping)\n      - wheel\n      - credentials\n    Format is {dst: src}.\n\n    Raises:\n        subprocess.CalledProcessError: If the file mounts are failed to be\n            copied.\n    \"\"\"\n    yaml_config = yaml_utils.read_yaml(tmp_yaml_path)\n\n    file_mounts = yaml_config.get('file_mounts', {})\n    # Remove the file mounts added by the newline.\n    if '' in file_mounts:\n        assert file_mounts[''] == '', file_mounts['']\n        file_mounts.pop('')\n\n    # Putting these in file_mounts hurts provisioning speed, as each file\n    # opens/closes an SSH connection.  Instead, we:\n    #  - cp them locally into a directory, each with a unique name to avoid\n    #    basename conflicts\n    #  - upload that directory as a file mount (1 connection)\n    #  - use a remote command to move all runtime files to their right places.\n\n    # Local tmp dir holding runtime files.\n    local_runtime_files_dir = tempstore.mkdtemp()\n    new_file_mounts = {_REMOTE_RUNTIME_FILES_DIR: local_runtime_files_dir}\n\n    # Generate local_src -> unique_name.\n    local_source_to_unique_name = {}\n    for local_src in file_mounts.values():\n        local_source_to_unique_name[local_src] = str(uuid.uuid4())\n\n    # (For remote) Build a command that copies runtime files to their right\n    # destinations.\n    # NOTE: we copy rather than move, because when launching >1 node, head node\n    # is fully set up first, and if moving then head node's files would already\n    # move out of _REMOTE_RUNTIME_FILES_DIR, which would cause setting up\n    # workers (from the head's files) to fail.  An alternative is softlink\n    # (then we need to make sure the usage of runtime files follow links).\n    commands = []\n    basenames = set()\n    for dst, src in file_mounts.items():\n        src_basename = local_source_to_unique_name[src]\n        dst_basename = os.path.basename(dst)\n        dst_parent_dir = os.path.dirname(dst)\n\n        # Validate by asserts here as these files are added by our backend.\n        # Our runtime files (wheel, yaml, credentials) do not have backslashes.\n        assert not src.endswith('/'), src\n        assert not dst.endswith('/'), dst\n        assert src_basename not in basenames, (\n            f'Duplicated src basename: {src_basename}; mounts: {file_mounts}')\n        basenames.add(src_basename)\n        # Our runtime files (wheel, yaml, credentials) are not relative paths.\n        assert dst_parent_dir, f'Found relative destination path: {dst}'\n\n        mkdir_parent = f'mkdir -p {dst_parent_dir}'\n        if os.path.isdir(os.path.expanduser(src)):\n            # Special case for directories. If the dst already exists as a\n            # folder, directly copy the folder will create a subfolder under\n            # the dst.\n            mkdir_parent = f'mkdir -p {dst}'\n            src_basename = f'{src_basename}/*'\n        mv = (f'cp -rf {_REMOTE_RUNTIME_FILES_DIR}/{src_basename} '\n              f'{dst_parent_dir}/{dst_basename}')\n        fragment = f'({mkdir_parent} && {mv})'\n        commands.append(fragment)\n    postprocess_runtime_files_command = '; '.join(commands)\n\n    setup_commands = yaml_config.get('setup_commands', [])\n    if setup_commands:\n        setup_commands[\n            0] = f'{postprocess_runtime_files_command}; {setup_commands[0]}'\n    else:\n        setup_commands = [postprocess_runtime_files_command]\n\n    yaml_config['file_mounts'] = new_file_mounts\n    yaml_config['setup_commands'] = setup_commands\n\n    # (For local) Copy all runtime files, including the just-written yaml, to\n    # local_runtime_files_dir/.\n    # < 0.3s to cp 6 clouds' credentials.\n    for local_src in file_mounts.values():\n        # cp <local_src> <local_runtime_files_dir>/<unique name of local_src>.\n        full_local_src = str(pathlib.Path(local_src).expanduser())\n        unique_name = local_source_to_unique_name[local_src]\n        # !r to add quotes for paths containing spaces.\n        subprocess.run(\n            f'cp -r {full_local_src!r} {local_runtime_files_dir}/{unique_name}',\n            shell=True,\n            check=True)\n\n    yaml_utils.dump_yaml(tmp_yaml_path, yaml_config)\n\n\ndef path_size_megabytes(path: str) -> int:\n    \"\"\"Returns the size of 'path' (directory or file) in megabytes.\n\n    Returns:\n        If successful: the size of 'path' in megabytes, rounded down. Otherwise,\n        -1.\n    \"\"\"\n    git_exclude_filter = ''\n    resolved_path = pathlib.Path(path).expanduser().resolve()\n    if (resolved_path / constants.SKY_IGNORE_FILE).exists():\n        rsync_filter = command_runner.RSYNC_FILTER_SKYIGNORE\n    else:\n        rsync_filter = command_runner.RSYNC_FILTER_GITIGNORE\n        if (resolved_path / command_runner.GIT_EXCLUDE).exists():\n            # Ensure file exists; otherwise, rsync will error out.\n            #\n            # We shlex.quote() because the path may contain spaces:\n            #   'my dir/.git/info/exclude'\n            # Without quoting rsync fails.\n            git_exclude_filter = command_runner.RSYNC_EXCLUDE_OPTION.format(\n                shlex.quote(str(resolved_path / command_runner.GIT_EXCLUDE)))\n    rsync_command = (f'rsync {command_runner.RSYNC_DISPLAY_OPTION} '\n                     f'{rsync_filter} '\n                     f'{git_exclude_filter} --dry-run {path!r}')\n    rsync_output = ''\n    try:\n        # rsync sometimes fails `--dry-run` for MacOS' rsync build, however this function is only used to display\n        # a warning message to the user if the size of a file/directory is too\n        # large, so we can safely ignore the error.\n        rsync_output = str(\n            subprocess.check_output(rsync_command,\n                                    shell=True,\n                                    stderr=subprocess.DEVNULL))\n    except subprocess.CalledProcessError:\n        logger.debug('Command failed, proceeding without estimating size: '\n                     f'{rsync_command}')\n        return -1\n    # 3.2.3:\n    #  total size is 250,957,728  speedup is 330.19 (DRY RUN)\n    # 2.6.9:\n    #  total size is 212627556  speedup is 2437.41\n    match = re.search(r'total size is ([\\d,]+)', rsync_output)\n    if match is not None:\n        try:\n            total_bytes = int(float(match.group(1).replace(',', '')))\n            return total_bytes // (1024**2)\n        except ValueError:\n            logger.debug('Failed to find \"total size\" in rsync output. Inspect '\n                         f'output of the following command: {rsync_command}')\n            pass  # Maybe different rsync versions have different output.\n    return -1\n\n\nclass FileMountHelper(object):\n    \"\"\"Helper for handling file mounts.\"\"\"\n\n    @classmethod\n    def wrap_file_mount(cls, path: str) -> str:\n        \"\"\"Prepends ~/<opaque dir>/ to a path to work around permission issues.\n\n        Examples:\n        /root/hello.txt -> ~/<opaque dir>/root/hello.txt\n        local.txt -> ~/<opaque dir>/local.txt\n\n        After the path is synced, we can later create a symlink to this wrapped\n        path from the original path, e.g., in the initialization_commands of the\n        ray autoscaler YAML.\n        \"\"\"\n        return os.path.join(_SKY_REMOTE_FILE_MOUNTS_DIR, path.lstrip('/'))\n\n    @classmethod\n    def make_safe_symlink_command(cls, *, source: str, target: str) -> str:\n        \"\"\"Returns a command that safely symlinks 'source' to 'target'.\n\n        All intermediate directories of 'source' will be owned by $(whoami),\n        excluding the root directory (/).\n\n        'source' must be an absolute path; both 'source' and 'target' must not\n        end with a slash (/).\n\n        This function is needed because a simple 'ln -s target source' may\n        fail: 'source' can have multiple levels (/a/b/c), its parent dirs may\n        or may not exist, can end with a slash, or may need sudo access, etc.\n\n        Cases of <target: local> file mounts and their behaviors:\n\n            /existing_dir: ~/local/dir\n              - error out saying this cannot be done as LHS already exists\n            /existing_file: ~/local/file\n              - error out saying this cannot be done as LHS already exists\n            /existing_symlink: ~/local/file\n              - overwrite the existing symlink; this is important because `sky\n                launch` can be run multiple times\n            Paths that start with ~/ and /tmp/ do not have the above\n            restrictions; they are delegated to rsync behaviors.\n        \"\"\"\n        assert os.path.isabs(source), source\n        assert not source.endswith('/') and not target.endswith('/'), (source,\n                                                                       target)\n        # Below, use sudo in case the symlink needs sudo access to create.\n        # Prepare to create the symlink:\n        #  1. make sure its dir(s) exist & are owned by $(whoami).\n        dir_of_symlink = os.path.dirname(source)\n        commands = [\n            # mkdir, then loop over '/a/b/c' as /a, /a/b, /a/b/c.  For each,\n            # chown $(whoami) on it so user can use these intermediate dirs\n            # (excluding /).\n            f'sudo mkdir -p {dir_of_symlink}',\n            # p: path so far\n            ('(p=\"\"; '\n             f'for w in $(echo {dir_of_symlink} | tr \"/\" \" \"); do '\n             'p=${p}/${w}; sudo chown $(whoami) $p; done)')\n        ]\n        #  2. remove any existing symlink (ln -f may throw 'cannot\n        #     overwrite directory', if the link exists and points to a\n        #     directory).\n        commands += [\n            # Error out if source is an existing, non-symlink directory/file.\n            f'((test -L {source} && sudo rm {source} &>/dev/null) || '\n            f'(test ! -e {source} || '\n            f'(echo \"!!! Failed mounting because path exists ({source})\"; '\n            'exit 1)))',\n        ]\n        commands += [\n            # Link.\n            f'sudo ln -s {target} {source}',\n            # chown.  -h to affect symlinks only.\n            f'sudo chown -h $(whoami) {source}',\n        ]\n        return ' && '.join(commands)\n\n\ndef _replace_yaml_dicts(\n        new_yaml: str, old_yaml: str, restore_key_names: Set[str],\n        restore_key_names_exceptions: Sequence[Tuple[str, ...]]) -> str:\n    \"\"\"Replaces 'new' with 'old' for all keys in restore_key_names.\n\n    The replacement will be applied recursively and only for the blocks\n    with the key in key_names, and have the same ancestors in both 'new'\n    and 'old' YAML tree.\n\n    The restore_key_names_exceptions is a list of key names that should not\n    be restored, i.e. those keys will be reset to the value in 'new' YAML\n    tree after the replacement.\n    \"\"\"\n\n    def _restore_block(new_block: Dict[str, Any], old_block: Dict[str, Any]):\n        for key, value in new_block.items():\n            if key in restore_key_names:\n                if key in old_block:\n                    new_block[key] = old_block[key]\n                else:\n                    del new_block[key]\n            elif isinstance(value, dict):\n                if key in old_block:\n                    _restore_block(value, old_block[key])\n\n    new_config = yaml_utils.safe_load(new_yaml)\n    old_config = yaml_utils.safe_load(old_yaml)\n    excluded_results = {}\n    # Find all key values excluded from restore\n    for exclude_restore_key_name_list in restore_key_names_exceptions:\n        excluded_result = new_config\n        found_excluded_key = True\n        for key in exclude_restore_key_name_list:\n            if (not isinstance(excluded_result, dict) or\n                    key not in excluded_result):\n                found_excluded_key = False\n                break\n            excluded_result = excluded_result[key]\n        if found_excluded_key:\n            excluded_results[exclude_restore_key_name_list] = excluded_result\n\n    # Restore from old config\n    _restore_block(new_config, old_config)\n\n    # Revert the changes for the excluded key values\n    for exclude_restore_key_name, value in excluded_results.items():\n        curr = new_config\n        for key in exclude_restore_key_name[:-1]:\n            curr = curr[key]\n        curr[exclude_restore_key_name[-1]] = value\n    return yaml_utils.dump_yaml_str(new_config)\n\n\ndef get_expirable_clouds(\n        enabled_clouds: Sequence[clouds.Cloud]) -> List[clouds.Cloud]:\n    \"\"\"Returns a list of clouds that use local credentials and whose credentials can expire.\n\n    This function checks each cloud in the provided sequence to determine if it uses local credentials\n    and if its credentials can expire. If both conditions are met, the cloud is added to the list of\n    expirable clouds.\n\n    Args:\n        enabled_clouds (Sequence[clouds.Cloud]): A sequence of cloud objects to check.\n\n    Returns:\n        list[clouds.Cloud]: A list of cloud objects that use local credentials and whose credentials can expire.\n    \"\"\"\n    expirable_clouds = []\n    local_credentials_value = schemas.RemoteIdentityOptions.LOCAL_CREDENTIALS.value\n    for cloud in enabled_clouds:\n        # Kubernetes config might have context-specific properties\n        if isinstance(cloud, clouds.Kubernetes):\n            # get all custom contexts\n            contexts = kubernetes_utils.get_custom_config_k8s_contexts()\n            # add remote_identity of each context if it exists\n            remote_identities: Optional[Union[str, List[Dict[str, str]]]] = None\n            for context in contexts:\n                context_remote_identity = skypilot_config.get_effective_region_config(\n                    cloud='kubernetes',\n                    region=context,\n                    keys=('remote_identity',),\n                    default_value=None)\n                if context_remote_identity is not None:\n                    if remote_identities is None:\n                        remote_identities = []\n                    if isinstance(context_remote_identity, str):\n                        assert isinstance(remote_identities, list)\n                        remote_identities.append(\n                            {context: context_remote_identity})\n                    elif isinstance(context_remote_identity, list):\n                        assert isinstance(remote_identities, list)\n                        remote_identities.extend(context_remote_identity)\n            # add global kubernetes remote identity if it exists, if not, add default\n            global_remote_identity = skypilot_config.get_effective_region_config(\n                cloud='kubernetes',\n                region=None,\n                keys=('remote_identity',),\n                default_value=None)\n            if global_remote_identity is not None:\n                if remote_identities is None:\n                    remote_identities = []\n                if isinstance(global_remote_identity, str):\n                    assert isinstance(remote_identities, list)\n                    remote_identities.append({'*': global_remote_identity})\n                elif isinstance(global_remote_identity, list):\n                    assert isinstance(remote_identities, list)\n                    remote_identities.extend(global_remote_identity)\n            if remote_identities is None:\n                remote_identities = schemas.get_default_remote_identity(\n                    str(cloud).lower())\n        else:\n            remote_identities = skypilot_config.get_effective_region_config(\n                cloud=str(cloud).lower(),\n                region=None,\n                keys=('remote_identity',),\n                default_value=None)\n            if remote_identities is None:\n                remote_identities = schemas.get_default_remote_identity(\n                    str(cloud).lower())\n\n        local_credential_expiring = cloud.can_credential_expire()\n        if isinstance(remote_identities, str):\n            if remote_identities == local_credentials_value and local_credential_expiring:\n                expirable_clouds.append(cloud)\n        elif isinstance(remote_identities, list):\n            for profile in remote_identities:\n                if list(profile.values(\n                ))[0] == local_credentials_value and local_credential_expiring:\n                    expirable_clouds.append(cloud)\n                    break\n    return expirable_clouds\n\n\ndef _get_volume_name(path: str, cluster_name_on_cloud: str) -> str:\n    path_hash = hashlib.md5(path.encode()).hexdigest()[:6]\n    return f'{cluster_name_on_cloud}-{path_hash}'\n\n\n# TODO: too many things happening here - leaky abstraction. Refactor.\n@timeline.event\ndef write_cluster_config(\n    to_provision: 'resources_lib.Resources',\n    num_nodes: int,\n    cluster_config_template: str,\n    cluster_name: str,\n    local_wheel_path: pathlib.Path,\n    wheel_hash: str,\n    region: clouds.Region,\n    zones: Optional[List[clouds.Zone]] = None,\n    dryrun: bool = False,\n    keep_launch_fields_in_existing_config: bool = True,\n    volume_mounts: Optional[List['volume_utils.VolumeMount']] = None,\n) -> Dict[str, str]:\n    \"\"\"Fills in cluster configuration templates and writes them out.\n\n    Returns:\n        Dict with the following keys:\n        - 'ray': Path to the generated Ray yaml config file\n        - 'cluster_name': Name of the cluster\n        - 'cluster_name_on_cloud': Name of the cluster as it appears in the\n          cloud provider\n        - 'config_hash': Hash of the cluster config and file mounts contents.\n          Can be missing if we unexpectedly failed to calculate the hash for\n          some reason. In that case we will continue without the optimization to\n          skip provisioning.\n\n    Raises:\n        exceptions.ResourcesUnavailableError: if the region/zones requested does\n            not appear in the catalog, or an ssh_proxy_command is specified but\n            not for the given region, or GPUs are requested in a Kubernetes\n            cluster but the cluster does not have nodes labeled with GPU types.\n        exceptions.InvalidCloudConfigs: if the user specifies some config for the\n            cloud that is not valid, e.g. remote_identity: SERVICE_ACCOUNT\n            for a cloud that does not support it, the caller should skip the\n            cloud in this case.\n    \"\"\"\n    # task.best_resources may not be equal to to_provision if the user\n    # is running a job with less resources than the cluster has.\n    cloud = to_provision.cloud\n    assert cloud is not None, to_provision\n\n    cluster_name_on_cloud = common_utils.make_cluster_name_on_cloud(\n        cluster_name, max_length=cloud.max_cluster_name_length())\n\n    # This can raise a ResourcesUnavailableError when:\n    #  * The region/zones requested does not appear in the catalog. It can be\n    #    triggered if the user changed the catalog file while there is a cluster\n    #    in the removed region/zone.\n    #  * GPUs are requested in a Kubernetes cluster but the cluster does not\n    #    have nodes labeled with GPU types.\n    #\n    # TODO(zhwu): We should change the exception type to a more specific one, as\n    # the ResourcesUnavailableError is overly used. Also, it would be better to\n    # move the check out of this function, i.e. the caller should be responsible\n    # for the validation.\n    # TODO(tian): Move more cloud agnostic vars to resources.py.\n    resources_vars = to_provision.make_deploy_variables(\n        resources_utils.ClusterName(\n            cluster_name,\n            cluster_name_on_cloud,\n        ), region, zones, num_nodes, dryrun, volume_mounts)\n    config_dict = {}\n\n    specific_reservations = set(\n        skypilot_config.get_effective_region_config(\n            cloud=str(to_provision.cloud).lower(),\n            region=to_provision.region,\n            keys=('specific_reservations',),\n            default_value=set()))\n\n    # Remote identity handling can have 4 cases:\n    # 1. LOCAL_CREDENTIALS (default for most clouds): Upload local credentials\n    # 2. SERVICE_ACCOUNT: SkyPilot creates and manages a service account\n    # 3. Custom service account: Use specified service account\n    # 4. NO_UPLOAD: Do not upload any credentials\n    #\n    # We need to upload credentials only if LOCAL_CREDENTIALS is specified. In\n    # other cases, we exclude the cloud from credential file uploads after\n    # running required checks.\n    assert cluster_name is not None\n    excluded_clouds: Set[clouds.Cloud] = set()\n    remote_identity_config = skypilot_config.get_effective_region_config(\n        cloud=str(cloud).lower(),\n        region=region.name,\n        keys=('remote_identity',),\n        default_value=None)\n    remote_identity = schemas.get_default_remote_identity(str(cloud).lower())\n    if isinstance(remote_identity_config, str):\n        remote_identity = remote_identity_config\n    if isinstance(remote_identity_config, list):\n        # Some clouds (e.g., AWS) support specifying multiple service accounts\n        # chosen based on the cluster name. Do the matching here to pick the\n        # correct one.\n        for profile in remote_identity_config:\n            if fnmatch.fnmatchcase(cluster_name, list(profile.keys())[0]):\n                remote_identity = list(profile.values())[0]\n                break\n    if remote_identity != schemas.RemoteIdentityOptions.LOCAL_CREDENTIALS.value:\n        # If LOCAL_CREDENTIALS is not specified, we add the cloud to the\n        # excluded_clouds set, but we must also check if the cloud supports\n        # service accounts.\n        if remote_identity == schemas.RemoteIdentityOptions.NO_UPLOAD.value:\n            # If NO_UPLOAD is specified, fall back to default remote identity\n            # for downstream logic but add it to excluded_clouds to skip\n            # credential file uploads.\n            remote_identity = schemas.get_default_remote_identity(\n                str(cloud).lower())\n        elif not cloud.supports_service_account_on_remote():\n            raise exceptions.InvalidCloudConfigs(\n                'remote_identity: SERVICE_ACCOUNT is specified in '\n                f'{skypilot_config.loaded_config_path!r} for {cloud}, but it '\n                'is not supported by this cloud. Remove the config or set: '\n                '`remote_identity: LOCAL_CREDENTIALS`.')\n        if isinstance(cloud, clouds.Kubernetes):\n            allowed_contexts = skypilot_config.get_workspace_cloud(\n                'kubernetes').get('allowed_contexts', None)\n            if allowed_contexts is None:\n                allowed_contexts = skypilot_config.get_effective_region_config(\n                    cloud='kubernetes',\n                    region=None,\n                    keys=('allowed_contexts',),\n                    default_value=None)\n            if allowed_contexts is None:\n                # Exclude both Kubernetes and SSH explicitly since:\n                # 1. isinstance(cloud, clouds.Kubernetes) matches both (SSH\n                #    inherits from Kubernetes)\n                # 2. Both share the same get_credential_file_mounts() which\n                #    returns the kubeconfig. So if we don't exclude both, the\n                #    unexcluded one will upload the kubeconfig.\n                # TODO(romilb): This is a workaround. The right long-term fix\n                # is to have SSH Node Pools use its own kubeconfig instead of\n                # sharing the global kubeconfig at ~/.kube/config. In the\n                # interim, SSH Node Pools' get_credential_file_mounts can filter\n                # contexts starting with ssh- and create a temp kubeconfig\n                # to upload.\n                excluded_clouds.add(clouds.Kubernetes())\n                excluded_clouds.add(clouds.SSH())\n        else:\n            excluded_clouds.add(cloud)\n\n    for cloud_str, cloud_obj in registry.CLOUD_REGISTRY.items():\n        remote_identity_config = skypilot_config.get_effective_region_config(\n            cloud=cloud_str.lower(),\n            region=region.name,\n            keys=('remote_identity',),\n            default_value=None)\n        if remote_identity_config:\n            if (remote_identity_config ==\n                    schemas.RemoteIdentityOptions.NO_UPLOAD.value):\n                excluded_clouds.add(cloud_obj)\n\n    credentials = sky_check.get_cloud_credential_file_mounts(excluded_clouds)\n\n    logging_agent = logs.get_logging_agent()\n    if logging_agent:\n        for k, v in logging_agent.get_credential_file_mounts().items():\n            assert k not in credentials, f'{k} already in credentials'\n            credentials[k] = v\n\n    private_key_path, _ = auth_utils.get_or_generate_keys()\n    auth_config = {'ssh_private_key': private_key_path}\n    region_name = resources_vars.get('region')\n\n    yaml_path = _get_yaml_path_from_cluster_name(cluster_name)\n\n    # Retrieve the ssh_proxy_command for the given cloud / region.\n    ssh_proxy_command_config = skypilot_config.get_effective_region_config(\n        cloud=str(cloud).lower(),\n        region=None,\n        keys=('ssh_proxy_command',),\n        default_value=None)\n    if (isinstance(ssh_proxy_command_config, str) or\n            ssh_proxy_command_config is None):\n        ssh_proxy_command = ssh_proxy_command_config\n    else:\n        # ssh_proxy_command_config: Dict[str, str], region_name -> command\n        # This type check is done by skypilot_config at config load time.\n\n        # There are two cases:\n        if keep_launch_fields_in_existing_config:\n            # (1) We're re-provisioning an existing cluster.\n            #\n            # We use None for ssh_proxy_command, which will be restored to the\n            # cluster's original value later by _replace_yaml_dicts().\n            ssh_proxy_command = None\n        else:\n            # (2) We're launching a new cluster.\n            #\n            # Resources.get_valid_regions_for_launchable() respects the keys (regions)\n            # in ssh_proxy_command in skypilot_config. So here we add an assert.\n            assert region_name in ssh_proxy_command_config, (\n                region_name, ssh_proxy_command_config)\n            ssh_proxy_command = ssh_proxy_command_config[region_name]\n\n    use_internal_ips = skypilot_config.get_effective_region_config(\n        cloud=str(cloud).lower(),\n        region=region.name,\n        keys=('use_internal_ips',),\n        default_value=False)\n    if isinstance(cloud, clouds.AWS):\n        # If the use_ssm flag is set to true, we use the ssm proxy command.\n        use_ssm = skypilot_config.get_effective_region_config(\n            cloud=str(cloud).lower(),\n            region=region.name,\n            keys=('use_ssm',),\n            default_value=None)\n\n        if use_ssm and ssh_proxy_command is not None:\n            raise exceptions.InvalidCloudConfigs(\n                'use_ssm is set to true, but ssh_proxy_command '\n                f'is already set to {ssh_proxy_command!r}. Please remove '\n                'ssh_proxy_command or set use_ssm to false.')\n\n        if use_internal_ips and ssh_proxy_command is None:\n            # Only if use_ssm is explicitly not set, we default to using SSM.\n            if use_ssm is None:\n                logger.warning(\n                    f'{colorama.Fore.YELLOW}'\n                    'use_internal_ips is set to true, '\n                    'but ssh_proxy_command is not set. Defaulting to '\n                    'using SSM. Specify ssh_proxy_command to use a different '\n                    'https://docs.skypilot.co/en/latest/reference/config.html#'\n                    f'aws.ssh_proxy_command.{colorama.Style.RESET_ALL}')\n                use_ssm = True\n\n        if use_ssm:\n            aws_profile = os.environ.get('AWS_PROFILE', None)\n            profile_str = f'--profile {aws_profile}' if aws_profile else ''\n            ip_address_filter = ('Name=private-ip-address,Values=%h'\n                                 if use_internal_ips else\n                                 'Name=ip-address,Values=%h')\n            get_instance_id_command = 'aws ec2 describe-instances ' + \\\n                f'--region {region_name} --filters {ip_address_filter} ' + \\\n                '--query \\\"Reservations[].Instances[].InstanceId\\\" ' + \\\n                f'{profile_str} --output text'\n            ssm_proxy_command = 'aws ssm start-session --target ' + \\\n                f'\\\"$({get_instance_id_command})\\\" ' + \\\n                f'--region {region_name} {profile_str} ' + \\\n                '--document-name AWS-StartSSHSession ' + \\\n                '--parameters portNumber=%p'\n            ssh_proxy_command = ssm_proxy_command\n            region_name = 'ssm-session'\n    logger.debug(f'Using ssh_proxy_command: {ssh_proxy_command!r}')\n\n    # User-supplied global instance tags from ~/.sky/config.yaml.\n    labels = skypilot_config.get_effective_region_config(\n        cloud=str(cloud).lower(),\n        region=region.name,\n        keys=('labels',),\n        default_value={})\n    # labels is a dict, which is guaranteed by the type check in\n    # schemas.py\n    assert isinstance(labels, dict), labels\n\n    # Get labels from resources and override from the labels to_provision.\n    if to_provision.labels:\n        labels.update(to_provision.labels)\n\n    # We disable conda auto-activation if the user has specified a docker image\n    # to use, which is likely to already have a conda environment activated.\n    conda_auto_activate = ('true' if to_provision.extract_docker_image() is None\n                           else 'false')\n    is_custom_docker = ('true' if to_provision.extract_docker_image()\n                        is not None else 'false')\n\n    # Check if the cluster name is a controller name.\n    is_remote_controller = False\n    controller = controller_utils.Controllers.from_name(\n        cluster_name, expect_exact_match=False)\n    if controller is not None:\n        is_remote_controller = True\n\n    # Here, if users specify the controller to be high availability, we will\n    # provision a high availability controller. Whether the cloud supports\n    # this feature has been checked by\n    # CloudImplementationFeatures.HIGH_AVAILABILITY_CONTROLLERS\n    high_availability_specified = controller_utils.high_availability_specified(\n        cluster_name)\n\n    volume_mount_vars = []\n    ephemeral_volume_mount_vars = []\n    if volume_mounts is not None:\n        for vol in volume_mounts:\n            if vol.is_ephemeral:\n                volume_name = _get_volume_name(vol.path, cluster_name_on_cloud)\n                vol.volume_name = volume_name\n                vol.volume_config.cloud = repr(cloud)\n                vol.volume_config.region = region.name\n                vol.volume_config.name = volume_name\n                ephemeral_volume_mount_vars.append(vol.to_yaml_config())\n            else:\n                volume_info = volume_utils.VolumeInfo(\n                    name=vol.volume_name,\n                    path=vol.path,\n                    volume_name_on_cloud=vol.volume_config.name_on_cloud,\n                    volume_id_on_cloud=vol.volume_config.id_on_cloud,\n                )\n                volume_mount_vars.append(volume_info)\n\n    runcmd = skypilot_config.get_effective_region_config(\n        cloud=str(to_provision.cloud).lower(),\n        region=to_provision.region,\n        keys=('post_provision_runcmd',),\n        default_value=None)\n\n    # Use a tmp file path to avoid incomplete YAML file being re-used in the\n    # future.\n    tmp_yaml_path = yaml_path + '.tmp'\n    common_utils.fill_template(\n        cluster_config_template,\n        dict(\n            resources_vars,\n            **{\n                'cluster_name_on_cloud': cluster_name_on_cloud,\n                'num_nodes': num_nodes,\n                'disk_size': to_provision.disk_size,\n                # If the current code is run by controller, propagate the real\n                # calling user which should've been passed in as the\n                # SKYPILOT_USER env var (see\n                # controller_utils.shared_controller_vars_to_fill().\n                'user': common_utils.get_cleaned_username(\n                    os.environ.get(constants.USER_ENV_VAR, '')),\n\n                # Networking configs\n                'use_internal_ips': skypilot_config.get_effective_region_config(\n                    cloud=str(cloud).lower(),\n                    region=region.name,\n                    keys=('use_internal_ips',),\n                    default_value=False),\n                'ssh_proxy_command': ssh_proxy_command,\n                'vpc_name': skypilot_config.get_effective_region_config(\n                    cloud=str(cloud).lower(),\n                    region=region.name,\n                    keys=('vpc_name',),\n                    default_value=None),\n                # User-supplied labels.\n                'labels': labels,\n                # User-supplied remote_identity\n                'remote_identity': remote_identity,\n                # The reservation pools that specified by the user. This is\n                # currently only used by AWS and GCP.\n                'specific_reservations': specific_reservations,\n\n                # Conda setup\n                # We should not use `.format`, as it contains '{}' as the bash\n                # syntax.\n                'conda_installation_commands':\n                    constants.CONDA_INSTALLATION_COMMANDS.replace(\n                        '{conda_auto_activate}',\n                        conda_auto_activate).replace('{is_custom_docker}',\n                                                     is_custom_docker),\n                # Currently only used by Slurm. For other clouds, it is\n                # already part of ray_skypilot_installation_commands\n                'setup_sky_dirs_commands': constants.SETUP_SKY_DIRS_COMMANDS,\n                'ray_skypilot_installation_commands':\n                    (constants.RAY_SKYPILOT_INSTALLATION_COMMANDS.replace(\n                        '{sky_wheel_hash}',\n                        wheel_hash).replace('{cloud}',\n                                            str(cloud).lower())),\n                'skypilot_wheel_installation_commands':\n                    constants.SKYPILOT_WHEEL_INSTALLATION_COMMANDS.replace(\n                        '{sky_wheel_hash}',\n                        wheel_hash).replace('{cloud}',\n                                            str(cloud).lower()),\n                'copy_skypilot_templates_commands':\n                    constants.COPY_SKYPILOT_TEMPLATES_COMMANDS,\n                # Port of Ray (GCS server).\n                # Ray's default port 6379 is conflicted with Redis.\n                'ray_port': constants.SKY_REMOTE_RAY_PORT,\n                'ray_dashboard_port': constants.SKY_REMOTE_RAY_DASHBOARD_PORT,\n                'ray_temp_dir': constants.SKY_REMOTE_RAY_TEMPDIR,\n                'dump_port_command': instance_setup.DUMP_RAY_PORTS,\n                # Sky-internal constants.\n                'sky_ray_cmd': constants.SKY_RAY_CMD,\n                # pip install needs to have python env activated to make sure\n                # installed packages are within the env path.\n                'sky_pip_cmd': f'{constants.SKY_PIP_CMD}',\n                # Activate the SkyPilot runtime environment when starting ray\n                # cluster, so that ray autoscaler can access cloud SDK and CLIs\n                # on remote\n                'sky_activate_python_env':\n                    constants.ACTIVATE_SKY_REMOTE_PYTHON_ENV,\n                'ray_version': constants.SKY_REMOTE_RAY_VERSION,\n                # Command for waiting ray cluster to be ready on head.\n                'ray_head_wait_initialized_command':\n                    instance_setup.RAY_HEAD_WAIT_INITIALIZED_COMMAND,\n\n                # Cloud credentials for cloud storage.\n                'credentials': credentials,\n                # Sky remote utils.\n                'sky_remote_path': SKY_REMOTE_PATH,\n                'sky_local_path': str(local_wheel_path),\n                # Add yaml file path to the template variables.\n                'sky_ray_yaml_remote_path':\n                    cluster_utils.SKY_CLUSTER_YAML_REMOTE_PATH,\n                'sky_ray_yaml_local_path': tmp_yaml_path,\n                'sky_version': str(version.parse(sky.__version__)),\n                'sky_wheel_hash': wheel_hash,\n                'ssh_max_sessions_config':\n                    constants.SET_SSH_MAX_SESSIONS_CONFIG_CMD,\n                # Authentication (optional).\n                **auth_config,\n\n                # Controller specific configs\n                'is_remote_controller': is_remote_controller,\n                'high_availability': high_availability_specified,\n\n                # Volume mounts\n                'volume_mounts': volume_mount_vars,\n                'ephemeral_volume_mounts': ephemeral_volume_mount_vars,\n\n                # runcmd to run before any of the SkyPilot runtime setup commands.\n                # This is currently only used by AWS and Kubernetes.\n                'runcmd': runcmd,\n            }),\n        output_path=tmp_yaml_path)\n    config_dict['cluster_name'] = cluster_name\n    config_dict['ray'] = yaml_path\n\n    # Add kubernetes config fields from ~/.sky/config\n    if isinstance(cloud, clouds.Kubernetes):\n        cluster_config_overrides = to_provision.cluster_config_overrides\n        with open(tmp_yaml_path, 'r', encoding='utf-8') as f:\n            tmp_yaml_str = f.read()\n        cluster_yaml_obj = yaml_utils.safe_load(tmp_yaml_str)\n        combined_yaml_obj = kubernetes_utils.combine_pod_config_fields_and_metadata(\n            cluster_yaml_obj,\n            cluster_config_overrides=cluster_config_overrides,\n            cloud=cloud,\n            context=region.name)\n        # Write the updated YAML back to the file\n        yaml_utils.dump_yaml(tmp_yaml_path, combined_yaml_obj)\n\n        pod_config: Dict[str, Any] = combined_yaml_obj['available_node_types'][\n            'ray_head_default']['node_config']\n        # Check pod spec only. For high availability controllers, we deploy pvc & deployment for the controller. Read kubernetes-ray.yml.j2 for more details.\n        pod_config.pop('deployment_spec', None)\n        pod_config.pop('pvc_spec', None)\n        valid, message = kubernetes_utils.check_pod_config(pod_config)\n        if not valid:\n            raise exceptions.InvalidCloudConfigs(\n                f'Invalid pod_config. Details: {message}')\n\n    if dryrun:\n        # If dryrun, return the unfinished tmp yaml path.\n        config_dict['ray'] = tmp_yaml_path\n        try:\n            config_dict['config_hash'] = _deterministic_cluster_yaml_hash(\n                tmp_yaml_path)\n        except Exception as e:  # pylint: disable=broad-except\n            logger.warning(f'Failed to calculate config_hash: {e}')\n            logger.debug('Full exception:', exc_info=e)\n        return config_dict\n    _add_auth_to_cluster_config(cloud, tmp_yaml_path)\n\n    # Restore the old yaml content for backward compatibility.\n    old_yaml_content = global_user_state.get_cluster_yaml_str(yaml_path)\n    if old_yaml_content is not None and keep_launch_fields_in_existing_config:\n        with open(tmp_yaml_path, 'r', encoding='utf-8') as f:\n            new_yaml_content = f.read()\n        restored_yaml_content = _replace_yaml_dicts(\n            new_yaml_content, old_yaml_content,\n            _RAY_YAML_KEYS_TO_RESTORE_FOR_BACK_COMPATIBILITY,\n            _RAY_YAML_KEYS_TO_RESTORE_EXCEPTIONS)\n        with open(tmp_yaml_path, 'w', encoding='utf-8') as f:\n            f.write(restored_yaml_content)\n\n    # Read the cluster_name_on_cloud from the restored yaml. This is a hack to\n    # make sure that launching on the same cluster across multiple users works\n    # correctly. See #8232.\n    yaml_config = yaml_utils.read_yaml(tmp_yaml_path)\n    config_dict['cluster_name_on_cloud'] = yaml_config['cluster_name']\n\n    # Make sure to do this before we optimize file mounts. Optimization is\n    # non-deterministic, but everything else before this point should be\n    # deterministic.\n    try:\n        config_dict['config_hash'] = _deterministic_cluster_yaml_hash(\n            tmp_yaml_path)\n    except Exception as e:  # pylint: disable=broad-except\n        logger.warning('Failed to calculate config_hash: '\n                       f'{common_utils.format_exception(e)}')\n        logger.debug('Full exception:', exc_info=e)\n\n    # Optimization: copy the contents of source files in file_mounts to a\n    # special dir, and upload that as the only file_mount instead. Delay\n    # calling this optimization until now, when all source files have been\n    # written and their contents finalized.\n    #\n    # Note that the ray yaml file will be copied into that special dir (i.e.,\n    # uploaded as part of the file_mounts), so the restore for backward\n    # compatibility should go before this call.\n    _optimize_file_mounts(tmp_yaml_path)\n\n    # commit the final yaml to the database\n    global_user_state.set_cluster_yaml(\n        cluster_name,\n        open(tmp_yaml_path, 'r', encoding='utf-8').read())\n\n    usage_lib.messages.usage.update_ray_yaml(tmp_yaml_path)\n\n    # Remove the tmp file.\n    if sky_logging.logging_enabled(logger, sky_logging.DEBUG):\n        debug_yaml_path = yaml_path + '.debug'\n        os.rename(tmp_yaml_path, debug_yaml_path)\n    else:\n        os.remove(tmp_yaml_path)\n\n    return config_dict\n\n\ndef _add_auth_to_cluster_config(cloud: clouds.Cloud, tmp_yaml_path: str):\n    \"\"\"Adds SSH key info to the cluster config.\n\n    This function's output removes comments included in the jinja2 template.\n    \"\"\"\n    config = yaml_utils.read_yaml(tmp_yaml_path)\n    # Check the availability of the cloud type.\n    if isinstance(\n            cloud,\n        (\n            clouds.AWS,\n            clouds.OCI,\n            clouds.SCP,\n            # TODO(jwj): Handle Slurm-specific auth logic\n            clouds.Slurm,\n            clouds.Vsphere,\n            clouds.Cudo,\n            clouds.Paperspace,\n            clouds.Azure,\n            clouds.DO,\n            clouds.Nebius,\n        )):\n        config = auth.configure_ssh_info(config)\n    elif isinstance(cloud, clouds.GCP):\n        config = auth.setup_gcp_authentication(config)\n    elif isinstance(cloud, clouds.Lambda):\n        config = auth.setup_lambda_authentication(config)\n    elif isinstance(cloud, clouds.Kubernetes):\n        config = auth.setup_kubernetes_authentication(config)\n    elif isinstance(cloud, clouds.IBM):\n        config = auth.setup_ibm_authentication(config)\n    elif isinstance(cloud, clouds.RunPod):\n        config = auth.setup_runpod_authentication(config)\n    elif isinstance(cloud, clouds.Vast):\n        config = auth.setup_vast_authentication(config)\n    elif isinstance(cloud, clouds.Fluidstack):\n        config = auth.setup_fluidstack_authentication(config)\n    elif isinstance(cloud, clouds.Hyperbolic):\n        config = auth.setup_hyperbolic_authentication(config)\n    elif isinstance(cloud, clouds.Shadeform):\n        config = auth.setup_shadeform_authentication(config)\n    elif isinstance(cloud, clouds.PrimeIntellect):\n        config = auth.setup_primeintellect_authentication(config)\n    elif isinstance(cloud, clouds.Seeweb):\n        config = auth.setup_seeweb_authentication(config)\n    elif isinstance(cloud, clouds.Yotta):\n        config = auth.setup_yotta_authentication(config)\n    else:\n        assert False, cloud\n    yaml_utils.dump_yaml(tmp_yaml_path, config)\n\n\ndef get_timestamp_from_run_timestamp(run_timestamp: str) -> float:\n    return datetime.strptime(\n        run_timestamp.partition('-')[2], '%Y-%m-%d-%H-%M-%S-%f').timestamp()\n\n\ndef _count_healthy_nodes_from_ray(output: str,\n                                  is_local_cloud: bool = False\n                                 ) -> Tuple[int, int]:\n    \"\"\"Count the number of healthy nodes from the output of `ray status`.\"\"\"\n\n    def get_ready_nodes_counts(pattern, output):\n        result = pattern.findall(output)\n        if not result:\n            return 0\n        assert len(result) == 1, result\n        return int(result[0])\n\n    # Check if the ray cluster is started with ray autoscaler. In new\n    # provisioner (#1702) and local mode, we started the ray cluster without ray\n    # autoscaler.\n    # If ray cluster is started with ray autoscaler, the output will be:\n    #  1 ray.head.default\n    #  ...\n    # TODO(zhwu): once we deprecate the old provisioner, we can remove this\n    # check.\n    ray_autoscaler_head = get_ready_nodes_counts(_LAUNCHED_HEAD_PATTERN, output)\n    is_local_ray_cluster = ray_autoscaler_head == 0\n\n    if is_local_ray_cluster or is_local_cloud:\n        # Ray cluster is launched with new provisioner\n        # For new provisioner and local mode, the output will be:\n        #  1 node_xxxx\n        #  1 node_xxxx\n        ready_head = 0\n        ready_workers = _LAUNCHED_LOCAL_WORKER_PATTERN.findall(output)\n        ready_workers = len(ready_workers)\n        if is_local_ray_cluster:\n            ready_head = 1\n            ready_workers -= 1\n        return ready_head, ready_workers\n\n    # Count number of nodes by parsing the output of `ray status`. The output\n    # looks like:\n    #   1 ray.head.default\n    #   2 ray.worker.default\n    ready_head = ray_autoscaler_head\n    ready_workers = get_ready_nodes_counts(_LAUNCHED_WORKER_PATTERN, output)\n    ready_reserved_workers = get_ready_nodes_counts(\n        _LAUNCHED_RESERVED_WORKER_PATTERN, output)\n    ready_workers += ready_reserved_workers\n    assert ready_head <= 1, f'#head node should be <=1 (Got {ready_head}).'\n    return ready_head, ready_workers\n\n\n@timeline.event\ndef _deterministic_cluster_yaml_hash(tmp_yaml_path: str) -> str:\n    \"\"\"Hash the cluster yaml and contents of file mounts to a unique string.\n\n    Two invocations of this function should return the same string if and only\n    if the contents of the yaml are the same and the file contents of all the\n    file_mounts specified in the yaml are the same.\n\n    Limitations:\n    - This function can be expensive if the file mounts are large. (E.g. a few\n      seconds for ~1GB.) This should be okay since we expect that the\n      file_mounts in the cluster yaml (the wheel and cloud credentials) will be\n      small.\n    - Symbolic links are not explicitly handled. Some symbolic link changes may\n      not be detected.\n\n    Implementation: We create a byte sequence that captures the state of the\n    yaml file and all the files in the file mounts, then hash the byte sequence.\n\n    The format of the byte sequence is:\n    32 bytes - sha256 hash of the yaml\n    for each file mount:\n      file mount remote destination (UTF-8), \\0\n      if the file mount source is a file:\n        'file' encoded to UTF-8\n        32 byte sha256 hash of the file contents\n      if the file mount source is a directory:\n        'dir' encoded to UTF-8\n        for each directory and subdirectory withinin the file mount (starting from\n            the root and descending recursively):\n          name of the directory (UTF-8), \\0\n          name of each subdirectory within the directory (UTF-8) terminated by \\0\n          \\0\n          for each file in the directory:\n            name of the file (UTF-8), \\0\n            32 bytes - sha256 hash of the file contents\n          \\0\n      if the file mount source is something else or does not exist, nothing\n      \\0\\0\n\n    Rather than constructing the whole byte sequence, which may be quite large,\n    we construct it incrementally by using hash.update() to add new bytes.\n    \"\"\"\n    # Load the yaml contents so that we can directly remove keys.\n    yaml_config = yaml_utils.read_yaml(tmp_yaml_path)\n    for key_list in _RAY_YAML_KEYS_TO_REMOVE_FOR_HASH:\n        dict_to_remove_from = yaml_config\n        found_key = True\n        for key in key_list[:-1]:\n            if (not isinstance(dict_to_remove_from, dict) or\n                    key not in dict_to_remove_from):\n                found_key = False\n                break\n            dict_to_remove_from = dict_to_remove_from[key]\n        if found_key and key_list[-1] in dict_to_remove_from:\n            dict_to_remove_from.pop(key_list[-1])\n\n    def _hash_file(path: str) -> bytes:\n        return common_utils.hash_file(path, 'sha256').digest()\n\n    config_hash = hashlib.sha256()\n\n    yaml_hash = hashlib.sha256(\n        yaml_utils.dump_yaml_str(yaml_config).encode('utf-8'))\n    config_hash.update(yaml_hash.digest())\n\n    file_mounts = yaml_config.get('file_mounts', {})\n    # Remove the file mounts added by the newline.\n    if '' in file_mounts:\n        assert file_mounts[''] == '', file_mounts['']\n        file_mounts.pop('')\n\n    for dst, src in sorted(file_mounts.items()):\n        if src == tmp_yaml_path:\n            # Skip the yaml file itself. We have already hashed a modified\n            # version of it. The file may include fields we don't want to hash.\n            continue\n\n        expanded_src = os.path.expanduser(src)\n        config_hash.update(dst.encode('utf-8') + b'\\0')\n\n        # If the file mount source is a symlink, this should be true. In that\n        # case we hash the contents of the symlink destination.\n        if os.path.isfile(expanded_src):\n            config_hash.update('file'.encode('utf-8'))\n            config_hash.update(_hash_file(expanded_src))\n\n        # This can also be a symlink to a directory. os.walk will treat it as a\n        # normal directory and list the contents of the symlink destination.\n        elif os.path.isdir(expanded_src):\n            config_hash.update('dir'.encode('utf-8'))\n\n            # Aside from expanded_src, os.walk will list symlinks to directories\n            # but will not recurse into them.\n            for (dirpath, dirnames, filenames) in os.walk(expanded_src):\n                config_hash.update(dirpath.encode('utf-8') + b'\\0')\n\n                # Note: inplace sort will also affect the traversal order of\n                # os.walk. We need it so that the os.walk order is\n                # deterministic.\n                dirnames.sort()\n                # This includes symlinks to directories. os.walk will recurse\n                # into all the directories but not the symlinks. We don't hash\n                # the link destination, so if a symlink to a directory changes,\n                # we won't notice.\n                for dirname in dirnames:\n                    config_hash.update(dirname.encode('utf-8') + b'\\0')\n                config_hash.update(b'\\0')\n\n                filenames.sort()\n                # This includes symlinks to files. We could hash the symlink\n                # destination itself but instead just hash the destination\n                # contents.\n                for filename in filenames:\n                    config_hash.update(filename.encode('utf-8') + b'\\0')\n                    config_hash.update(\n                        _hash_file(os.path.join(dirpath, filename)))\n                config_hash.update(b'\\0')\n\n        else:\n            logger.debug(\n                f'Unexpected file_mount that is not a file or dir: {src}')\n\n        config_hash.update(b'\\0\\0')\n\n    return config_hash.hexdigest()\n\n\ndef get_docker_user(ip: str, cluster_config_file: str) -> str:\n    \"\"\"Find docker container username.\"\"\"\n    ssh_credentials = ssh_credential_from_yaml(cluster_config_file)\n    runner = command_runner.SSHCommandRunner(node=(ip, 22), **ssh_credentials)\n    container_name = constants.DEFAULT_DOCKER_CONTAINER_NAME\n    whoami_returncode, whoami_stdout, whoami_stderr = runner.run(\n        f'sudo docker exec {container_name} whoami',\n        stream_logs=False,\n        require_outputs=True)\n    assert whoami_returncode == 0, (\n        f'Failed to get docker container user. Return '\n        f'code: {whoami_returncode}, Error: {whoami_stderr}')\n    docker_user = whoami_stdout.strip()\n    logger.debug(f'Docker container user: {docker_user}')\n    return docker_user\n\n\n@timeline.event\ndef wait_until_ray_cluster_ready(\n    cluster_config_file: str,\n    num_nodes: int,\n    log_path: str,\n    is_local_cloud: bool = False,\n    nodes_launching_progress_timeout: Optional[int] = None,\n) -> Tuple[bool, Optional[str]]:\n    \"\"\"Wait until the ray cluster is set up on VMs or in containers.\n\n    Returns:  whether the entire ray cluster is ready, and docker username\n    if launched with docker.\n    \"\"\"\n    # Manually fetching head ip instead of using `ray exec` to avoid the bug\n    # that `ray exec` fails to connect to the head node after some workers\n    # launched especially for Azure.\n    try:\n        head_ip = _query_head_ip_with_retries(\n            cluster_config_file, max_attempts=WAIT_HEAD_NODE_IP_MAX_ATTEMPTS)\n    except exceptions.FetchClusterInfoError as e:\n        logger.error(common_utils.format_exception(e))\n        return False, None  # failed\n\n    config = global_user_state.get_cluster_yaml_dict(cluster_config_file)\n\n    docker_user = None\n    if 'docker' in config:\n        docker_user = get_docker_user(head_ip, cluster_config_file)\n\n    if num_nodes <= 1:\n        return True, docker_user\n\n    ssh_credentials = ssh_credential_from_yaml(cluster_config_file, docker_user)\n    last_nodes_so_far = 0\n    start = time.time()\n    runner = command_runner.SSHCommandRunner(node=(head_ip, 22),\n                                             **ssh_credentials)\n    with rich_utils.safe_status(\n            ux_utils.spinner_message('Waiting for workers',\n                                     log_path=log_path)) as worker_status:\n        while True:\n            rc, output, stderr = runner.run(\n                instance_setup.RAY_STATUS_WITH_SKY_RAY_PORT_COMMAND,\n                log_path=log_path,\n                stream_logs=False,\n                require_outputs=True,\n                separate_stderr=True)\n            subprocess_utils.handle_returncode(\n                rc, instance_setup.RAY_STATUS_WITH_SKY_RAY_PORT_COMMAND,\n                'Failed to run ray status on head node.', stderr)\n            logger.debug(output)\n\n            ready_head, ready_workers = _count_healthy_nodes_from_ray(\n                output, is_local_cloud=is_local_cloud)\n\n            worker_status.update(\n                ux_utils.spinner_message(\n                    f'{ready_workers} out of {num_nodes - 1} '\n                    'workers ready',\n                    log_path=log_path))\n\n            # In the local case, ready_head=0 and ready_workers=num_nodes. This\n            # is because there is no matching regex for _LAUNCHED_HEAD_PATTERN.\n            if ready_head + ready_workers == num_nodes:\n                # All nodes are up.\n                break\n\n            # Pending workers that have been launched by ray up.\n            found_ips = _LAUNCHING_IP_PATTERN.findall(output)\n            pending_workers = len(found_ips)\n\n            # TODO(zhwu): Handle the case where the following occurs, where ray\n            # cluster is not correctly started on the cluster.\n            # Pending:\n            #  172.31.9.121: ray.worker.default, uninitialized\n            nodes_so_far = ready_head + ready_workers + pending_workers\n\n            # Check the number of nodes that are fetched. Timeout if no new\n            # nodes fetched in a while (nodes_launching_progress_timeout),\n            # though number of nodes_so_far is still not as expected.\n            if nodes_so_far > last_nodes_so_far:\n                # Reset the start time if the number of launching nodes\n                # changes, i.e. new nodes are launched.\n                logger.debug('Reset start time, as new nodes are launched. '\n                             f'({last_nodes_so_far} -> {nodes_so_far})')\n                start = time.time()\n                last_nodes_so_far = nodes_so_far\n            elif (nodes_launching_progress_timeout is not None and\n                  time.time() - start > nodes_launching_progress_timeout and\n                  nodes_so_far != num_nodes):\n                logger.error(\n                    'Timed out: waited for more than '\n                    f'{nodes_launching_progress_timeout} seconds for new '\n                    'workers to be provisioned, but no progress.')\n                return False, None  # failed\n\n            if '(no pending nodes)' in output and '(no failures)' in output:\n                # Bug in ray autoscaler: e.g., on GCP, if requesting 2 nodes\n                # that GCP can satisfy only by half, the worker node would be\n                # forgotten. The correct behavior should be for it to error out.\n                logger.error(\n                    'Failed to launch multiple nodes on '\n                    'GCP due to a nondeterministic bug in ray autoscaler.')\n                return False, None  # failed\n            time.sleep(10)\n    return True, docker_user  # success\n\n\ndef _get_ssh_control_name(config: Dict[str, Any]) -> str:\n    ssh_provider_module = config['provider']['module']\n    ssh_control_name = config.get('cluster_name',\n                                  command_runner.DEFAULT_SSH_CONTROL_NAME)\n    if 'slurm' in ssh_provider_module:\n        # For Slurm, multiple SkyPilot clusters may share the same underlying\n        # Slurm login node. By using a fixed ssh_control_name ('__default__'),\n        # we ensure that all connections to the same login node reuse the same\n        # SSH ControlMaster process, avoiding repeated SSH handshakes.\n        #\n        # The %C token in ControlPath (see ssh_options_list) ensures that\n        # connections to different login nodes use different sockets, avoiding\n        # collisions between different Slurm clusters.\n        ssh_control_name = command_runner.DEFAULT_SSH_CONTROL_NAME\n    return ssh_control_name\n\n\ndef ssh_credential_from_yaml(\n    cluster_yaml: Optional[str],\n    docker_user: Optional[str] = None,\n    ssh_user: Optional[str] = None,\n) -> Dict[str, Any]:\n    \"\"\"Returns ssh_user, ssh_private_key and ssh_control name.\n\n    Args:\n        cluster_yaml: path to the cluster yaml.\n        docker_user: when using custom docker image, use this user to ssh into\n            the docker container.\n        ssh_user: override the ssh_user in the cluster yaml.\n    \"\"\"\n    if cluster_yaml is None:\n        return dict()\n    config = global_user_state.get_cluster_yaml_dict(cluster_yaml)\n    auth_section = config['auth']\n    if ssh_user is None:\n        ssh_user = auth_section['ssh_user'].strip()\n    ssh_private_key_path = auth_section.get('ssh_private_key')\n    ssh_control_name = _get_ssh_control_name(config)\n    ssh_proxy_command = auth_section.get('ssh_proxy_command')\n\n    # Update the ssh_user placeholder in proxy command, if required\n    if (ssh_proxy_command is not None and\n            constants.SKY_SSH_USER_PLACEHOLDER in ssh_proxy_command):\n        ssh_proxy_command = ssh_proxy_command.replace(\n            constants.SKY_SSH_USER_PLACEHOLDER, ssh_user)\n\n    credentials = {\n        'ssh_user': ssh_user,\n        'ssh_private_key': ssh_private_key_path,\n        'ssh_control_name': ssh_control_name,\n        'ssh_proxy_command': ssh_proxy_command,\n    }\n    if docker_user is not None:\n        credentials['docker_user'] = docker_user\n    ssh_provider_module = config['provider']['module']\n    # If we are running ssh command on kubernetes node.\n    if 'kubernetes' in ssh_provider_module:\n        credentials['disable_control_master'] = True\n    return credentials\n\n\ndef ssh_credentials_from_handles(\n    handles: List['cloud_vm_ray_backend.CloudVmRayResourceHandle'],\n) -> List[Dict[str, Any]]:\n    \"\"\"Returns ssh_user, ssh_private_key and ssh_control name.\n    \"\"\"\n    non_empty_cluster_yaml_paths = [\n        handle.cluster_yaml\n        for handle in handles\n        if handle.cluster_yaml is not None\n    ]\n    cluster_yaml_dicts = global_user_state.get_cluster_yaml_dict_multiple(\n        non_empty_cluster_yaml_paths)\n    cluster_yaml_dicts_to_index = {\n        cluster_yaml_path: cluster_yaml_dict\n        for cluster_yaml_path, cluster_yaml_dict in zip(\n            non_empty_cluster_yaml_paths, cluster_yaml_dicts)\n    }\n\n    credentials_to_return: List[Dict[str, Any]] = []\n    for handle in handles:\n        if handle.cluster_yaml is None:\n            credentials_to_return.append(dict())\n            continue\n        ssh_user = handle.ssh_user\n        docker_user = handle.docker_user\n        config = cluster_yaml_dicts_to_index[handle.cluster_yaml]\n        auth_section = config['auth']\n        if ssh_user is None:\n            ssh_user = auth_section['ssh_user'].strip()\n        ssh_private_key_path = auth_section.get('ssh_private_key')\n        ssh_control_name = _get_ssh_control_name(config)\n        ssh_proxy_command = auth_section.get('ssh_proxy_command')\n\n        # Update the ssh_user placeholder in proxy command, if required\n        if (ssh_proxy_command is not None and\n                constants.SKY_SSH_USER_PLACEHOLDER in ssh_proxy_command):\n            ssh_proxy_command = ssh_proxy_command.replace(\n                constants.SKY_SSH_USER_PLACEHOLDER, ssh_user)\n\n        credentials = {\n            'ssh_user': ssh_user,\n            'ssh_private_key': ssh_private_key_path,\n            'ssh_control_name': ssh_control_name,\n            'ssh_proxy_command': ssh_proxy_command,\n        }\n        if docker_user is not None:\n            credentials['docker_user'] = docker_user\n        ssh_provider_module = config['provider']['module']\n        # If we are running ssh command on kubernetes node.\n        if 'kubernetes' in ssh_provider_module:\n            credentials['disable_control_master'] = True\n        credentials_to_return.append(credentials)\n\n    return credentials_to_return\n\n\ndef parallel_data_transfer_to_nodes(\n        runners: List[command_runner.CommandRunner],\n        source: Optional[str],\n        target: str,\n        cmd: Optional[str],\n        run_rsync: bool,\n        *,\n        action_message: str,\n        # Advanced options.\n        log_path: str = os.devnull,\n        stream_logs: bool = False,\n        source_bashrc: bool = False,\n        num_threads: Optional[int] = None):\n    \"\"\"Runs a command on all nodes and optionally runs rsync from src->dst.\n\n    Args:\n        runners: A list of CommandRunner objects that represent multiple nodes.\n        source: Optional[str]; Source for rsync on local node\n        target: str; Destination on remote node for rsync\n        cmd: str; Command to be executed on all nodes\n        action_message: str; Message to be printed while the command runs\n        log_path: str; Path to the log file\n        stream_logs: bool; Whether to stream logs to stdout\n        source_bashrc: bool; Source bashrc before running the command.\n        num_threads: Optional[int]; Number of threads to use.\n    \"\"\"\n    style = colorama.Style\n\n    origin_source = source\n\n    def _sync_node(runner: 'command_runner.CommandRunner') -> None:\n        if cmd is not None:\n            rc, stdout, stderr = runner.run(cmd,\n                                            log_path=log_path,\n                                            stream_logs=stream_logs,\n                                            require_outputs=True,\n                                            source_bashrc=source_bashrc)\n            err_msg = (f'{colorama.Style.RESET_ALL}{colorama.Style.DIM}'\n                       f'----- CMD -----\\n'\n                       f'{cmd}\\n'\n                       f'----- CMD END -----\\n'\n                       f'{colorama.Style.RESET_ALL}'\n                       f'{colorama.Fore.RED}'\n                       f'Failed to run command before rsync '\n                       f'{origin_source} -> {target}. '\n                       f'{colorama.Style.RESET_ALL}')\n            if log_path != os.devnull:\n                err_msg += ux_utils.log_path_hint(log_path)\n            subprocess_utils.handle_returncode(rc,\n                                               cmd,\n                                               err_msg,\n                                               stderr=stdout + stderr)\n\n        if run_rsync:\n            assert source is not None\n            # TODO(zhwu): Optimize for large amount of files.\n            # zip / transfer / unzip\n            runner.rsync(\n                source=source,\n                target=target,\n                up=True,\n                log_path=log_path,\n                stream_logs=stream_logs,\n            )\n\n    num_nodes = len(runners)\n    plural = 's' if num_nodes > 1 else ''\n    message = (f'  {style.DIM}{action_message} (to {num_nodes} node{plural})'\n               f': {origin_source} -> {target}{style.RESET_ALL}')\n    logger.info(message)\n    subprocess_utils.run_in_parallel(_sync_node, runners, num_threads)\n\n\ndef check_local_gpus() -> bool:\n    \"\"\"Checks if GPUs are available locally.\n\n    Returns whether GPUs are available on the local machine by checking\n    if nvidia-smi is installed and returns zero return code.\n\n    Returns True if nvidia-smi is installed and returns zero return code,\n    False if not.\n    \"\"\"\n    is_functional = False\n    installation_check = subprocess.run(['which', 'nvidia-smi'],\n                                        stdout=subprocess.DEVNULL,\n                                        stderr=subprocess.DEVNULL,\n                                        check=False)\n    is_installed = installation_check.returncode == 0\n    if is_installed:\n        execution_check = subprocess.run(['nvidia-smi'],\n                                         stdout=subprocess.DEVNULL,\n                                         stderr=subprocess.DEVNULL,\n                                         check=False)\n        is_functional = execution_check.returncode == 0\n    return is_functional\n\n\ndef _query_head_ip_with_retries(cluster_yaml: str,\n                                max_attempts: int = 1) -> str:\n    \"\"\"Returns the IP of the head node by querying the cloud.\n\n    Raises:\n      exceptions.FetchClusterInfoError: if we failed to get the head IP.\n    \"\"\"\n    backoff = common_utils.Backoff(initial_backoff=5, max_backoff_factor=5)\n    for i in range(max_attempts):\n        try:\n            full_cluster_yaml = str(pathlib.Path(cluster_yaml).expanduser())\n            out = subprocess_utils.run(\n                f'ray get-head-ip {full_cluster_yaml!r}',\n                stdout=subprocess.PIPE,\n                stderr=subprocess.DEVNULL).stdout.decode().strip()\n            head_ip_list = re.findall(IP_ADDR_REGEX, out)\n            if len(head_ip_list) > 1:\n                # This could be triggered if e.g., some logging is added in\n                # skypilot_config, a module that has some code executed\n                # whenever `sky` is imported.\n                logger.warning(\n                    'Detected more than 1 IP from the output of '\n                    'the `ray get-head-ip` command. This could '\n                    'happen if there is extra output from it, '\n                    'which should be inspected below.\\nProceeding with '\n                    f'the last detected IP ({head_ip_list[-1]}) as head IP.'\n                    f'\\n== Output ==\\n{out}'\n                    f'\\n== Output ends ==')\n                head_ip_list = head_ip_list[-1:]\n            assert 1 == len(head_ip_list), (out, head_ip_list)\n            head_ip = head_ip_list[0]\n            break\n        except subprocess.CalledProcessError as e:\n            if i == max_attempts - 1:\n                raise exceptions.FetchClusterInfoError(\n                    reason=exceptions.FetchClusterInfoError.Reason.HEAD) from e\n            # Retry if the cluster is not up yet.\n            logger.debug('Retrying to get head ip.')\n            time.sleep(backoff.current_backoff())\n    return head_ip\n\n\n@timeline.event\ndef get_node_ips(cluster_yaml: str,\n                 expected_num_nodes: int,\n                 head_ip_max_attempts: int = 1,\n                 worker_ip_max_attempts: int = 1,\n                 get_internal_ips: bool = False) -> List[str]:\n    \"\"\"Returns the IPs of all nodes in the cluster, with head node at front.\n\n    Args:\n        cluster_yaml: Path to the cluster yaml.\n        expected_num_nodes: Expected number of nodes in the cluster.\n        head_ip_max_attempts: Max attempts to get head ip.\n        worker_ip_max_attempts: Max attempts to get worker ips.\n        get_internal_ips: Whether to get internal IPs. When False, it is still\n            possible to get internal IPs if the cluster does not have external\n            IPs.\n\n    Raises:\n        exceptions.FetchClusterInfoError: if we failed to get the IPs. e.reason is\n            HEAD or WORKER.\n    \"\"\"\n    ray_config = global_user_state.get_cluster_yaml_dict(cluster_yaml)\n    # Use the new provisioner for AWS.\n    provider_name = cluster_utils.get_provider_name(ray_config)\n    cloud = registry.CLOUD_REGISTRY.from_str(provider_name)\n    assert cloud is not None, provider_name\n\n    if cloud.PROVISIONER_VERSION >= clouds.ProvisionerVersion.SKYPILOT:\n        try:\n            metadata = provision_lib.get_cluster_info(\n                provider_name, ray_config['provider'].get('region'),\n                ray_config['cluster_name'], ray_config['provider'])\n        except Exception as e:  # pylint: disable=broad-except\n            # This could happen when the VM is not fully launched, and a user\n            # is trying to terminate it with `sky down`.\n            logger.debug(\n                'Failed to get cluster info for '\n                f'{ray_config[\"cluster_name\"]} from the new provisioner '\n                f'with {common_utils.format_exception(e)}.')\n            raise exceptions.FetchClusterInfoError(\n                exceptions.FetchClusterInfoError.Reason.HEAD) from e\n        if len(metadata.instances) < expected_num_nodes:\n            # Simulate the exception when Ray head node is not up.\n            raise exceptions.FetchClusterInfoError(\n                exceptions.FetchClusterInfoError.Reason.HEAD)\n        return metadata.get_feasible_ips(get_internal_ips)\n\n    if get_internal_ips:\n        with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n            ray_config['provider']['use_internal_ips'] = True\n            yaml.dump(ray_config, f)\n            cluster_yaml = f.name\n\n    # Check the network connection first to avoid long hanging time for\n    # ray get-head-ip below, if a long-lasting network connection failure\n    # happens.\n    check_network_connection()\n    head_ip = _query_head_ip_with_retries(cluster_yaml,\n                                          max_attempts=head_ip_max_attempts)\n    head_ip_list = [head_ip]\n    if expected_num_nodes > 1:\n        backoff = common_utils.Backoff(initial_backoff=5, max_backoff_factor=5)\n\n        for retry_cnt in range(worker_ip_max_attempts):\n            try:\n                full_cluster_yaml = str(pathlib.Path(cluster_yaml).expanduser())\n                proc = subprocess_utils.run(\n                    f'ray get-worker-ips {full_cluster_yaml!r}',\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE)\n                out = proc.stdout.decode()\n                break\n            except subprocess.CalledProcessError as e:\n                if retry_cnt == worker_ip_max_attempts - 1:\n                    raise exceptions.FetchClusterInfoError(\n                        exceptions.FetchClusterInfoError.Reason.WORKER) from e\n                # Retry if the ssh is not ready for the workers yet.\n                backoff_time = backoff.current_backoff()\n                logger.debug('Retrying to get worker ip '\n                             f'[{retry_cnt}/{worker_ip_max_attempts}] in '\n                             f'{backoff_time} seconds.')\n                time.sleep(backoff_time)\n        worker_ips = re.findall(IP_ADDR_REGEX, out)\n        if len(worker_ips) != expected_num_nodes - 1:\n            n = expected_num_nodes - 1\n            if len(worker_ips) > n:\n                # This could be triggered if e.g., some logging is added in\n                # skypilot_config, a module that has some code executed whenever\n                # `sky` is imported.\n                logger.warning(\n                    f'Expected {n} worker IP(s); found '\n                    f'{len(worker_ips)}: {worker_ips}'\n                    '\\nThis could happen if there is extra output from '\n                    '`ray get-worker-ips`, which should be inspected below.'\n                    f'\\n== Output ==\\n{out}'\n                    f'\\n== Output ends ==')\n                logger.warning(f'\\nProceeding with the last {n} '\n                               f'detected IP(s): {worker_ips[-n:]}.')\n                worker_ips = worker_ips[-n:]\n            else:\n                raise exceptions.FetchClusterInfoError(\n                    exceptions.FetchClusterInfoError.Reason.WORKER)\n    else:\n        worker_ips = []\n    return head_ip_list + worker_ips\n\n\ndef check_network_connection():\n    # Tolerate 3 retries as it is observed that connections can fail.\n    http = requests.Session()\n    http.mount('https://', adapters.HTTPAdapter())\n    http.mount('http://', adapters.HTTPAdapter())\n\n    # Alternate between IPs on each retry\n    max_retries = 3\n    timeout = 0.5\n\n    for _ in range(max_retries):\n        for ip in _TEST_IP_LIST:\n            try:\n                http.head(ip, timeout=timeout)\n                return\n            except (requests.Timeout, requests.exceptions.ConnectionError):\n                continue\n\n        timeout *= 2  # Double the timeout for next retry\n\n    # If we get here, all IPs failed\n    # Assume network connection is down\n    raise exceptions.NetworkError('Could not refresh the cluster. '\n                                  'Network seems down.')\n\n\nasync def async_check_network_connection():\n    \"\"\"Check if the network connection is available.\n\n    Tolerates 3 retries as it is observed that connections can fail.\n    Uses aiohttp for async HTTP requests.\n    \"\"\"\n    # Create a session with retry logic\n    timeout = ClientTimeout(total=15)\n    connector = TCPConnector(limit=1)  # Limit to 1 connection at a time\n\n    async with aiohttp.ClientSession(timeout=timeout,\n                                     connector=connector) as session:\n        for i, ip in enumerate(_TEST_IP_LIST):\n            try:\n                async with session.head(ip) as response:\n                    if response.status < 400:  # Any 2xx or 3xx status is good\n                        return\n            except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n                if i == len(_TEST_IP_LIST) - 1:\n                    raise exceptions.NetworkError(\n                        'Could not refresh the cluster. '\n                        'Network seems down.') from e\n                # If not the last IP, continue to try the next one\n                continue\n\n\n@timeline.event\ndef check_owner_identity(cluster_name: str) -> None:\n    \"\"\"Check if current user is the same as the user who created the cluster.\n\n    Raises:\n        exceptions.ClusterOwnerIdentityMismatchError: if the current user is\n          not the same as the user who created the cluster.\n        exceptions.CloudUserIdentityError: if we fail to get the current user\n          identity.\n    \"\"\"\n    if env_options.Options.SKIP_CLOUD_IDENTITY_CHECK.get():\n        return\n    record = global_user_state.get_cluster_from_name(cluster_name,\n                                                     include_user_info=False,\n                                                     summary_response=True)\n    if record is None:\n        return\n    _check_owner_identity_with_record(cluster_name, record)\n\n\ndef _check_owner_identity_with_record(cluster_name: str,\n                                      record: Dict[str, Any]) -> None:\n    if env_options.Options.SKIP_CLOUD_IDENTITY_CHECK.get():\n        return\n    handle = record['handle']\n    if not isinstance(handle, backends.CloudVmRayResourceHandle):\n        return\n    active_workspace = skypilot_config.get_active_workspace()\n    cluster_workspace = record.get('workspace',\n                                   constants.SKYPILOT_DEFAULT_WORKSPACE)\n    if active_workspace != cluster_workspace:\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.ClusterOwnerIdentityMismatchError(\n                f'{colorama.Fore.YELLOW}'\n                f'The cluster {cluster_name!r} is in workspace '\n                f'{cluster_workspace!r}, but the active workspace is '\n                f'{active_workspace!r}.{colorama.Fore.RESET}')\n\n    launched_resources = handle.launched_resources.assert_launchable()\n    cloud = launched_resources.cloud\n    user_identities = cloud.get_user_identities()\n    owner_identity = record['owner']\n    if user_identities is None:\n        # Skip the check if the cloud does not support user identity.\n        return\n    # The user identity can be None, if the cluster is created by an older\n    # version of SkyPilot. In that case, we set the user identity to the\n    # current active one.\n    # NOTE: a user who upgrades SkyPilot and switches to a new cloud identity\n    # immediately without `sky status --refresh` first, will cause a leakage\n    # of the existing cluster. We deem this an acceptable tradeoff mainly\n    # because multi-identity is not common (at least at the moment).\n    if owner_identity is None:\n        global_user_state.set_owner_identity_for_cluster(\n            cluster_name, user_identities[0])\n    else:\n        assert isinstance(owner_identity, list)\n        # It is OK if the owner identity is shorter, which will happen when\n        # the cluster is launched before #1808. In that case, we only check\n        # the same length (zip will stop at the shorter one).\n        for identity in user_identities:\n            for i, (owner, current) in enumerate(zip(owner_identity, identity)):\n                # Clean up the owner identity for the backslash and newlines, caused\n                # by the cloud CLI output, e.g. gcloud.\n                owner = owner.replace('\\n', '').replace('\\\\', '')\n                if owner == current:\n                    if i != 0:\n                        logger.warning(\n                            f'The cluster was owned by {owner_identity}, but '\n                            f'a new identity {identity} is activated. We still '\n                            'allow the operation as the two identities are '\n                            'likely to have the same access to the cluster. '\n                            'Please be aware that this can cause unexpected '\n                            'cluster leakage if the two identities are not '\n                            'actually equivalent (e.g., belong to the same '\n                            'person).')\n                    if i != 0 or len(owner_identity) != len(identity):\n                        # We update the owner of a cluster, when:\n                        # 1. The strictest identty (i.e. the first one) does not\n                        # match, but the latter ones match.\n                        # 2. The length of the two identities are different,\n                        # which will only happen when the cluster is launched\n                        # before #1808. Update the user identity to avoid\n                        # showing the warning above again.\n                        global_user_state.set_owner_identity_for_cluster(\n                            cluster_name, identity)\n                    return  # The user identity matches.\n        # Generate error message if no match found\n        if len(user_identities) == 1:\n            err_msg = f'the activated identity is {user_identities[0]!r}.'\n        else:\n            err_msg = (f'available identities are {user_identities!r}.')\n        if cloud.is_same_cloud(clouds.Kubernetes()):\n            err_msg += (' Check your kubeconfig file and make sure the '\n                        'correct context is available.')\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.ClusterOwnerIdentityMismatchError(\n                f'{cluster_name!r} ({cloud}) is owned by account '\n                f'{owner_identity!r}, but ' + err_msg)\n\n\ndef tag_filter_for_cluster(cluster_name: str) -> Dict[str, str]:\n    \"\"\"Returns a tag filter for the cluster.\"\"\"\n    return {\n        'ray-cluster-name': cluster_name,\n    }\n\n\n@context_utils.cancellation_guard\ndef _query_cluster_status_via_cloud_api(\n    handle: 'cloud_vm_ray_backend.CloudVmRayResourceHandle',\n    retry_if_missing: bool,\n) -> List[Tuple[status_lib.ClusterStatus, Optional[str]]]:\n    \"\"\"Returns the status of the cluster as a list of tuples corresponding\n    to the node status and an optional reason string for said status.\n\n    Raises:\n        exceptions.ClusterStatusFetchingError: the cluster status cannot be\n          fetched from the cloud provider.\n    \"\"\"\n    cluster_name = handle.cluster_name\n    cluster_name_on_cloud = handle.cluster_name_on_cloud\n    cluster_name_in_hint = common_utils.cluster_name_in_hint(\n        handle.cluster_name, cluster_name_on_cloud)\n    # Use region and zone from the cluster config, instead of the\n    # handle.launched_resources, because the latter may not be set\n    # correctly yet.\n    ray_config = global_user_state.get_cluster_yaml_dict(handle.cluster_yaml)\n    provider_config = ray_config['provider']\n\n    # Query the cloud provider.\n    # TODO(suquark): move implementations of more clouds here\n    cloud = handle.launched_resources.cloud\n    assert cloud is not None, handle\n    if cloud.STATUS_VERSION >= clouds.StatusVersion.SKYPILOT:\n        cloud_name = repr(handle.launched_resources.cloud)\n        try:\n            node_status_dict = provision_lib.query_instances(\n                cloud_name,\n                cluster_name,\n                cluster_name_on_cloud,\n                provider_config,\n                retry_if_missing=retry_if_missing)\n            logger.debug(f'Querying {cloud_name} cluster '\n                         f'{cluster_name_in_hint} '\n                         f'status:\\n{pprint.pformat(node_status_dict)}')\n            node_statuses = list(node_status_dict.values())\n        except Exception as e:  # pylint: disable=broad-except\n            with ux_utils.print_exception_no_traceback():\n                raise exceptions.ClusterStatusFetchingError(\n                    f'Failed to query {cloud_name} cluster '\n                    f'{cluster_name_in_hint} '\n                    f'status: {common_utils.format_exception(e, use_bracket=True)}'\n                )\n    else:\n        region = provider_config.get('region') or provider_config.get(\n            'location')\n        zone = ray_config['provider'].get('availability_zone')\n        # TODO (kyuds): refactor cloud.query_status api to include reason.\n        # Currently not refactoring as this API is actually supposed to be\n        # deprecated soon.\n        node_statuses = cloud.query_status(\n            cluster_name_on_cloud,\n            tag_filter_for_cluster(cluster_name_on_cloud), region, zone)\n        node_statuses = [(status, None) for status in node_statuses]\n    return node_statuses\n\n\ndef _query_cluster_info_via_cloud_api(\n    handle: 'cloud_vm_ray_backend.CloudVmRayResourceHandle'\n) -> provision_common.ClusterInfo:\n    \"\"\"Returns the cluster info.\n\n    Raises:\n        exceptions.NotSupportedError: the cloud does not support the new provisioner.\n        exceptions.FetchClusterInfoError: the cluster info cannot be\n          fetched from the cloud provider.\n    \"\"\"\n    cloud = handle.launched_resources.cloud\n    assert cloud is not None, handle\n    if cloud.STATUS_VERSION >= clouds.StatusVersion.SKYPILOT:\n        try:\n            cloud_name = repr(cloud)\n            ray_config = global_user_state.get_cluster_yaml_dict(\n                handle.cluster_yaml)\n            provider_config = ray_config['provider']\n            region = provider_config.get('region') or provider_config.get(\n                'location')\n            cluster_info = provision_lib.get_cluster_info(\n                cloud_name, region, handle.cluster_name_on_cloud,\n                provider_config)\n            logger.debug(\n                f'Querying {cloud_name} cluster '\n                f'{handle.cluster_name_on_cloud} '\n                f'head instance:\\n{cluster_info.get_head_instance()}\\n'\n                f'worker instances:\\n{cluster_info.get_worker_instances()}')\n            return cluster_info\n        except Exception as e:  # pylint: disable=broad-except\n            with ux_utils.print_exception_no_traceback():\n                raise exceptions.FetchClusterInfoError(\n                    reason=exceptions.FetchClusterInfoError.Reason.UNKNOWN\n                ) from e\n    else:\n        raise exceptions.NotSupportedError(\n            f'The cloud {cloud} does not support the SkyPilot provisioner.')\n\n\ndef check_can_clone_disk_and_override_task(\n    cluster_name: str, target_cluster_name: Optional[str], task: 'task_lib.Task'\n) -> Tuple['task_lib.Task', 'cloud_vm_ray_backend.CloudVmRayResourceHandle']:\n    \"\"\"Check if the task is compatible to clone disk from the source cluster.\n\n    Args:\n        cluster_name: The name of the cluster to clone disk from.\n        target_cluster_name: The name of the target cluster.\n        task: The task to check.\n\n    Returns:\n        The task to use and the resource handle of the source cluster.\n\n    Raises:\n        exceptions.ClusterDoesNotExist: If the source cluster does not exist.\n        exceptions.NotSupportedError: If the source cluster is not valid or the\n            task is not compatible to clone disk from the source cluster.\n    \"\"\"\n    source_cluster_status, handle = refresh_cluster_status_handle(cluster_name)\n    if source_cluster_status is None:\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.ClusterDoesNotExist(\n                f'Cannot find cluster {cluster_name!r} to clone disk from.')\n\n    if not isinstance(handle, backends.CloudVmRayResourceHandle):\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.NotSupportedError(\n                f'Cannot clone disk from a non-cloud cluster {cluster_name!r}.')\n\n    if source_cluster_status != status_lib.ClusterStatus.STOPPED:\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.NotSupportedError(\n                f'Cannot clone disk from cluster {cluster_name!r} '\n                f'({source_cluster_status.value!r}). Please stop the '\n                f'cluster first: sky stop {cluster_name}')\n\n    if target_cluster_name is not None:\n        target_cluster_status, _ = refresh_cluster_status_handle(\n            target_cluster_name)\n        if target_cluster_status is not None:\n            with ux_utils.print_exception_no_traceback():\n                raise exceptions.NotSupportedError(\n                    f'The target cluster {target_cluster_name!r} already exists. Cloning '\n                    'disk is only supported when creating a new cluster. To fix: specify '\n                    'a new target cluster name.')\n\n    new_task_resources = []\n    launched_resources = handle.launched_resources.assert_launchable()\n    original_cloud = launched_resources.cloud\n    original_cloud.check_features_are_supported(\n        launched_resources,\n        {clouds.CloudImplementationFeatures.CLONE_DISK_FROM_CLUSTER})\n\n    has_override = False\n    has_disk_size_met = False\n    has_cloud_met = False\n    for task_resources in task.resources:\n        if handle.launched_resources.disk_size > task_resources.disk_size:\n            # The target cluster's disk should be at least as large as the source.\n            continue\n        has_disk_size_met = True\n        if task_resources.cloud is not None and not original_cloud.is_same_cloud(\n                task_resources.cloud):\n            continue\n        has_cloud_met = True\n\n        override_param: Dict[str, Any] = {}\n        if task_resources.cloud is None:\n            override_param['cloud'] = original_cloud\n        if task_resources.region is None:\n            override_param['region'] = handle.launched_resources.region\n\n        if override_param:\n            logger.info(\n                f'No cloud/region specified for the task {task_resources}. Using the same region '\n                f'as source cluster {cluster_name!r}: '\n                f'{handle.launched_resources.cloud}'\n                f'({handle.launched_resources.region}).')\n            has_override = True\n        task_resources = task_resources.copy(**override_param)\n        new_task_resources.append(task_resources)\n\n    if not new_task_resources:\n        if not has_disk_size_met:\n            with ux_utils.print_exception_no_traceback():\n                target_cluster_name_str = f' {target_cluster_name!r}'\n                if target_cluster_name is None:\n                    target_cluster_name_str = ''\n                raise exceptions.NotSupportedError(\n                    f'The target cluster{target_cluster_name_str} should have a disk size '\n                    f'of at least {handle.launched_resources.disk_size} GB to clone the '\n                    f'disk from {cluster_name!r}.')\n        if not has_cloud_met:\n            task_resources_cloud_str = '[' + ','.join(\n                [f'{res.cloud}' for res in task.resources]) + ']'\n            task_resources_str = '[' + ','.join(\n                [f'{res}' for res in task.resources]) + ']'\n            with ux_utils.print_exception_no_traceback():\n                raise ValueError(\n                    f'Cannot clone disk across cloud from {original_cloud} to '\n                    f'{task_resources_cloud_str} for resources {task_resources_str}.'\n                )\n        assert False, 'Should not reach here.'\n    # set the new_task_resources to be the same type (list or set) as the\n    # original task.resources\n    if has_override:\n        task.set_resources(type(task.resources)(new_task_resources))\n        # Reset the best_resources to triger re-optimization\n        # later, so that the new task_resources will be used.\n        task.best_resources = None\n    return task, handle\n\n\ndef _update_cluster_status(\n        cluster_name: str,\n        record: Dict[str, Any],\n        retry_if_missing: bool,\n        include_user_info: bool = True,\n        summary_response: bool = False) -> Optional[Dict[str, Any]]:\n    \"\"\"Update the cluster status.\n\n    The cluster status is updated by checking ray cluster and real status from\n    cloud.\n\n    The function will update the cached cluster status in the global state. For\n    the design of the cluster status and transition, please refer to the\n    sky/design_docs/cluster_status.md\n\n    Note: this function is only safe to be called when the caller process is\n    holding the cluster lock, which means no other processes are modifying the\n    cluster.\n\n    Returns:\n        If the cluster is terminated or does not exist, return None. Otherwise\n        returns the input record with status and handle potentially updated.\n\n    Raises:\n        exceptions.ClusterOwnerIdentityMismatchError: if the current user is\n          not the same as the user who created the cluster.\n        exceptions.CloudUserIdentityError: if we fail to get the current user\n          identity.\n        exceptions.ClusterStatusFetchingError: the cluster status cannot be\n          fetched from the cloud provider or there are leaked nodes causing\n          the node number larger than expected.\n    \"\"\"\n    handle = record['handle']\n    if handle.cluster_yaml is None:\n        # Remove cluster from db since this cluster does not have a config file\n        # or any other ongoing requests\n        global_user_state.add_cluster_event(\n            cluster_name,\n            None,\n            'Cluster has no YAML file. Removing the cluster from cache.',\n            global_user_state.ClusterEventType.STATUS_CHANGE,\n            nop_if_duplicate=True)\n        global_user_state.remove_cluster(cluster_name, terminate=True)\n        logger.debug(f'Cluster {cluster_name!r} has no YAML file. '\n                     'Removing the cluster from cache.')\n        return None\n    if not isinstance(handle, backends.CloudVmRayResourceHandle):\n        return record\n    cluster_name = handle.cluster_name\n\n    node_statuses = _query_cluster_status_via_cloud_api(\n        handle, retry_if_missing=retry_if_missing)\n\n    all_nodes_up = (all(status[0] == status_lib.ClusterStatus.UP\n                        for status in node_statuses) and\n                    len(node_statuses) == handle.launched_nodes)\n\n    external_cluster_failures = ExternalFailureSource.get(\n        cluster_hash=record['cluster_hash'])\n    logger.debug(f'Cluster {cluster_name} with cluster_hash '\n                 f'{record[\"cluster_hash\"]} has external cluster failures: '\n                 f'{external_cluster_failures}')\n\n    def get_node_counts_from_ray_status(\n            runner: command_runner.CommandRunner) -> Tuple[int, int, str, str]:\n        rc, output, stderr = runner.run(\n            instance_setup.RAY_STATUS_WITH_SKY_RAY_PORT_COMMAND,\n            stream_logs=False,\n            require_outputs=True,\n            separate_stderr=True)\n        if rc:\n            raise exceptions.CommandError(\n                rc, instance_setup.RAY_STATUS_WITH_SKY_RAY_PORT_COMMAND,\n                f'Failed to check ray cluster\\'s healthiness.\\n'\n                '-- stdout --\\n'\n                f'{output}\\n', stderr)\n        return (*_count_healthy_nodes_from_ray(output), output, stderr)\n\n    ray_status_details: Optional[str] = None\n\n    def run_ray_status_to_check_ray_cluster_healthy() -> bool:\n        nonlocal ray_status_details\n        try:\n            # NOTE: fetching the IPs is very slow as it calls into\n            # `ray get head-ip/worker-ips`. Using cached IPs is safe because\n            # in the worst case we time out in the `ray status` SSH command\n            # below.\n            runners = handle.get_command_runners(force_cached=True)\n            # This happens when user interrupt the `sky launch` process before\n            # the first time resources handle is written back to local database.\n            # This is helpful when user interrupt after the provision is done\n            # and before the skylet is restarted. After #2304 is merged, this\n            # helps keep the cluster status to INIT after `sky status -r`, so\n            # user will be notified that any auto stop/down might not be\n            # triggered.\n            if not runners:\n                logger.debug(f'Refreshing status ({cluster_name!r}): No cached '\n                             f'IPs found. Handle: {handle}')\n                raise exceptions.FetchClusterInfoError(\n                    reason=exceptions.FetchClusterInfoError.Reason.HEAD)\n            head_runner = runners[0]\n\n            total_nodes = handle.launched_nodes * handle.num_ips_per_node\n\n            cloud_name = repr(handle.launched_resources.cloud).lower()\n            # Initialize variables in case all retries fail\n            ready_head = 0\n            ready_workers = 0\n            output = ''\n            stderr = ''\n            for i in range(5):\n                try:\n                    ready_head, ready_workers, output, stderr = (\n                        get_node_counts_from_ray_status(head_runner))\n                except exceptions.CommandError as e:\n                    logger.debug(f'Refreshing status ({cluster_name!r}) attempt'\n                                 f' {i}: {common_utils.format_exception(e)}')\n                    if cloud_name != 'kubernetes':\n                        # Non-k8s clusters can be manually restarted and:\n                        # 1. Get new IP addresses, or\n                        # 2. Not have the SkyPilot runtime setup\n                        #\n                        # So we should surface a message to the user to\n                        # help them recover from this inconsistent state.\n                        has_new_ip_addr = (\n                            e.detailed_reason is not None and\n                            _SSH_CONNECTION_TIMED_OUT_PATTERN.search(\n                                e.detailed_reason.strip()) is not None)\n                        runtime_not_setup = (_RAY_CLUSTER_NOT_FOUND_MESSAGE\n                                             in e.error_msg)\n                        if has_new_ip_addr or runtime_not_setup:\n                            yellow = colorama.Fore.YELLOW\n                            bright = colorama.Style.BRIGHT\n                            reset = colorama.Style.RESET_ALL\n                            ux_utils.console_newline()\n                            logger.warning(\n                                f'{yellow}Failed getting cluster status despite all nodes '\n                                f'being up ({cluster_name!r}). '\n                                f'If the cluster was restarted manually, try running: '\n                                f'{reset}{bright}sky start {cluster_name}{reset} '\n                                f'{yellow}to recover from INIT status.{reset}')\n                            return False\n                        raise e\n                    # We retry for kubernetes because coreweave can have a\n                    # transient network issue.\n                    time.sleep(1)\n                    continue\n                if ready_head + ready_workers == total_nodes:\n                    return True\n                logger.debug(f'Refreshing status ({cluster_name!r}) attempt '\n                             f'{i}: ray status not showing all nodes '\n                             f'({ready_head + ready_workers}/{total_nodes});\\n'\n                             f'output:\\n{output}\\nstderr:\\n{stderr}')\n\n                # If cluster JUST started, maybe not all the nodes have shown\n                # up. Try again for a few seconds.\n                # Note: We are okay with this performance hit because it's very\n                # rare to normally hit this case. It requires:\n                # - All the instances in the cluster are up on the cloud side\n                #   (not preempted), but\n                # - The ray cluster is somehow degraded so not all instances are\n                #   showing up\n                time.sleep(1)\n\n            ray_status_details = (\n                f'{ready_head + ready_workers}/{total_nodes} ready')\n            raise RuntimeError(\n                f'Refreshing status ({cluster_name!r}): ray status not showing '\n                f'all nodes ({ready_head + ready_workers}/'\n                f'{total_nodes});\\noutput:\\n{output}\\nstderr:\\n{stderr}')\n\n        except exceptions.FetchClusterInfoError:\n            ray_status_details = 'failed to get IPs'\n            logger.debug(\n                f'Refreshing status ({cluster_name!r}) failed to get IPs.')\n        except RuntimeError as e:\n            if ray_status_details is None:\n                ray_status_details = str(e)\n            logger.debug(common_utils.format_exception(e))\n        except Exception as e:  # pylint: disable=broad-except\n            # This can be raised by `external_ssh_ports()`, due to the\n            # underlying call to kubernetes API.\n            ray_status_details = str(e)\n            logger.debug(f'Refreshing status ({cluster_name!r}) failed: ',\n                         exc_info=e)\n        return False\n\n    # Determining if the cluster is healthy (UP):\n    #\n    # For non-spot clusters: If ray status shows all nodes are healthy, it is\n    # safe to set the status to UP as starting ray is the final step of sky\n    # launch. But we found that ray status is way too slow (see NOTE below) so\n    # we always query the cloud provider first which is faster.\n    #\n    # For spot clusters: the above can be unsafe because the Ray cluster may\n    # remain healthy for a while before the cloud completely preempts the VMs.\n    # We have mitigated this by again first querying the VM state from the cloud\n    # provider.\n    cloud = handle.launched_resources.cloud\n\n    # For Slurm, skip Ray health check since it doesn't use Ray.\n    should_check_ray = cloud is not None and cloud.uses_ray()\n    if (all_nodes_up and (not should_check_ray or\n                          run_ray_status_to_check_ray_cluster_healthy()) and\n            not external_cluster_failures):\n        # NOTE: all_nodes_up calculation is fast due to calling cloud CLI;\n        # run_ray_status_to_check_all_nodes_up() is slow due to calling `ray get\n        # head-ip/worker-ips`.\n        record['status'] = status_lib.ClusterStatus.UP\n        # Add cluster event for instance status check.\n        global_user_state.add_cluster_event(\n            cluster_name,\n            status_lib.ClusterStatus.UP,\n            'All nodes up; SkyPilot runtime healthy.',\n            global_user_state.ClusterEventType.STATUS_CHANGE,\n            nop_if_duplicate=True)\n        global_user_state.add_or_update_cluster(\n            cluster_name,\n            handle,\n            requested_resources=None,\n            ready=True,\n            is_launch=False,\n            existing_cluster_hash=record['cluster_hash'])\n        return global_user_state.get_cluster_from_name(\n            cluster_name,\n            include_user_info=include_user_info,\n            summary_response=summary_response)\n\n    # All cases below are transitioning the cluster to non-UP states.\n    launched_resources = handle.launched_resources.assert_launchable()\n    if (not node_statuses and launched_resources.cloud.STATUS_VERSION >=\n            clouds.StatusVersion.SKYPILOT):\n        # Note: launched_at is set during sky launch, even on an existing\n        # cluster. This will catch the case where the cluster was terminated on\n        # the cloud and restarted by sky launch.\n        time_since_launch = time.time() - record['launched_at']\n        if (record['status'] == status_lib.ClusterStatus.INIT and\n                time_since_launch < _LAUNCH_DOUBLE_CHECK_WINDOW):\n            # It's possible the instances for this cluster were just created,\n            # and haven't appeared yet in the cloud API/console. Wait for a bit\n            # and check again. This is a best-effort leak prevention check.\n            # See https://github.com/skypilot-org/skypilot/issues/4431.\n            time.sleep(_LAUNCH_DOUBLE_CHECK_DELAY)\n            node_statuses = _query_cluster_status_via_cloud_api(\n                handle, retry_if_missing=False)\n            # Note: even if all the node_statuses are UP now, we will still\n            # consider this cluster abnormal, and its status will be INIT.\n\n    if len(node_statuses) > handle.launched_nodes:\n        # Unexpected: in the queried region more than 1 cluster with the same\n        # constructed name tag returned. This will typically not happen unless\n        # users manually create a cluster with that constructed name or there\n        # was a resource leak caused by different launch hash before #1671\n        # was merged.\n        #\n        # (Technically speaking, even if returned num nodes <= num\n        # handle.launched_nodes), not including the launch hash could mean the\n        # returned nodes contain some nodes that do not belong to the logical\n        # skypilot cluster. Doesn't seem to be a good way to handle this for\n        # now?)\n        #\n        # We have not experienced the above; adding as a safeguard.\n        #\n        # Since we failed to refresh, raise the status fetching error.\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.ClusterStatusFetchingError(\n                f'Found {len(node_statuses)} node(s) with the same cluster name'\n                f' tag in the cloud provider for cluster {cluster_name!r}, '\n                f'which should have {handle.launched_nodes} nodes. This '\n                f'normally should not happen. {colorama.Fore.RED}Please check '\n                'the cloud console and fix any possible resources leakage '\n                '(e.g., if there are any stopped nodes and they do not have '\n                'data or are unhealthy, terminate them).'\n                f'{colorama.Style.RESET_ALL}')\n    assert len(node_statuses) <= handle.launched_nodes\n\n    # If the node_statuses is empty, it should mean that all the nodes are\n    # terminated and we can set the cluster status to TERMINATED. This handles\n    # the edge case where the cluster is terminated by the user manually through\n    # the UI.\n    to_terminate = not node_statuses\n\n    # A cluster is considered \"abnormal\", if some (but not all) nodes are\n    # TERMINATED, or not all nodes are STOPPED. We check that with the following\n    # logic:\n    #   * Not all nodes are terminated and there's at least one node\n    #     terminated; or\n    #   * Any of the non-TERMINATED nodes is in a non-STOPPED status.\n    #\n    # This includes these special cases:\n    #   * All stopped are considered normal and will be cleaned up at the end\n    #     of the function.\n    #   * Some of the nodes UP should be considered abnormal, because the ray\n    #     cluster is probably down.\n    #   * The cluster is partially terminated or stopped should be considered\n    #     abnormal.\n    #   * The cluster is partially or completely in the INIT state, which means\n    #     that provisioning was interrupted. This is considered abnormal.\n    #\n    # An abnormal cluster will transition to INIT, and one of the following will happen:\n    #  (1) If the SkyPilot provisioner is used AND the head node is alive, we\n    #      will not reset the autostop setting. Because autostop is handled by\n    #      the skylet through the cloud APIs, and will continue to function\n    #      regardless of the ray cluster's health.\n    #  (2) Otherwise, we will reset the autostop setting, unless the cluster is\n    #      autostopping/autodowning.\n    some_nodes_terminated = 0 < len(node_statuses) < handle.launched_nodes\n    some_nodes_not_stopped = any(status[0] != status_lib.ClusterStatus.STOPPED\n                                 for status in node_statuses)\n    is_abnormal = (some_nodes_terminated or some_nodes_not_stopped)\n\n    if is_abnormal and not external_cluster_failures:\n        # If all nodes are up and ray cluster is healthy, we would have returned\n        # earlier. So if all_nodes_up is True and we are here, it means the ray\n        # cluster must have been unhealthy.\n        ray_cluster_unhealthy = all_nodes_up\n        status_reason = ', '.join(\n            [status[1] for status in node_statuses if status[1] is not None])\n\n        if some_nodes_terminated:\n            init_reason = 'one or more nodes terminated'\n        elif ray_cluster_unhealthy:\n            init_reason = f'ray cluster is unhealthy ({ray_status_details})'\n        elif some_nodes_not_stopped:\n            init_reason = 'some but not all nodes are stopped'\n        logger.debug('The cluster is abnormal. Setting to INIT status. '\n                     f'node_statuses: {node_statuses}')\n        if record['autostop'] >= 0:\n            is_head_node_alive = False\n            if launched_resources.cloud.PROVISIONER_VERSION >= clouds.ProvisionerVersion.SKYPILOT:\n                # Check if the head node is alive\n                try:\n                    cluster_info = _query_cluster_info_via_cloud_api(handle)\n                    is_head_node_alive = cluster_info.get_head_instance(\n                    ) is not None\n                except Exception as e:  # pylint: disable=broad-except\n                    logger.debug(\n                        f'Failed to get cluster info for {cluster_name!r}: '\n                        f'{common_utils.format_exception(e)}')\n\n            backend = get_backend_from_handle(handle)\n            if isinstance(backend, backends.CloudVmRayBackend):\n                if is_head_node_alive:\n                    logger.debug(\n                        f'Skipping autostop reset for cluster {cluster_name!r} '\n                        'because the head node is alive.')\n                elif not backend.is_definitely_autostopping(handle,\n                                                            stream_logs=False):\n                    # Friendly hint.\n                    autostop = record['autostop']\n                    maybe_down_str = ' --down' if record['to_down'] else ''\n                    noun = 'autodown' if record['to_down'] else 'autostop'\n\n                    # Reset the autostopping as the cluster is abnormal, and may\n                    # not correctly autostop. Resetting the autostop will let\n                    # the user know that the autostop may not happen to avoid\n                    # leakages from the assumption that the cluster will autostop.\n                    success = True\n                    reset_local_autostop = True\n                    try:\n                        backend.set_autostop(\n                            handle,\n                            -1,\n                            autostop_lib.DEFAULT_AUTOSTOP_WAIT_FOR,\n                            stream_logs=False)\n                    except (exceptions.CommandError,\n                            grpc.FutureTimeoutError) as e:\n                        success = False\n                        if isinstance(e, grpc.FutureTimeoutError) or (\n                                isinstance(e, exceptions.CommandError) and\n                                e.returncode == 255):\n                            word = 'autostopped' if noun == 'autostop' else 'autodowned'\n                            logger.debug(f'The cluster is likely {word}.')\n                            reset_local_autostop = False\n                    except (Exception, SystemExit) as e:  # pylint: disable=broad-except\n                        success = False\n                        logger.debug(f'Failed to reset autostop. Due to '\n                                     f'{common_utils.format_exception(e)}')\n                    if reset_local_autostop:\n                        global_user_state.set_cluster_autostop_value(\n                            handle.cluster_name, -1, to_down=False)\n\n                    if success:\n                        operation_str = (f'Canceled {noun} on the cluster '\n                                         f'{cluster_name!r}')\n                    else:\n                        operation_str = (\n                            f'Attempted to cancel {noun} on the '\n                            f'cluster {cluster_name!r} with best effort')\n                    yellow = colorama.Fore.YELLOW\n                    bright = colorama.Style.BRIGHT\n                    reset = colorama.Style.RESET_ALL\n                    ux_utils.console_newline()\n                    logger.warning(\n                        f'{yellow}{operation_str}, since it is found to be in an '\n                        f'abnormal state. To fix, try running: {reset}{bright}sky '\n                        f'start -f -i {autostop}{maybe_down_str} {cluster_name}'\n                        f'{reset}')\n                else:\n                    ux_utils.console_newline()\n                    operation_str = 'autodowning' if record[\n                        'to_down'] else 'autostopping'\n                    logger.info(\n                        f'Cluster {cluster_name!r} is {operation_str}. Setting to '\n                        'INIT status; try refresh again in a while.')\n\n        # If the user starts part of a STOPPED cluster, we still need a status\n        # to represent the abnormal status. For spot cluster, it can also\n        # represent that the cluster is partially preempted.\n        # TODO(zhwu): the definition of INIT should be audited/changed.\n        # Adding a new status UNHEALTHY for abnormal status can be a choice.\n        init_reason_regex = None\n        if not status_reason:\n            # If there is not a status reason, don't re-add (and overwrite) the\n            # event if there is already an event with the same reason which may\n            # have a status reason.\n            # Some status reason clears after a certain time (e.g. k8s events\n            # are only stored for an hour by default), so it is possible that\n            # the previous event has a status reason, but now it does not.\n            init_reason_regex = (f'^Cluster is abnormal because '\n                                 f'{re.escape(init_reason)}.*')\n        log_message = f'Cluster is abnormal because {init_reason}'\n        if status_reason:\n            log_message += f' ({status_reason})'\n        log_message += '. Transitioned to INIT.'\n        global_user_state.add_cluster_event(\n            cluster_name,\n            status_lib.ClusterStatus.INIT,\n            log_message,\n            global_user_state.ClusterEventType.STATUS_CHANGE,\n            nop_if_duplicate=True,\n            duplicate_regex=init_reason_regex)\n        global_user_state.add_or_update_cluster(\n            cluster_name,\n            handle,\n            requested_resources=None,\n            ready=False,\n            is_launch=False,\n            existing_cluster_hash=record['cluster_hash'])\n        return global_user_state.get_cluster_from_name(\n            cluster_name,\n            include_user_info=include_user_info,\n            summary_response=summary_response)\n    # Now either:\n    # (1) is_abnormal is False: either node_statuses is empty or all nodes are\n    #                           STOPPED\n    # or\n    # (2) there are external cluster failures reported by a plugin.\n\n    # If there are external cluster failures and the cluster has not been\n    # terminated on cloud (to_terminate), we can return the cluster record as is.\n    # This is because when an external failure is detected, the cluster will be\n    # marked as INIT with a reason indicating the details of the failure. So, we\n    # do not want to modify the cluster status in this function except for in the\n    # case where the cluster has been terminated on cloud, in which case we should\n    # clean up the cluster from SkyPilot's global state.\n    if external_cluster_failures and not to_terminate:\n        return global_user_state.get_cluster_from_name(\n            cluster_name,\n            include_user_info=include_user_info,\n            summary_response=summary_response)\n\n    verb = 'terminated' if to_terminate else 'stopped'\n    backend = backends.CloudVmRayBackend()\n    global_user_state.add_cluster_event(\n        cluster_name,\n        None,\n        f'All nodes {verb}, cleaning up the cluster.',\n        global_user_state.ClusterEventType.STATUS_CHANGE,\n        # This won't do anything for a terminated cluster, but it's needed for a\n        # stopped cluster.\n        nop_if_duplicate=True,\n    )\n    backend.post_teardown_cleanup(handle, terminate=to_terminate, purge=False)\n    return global_user_state.get_cluster_from_name(\n        cluster_name,\n        include_user_info=include_user_info,\n        summary_response=summary_response)\n\n\ndef _must_refresh_cluster_status(\n        record: Dict[str, Any],\n        force_refresh_statuses: Optional[Set[status_lib.ClusterStatus]]\n) -> bool:\n    force_refresh_for_cluster = (force_refresh_statuses is not None and\n                                 record['status'] in force_refresh_statuses)\n\n    use_spot = record['handle'].launched_resources.use_spot\n    has_autostop = (record['status'] != status_lib.ClusterStatus.STOPPED and\n                    record['autostop'] >= 0)\n    recently_refreshed = (record['status_updated_at'] is not None and\n                          time.time() - record['status_updated_at'] <\n                          _CLUSTER_STATUS_CACHE_DURATION_SECONDS)\n    is_stale = (use_spot or has_autostop) and not recently_refreshed\n\n    return force_refresh_for_cluster or is_stale\n\n\ndef refresh_cluster_record(\n        cluster_name: str,\n        *,\n        force_refresh_statuses: Optional[Set[status_lib.ClusterStatus]] = None,\n        cluster_lock_already_held: bool = False,\n        cluster_status_lock_timeout: int = CLUSTER_STATUS_LOCK_TIMEOUT_SECONDS,\n        include_user_info: bool = True,\n        summary_response: bool = False,\n        retry_if_missing: bool = True) -> Optional[Dict[str, Any]]:\n    \"\"\"Refresh the cluster, and return the possibly updated record.\n\n    The function will update the cached cluster status in the global state. For\n    the design of the cluster status and transition, please refer to the\n    sky/design_docs/cluster_status.md\n\n    Args:\n        cluster_name: The name of the cluster.\n        force_refresh_statuses: if specified, refresh the cluster if it has one\n          of the specified statuses. Additionally, clusters satisfying the\n          following conditions will be refreshed no matter the argument is\n          specified or not:\n            - the most latest available status update is more than\n              _CLUSTER_STATUS_CACHE_DURATION_SECONDS old, and one of:\n                1. the cluster is a spot cluster, or\n                2. cluster autostop is set and the cluster is not STOPPED.\n        cluster_lock_already_held: Whether the caller is already holding the\n          per-cluster lock. You MUST NOT set this to True if the caller does not\n          already hold the lock. If True, we will not acquire the lock before\n          updating the status. Failing to hold the lock while updating the\n          status can lead to correctness issues - e.g. an launch in-progress may\n          appear to be DOWN incorrectly. Even if this is set to False, the lock\n          may not be acquired if the status does not need to be refreshed.\n        cluster_status_lock_timeout: The timeout to acquire the per-cluster\n          lock. If timeout, the function will use the cached status. If the\n          value is <0, do not timeout (wait for the lock indefinitely). By\n          default, this is set to CLUSTER_STATUS_LOCK_TIMEOUT_SECONDS. Warning:\n          if correctness is required, you must set this to -1.\n        retry_if_missing: Whether to retry the call to the cloud api if the\n          cluster is not found when querying the live status on the cloud.\n\n    Returns:\n        If the cluster is terminated or does not exist, return None.\n        Otherwise returns the cluster record.\n\n    Raises:\n        exceptions.ClusterOwnerIdentityMismatchError: if the current user is\n          not the same as the user who created the cluster.\n        exceptions.CloudUserIdentityError: if we fail to get the current user\n          identity.\n        exceptions.ClusterStatusFetchingError: the cluster status cannot be\n          fetched from the cloud provider or there are leaked nodes causing\n          the node number larger than expected.\n    \"\"\"\n\n    ctx = context_lib.get()\n    record = global_user_state.get_cluster_from_name(\n        cluster_name,\n        include_user_info=include_user_info,\n        summary_response=summary_response)\n    if record is None:\n        return None\n    # TODO(zhwu, 05/20): switch to the specific workspace to make sure we are\n    # using the correct cloud credentials.\n    workspace = record.get('workspace', constants.SKYPILOT_DEFAULT_WORKSPACE)\n    with skypilot_config.local_active_workspace_ctx(workspace):\n        # check_owner_identity returns if the record handle is\n        # not a CloudVmRayResourceHandle\n        _check_owner_identity_with_record(cluster_name, record)\n\n        # The loop logic allows us to notice if the status was updated in the\n        # global_user_state by another process and stop trying to get the lock.\n        lock = locks.get_lock(cluster_status_lock_id(cluster_name))\n        start_time = time.perf_counter()\n\n        # Loop until we have an up-to-date status or until we acquire the lock.\n        while True:\n            # Check if the context is canceled.\n            if ctx is not None and ctx.is_canceled():\n                raise asyncio.CancelledError()\n            # Check to see if we can return the cached status.\n            if not _must_refresh_cluster_status(record, force_refresh_statuses):\n                return record\n\n            if cluster_lock_already_held:\n                return _update_cluster_status(cluster_name, record,\n                                              retry_if_missing,\n                                              include_user_info,\n                                              summary_response)\n\n            # Try to acquire the lock so we can fetch the status.\n            try:\n                with lock.acquire(blocking=False):\n                    # Check the cluster status again, since it could have been\n                    # updated between our last check and acquiring the lock.\n                    record = global_user_state.get_cluster_from_name(\n                        cluster_name,\n                        include_user_info=include_user_info,\n                        summary_response=summary_response)\n                    if record is None or not _must_refresh_cluster_status(\n                            record, force_refresh_statuses):\n                        return record\n                    # Update and return the cluster status.\n                    return _update_cluster_status(cluster_name, record,\n                                                  retry_if_missing,\n                                                  include_user_info,\n                                                  summary_response)\n\n            except locks.LockTimeout:\n                # lock.acquire() will throw a Timeout exception if the lock is not\n                # available and we have blocking=False.\n                pass\n\n            # Logic adapted from FileLock.acquire().\n            # If cluster_status_lock_time is <0, we will never hit this. No timeout.\n            # Otherwise, if we have timed out, return the cached status. This has\n            # the potential to cause correctness issues, but if so it is the\n            # caller's responsibility to set the timeout to -1.\n            if 0 <= cluster_status_lock_timeout < time.perf_counter(\n            ) - start_time:\n                logger.debug(\n                    'Refreshing status: Failed get the lock for cluster '\n                    f'{cluster_name!r}. Using the cached status.')\n                return record\n            time.sleep(lock.poll_interval)\n\n            # Refresh for next loop iteration.\n            record = global_user_state.get_cluster_from_name(\n                cluster_name,\n                include_user_info=include_user_info,\n                summary_response=summary_response)\n            if record is None:\n                return None\n\n\n@timeline.event\n@context_utils.cancellation_guard\ndef refresh_cluster_status_handle(\n    cluster_name: str,\n    *,\n    force_refresh_statuses: Optional[Set[status_lib.ClusterStatus]] = None,\n    cluster_lock_already_held: bool = False,\n    cluster_status_lock_timeout: int = CLUSTER_STATUS_LOCK_TIMEOUT_SECONDS,\n    retry_if_missing: bool = True,\n) -> Tuple[Optional[status_lib.ClusterStatus],\n           Optional[backends.ResourceHandle]]:\n    \"\"\"Refresh the cluster, and return the possibly updated status and handle.\n\n    This is a wrapper of refresh_cluster_record, which returns the status and\n    handle of the cluster.\n    Please refer to the docstring of refresh_cluster_record for the details.\n    \"\"\"\n    record = refresh_cluster_record(\n        cluster_name,\n        force_refresh_statuses=force_refresh_statuses,\n        cluster_lock_already_held=cluster_lock_already_held,\n        cluster_status_lock_timeout=cluster_status_lock_timeout,\n        include_user_info=False,\n        summary_response=True,\n        retry_if_missing=retry_if_missing)\n    if record is None:\n        return None, None\n    return record['status'], record['handle']\n\n\n# =====================================\n\n\n@typing.overload\ndef check_cluster_available(\n    cluster_name: str,\n    *,\n    operation: str,\n    check_cloud_vm_ray_backend: Literal[True] = True,\n    dryrun: bool = ...,\n) -> 'cloud_vm_ray_backend.CloudVmRayResourceHandle':\n    ...\n\n\n@typing.overload\ndef check_cluster_available(\n    cluster_name: str,\n    *,\n    operation: str,\n    check_cloud_vm_ray_backend: Literal[False],\n    dryrun: bool = ...,\n) -> backends.ResourceHandle:\n    ...\n\n\n@context_utils.cancellation_guard\ndef check_cluster_available(\n    cluster_name: str,\n    *,\n    operation: str,\n    check_cloud_vm_ray_backend: bool = True,\n    dryrun: bool = False,\n) -> backends.ResourceHandle:\n    \"\"\"Check if the cluster is available.\n\n    Raises:\n        exceptions.ClusterDoesNotExist: if the cluster does not exist.\n        exceptions.ClusterNotUpError: if the cluster is not UP.\n        exceptions.NotSupportedError: if the cluster is not based on\n          CloudVmRayBackend.\n        exceptions.ClusterOwnerIdentityMismatchError: if the current user is\n          not the same as the user who created the cluster.\n        exceptions.CloudUserIdentityError: if we fail to get the current user\n          identity.\n    \"\"\"\n    record = global_user_state.get_cluster_from_name(cluster_name,\n                                                     include_user_info=False,\n                                                     summary_response=True)\n    if dryrun:\n        assert record is not None, cluster_name\n        return record['handle']\n\n    previous_cluster_status = None\n    if record is not None:\n        previous_cluster_status = record['status']\n\n    try:\n        cluster_status, handle = refresh_cluster_status_handle(cluster_name)\n    except exceptions.ClusterStatusFetchingError as e:\n        # Failed to refresh the cluster status is not fatal error as the callers\n        # can still be done by only using ssh, but the ssh can hang if the\n        # cluster is not up (e.g., autostopped).\n\n        # We do not catch the exception for cloud identity checking for now, in\n        # order to disable all operations on clusters created by another user\n        # identity.  That will make the design simpler and easier to\n        # understand, but it might be useful to allow the user to use\n        # operations that only involve ssh (e.g., sky exec, sky logs, etc) even\n        # if the user is not the owner of the cluster.\n        ux_utils.console_newline()\n        logger.warning(\n            f'Failed to refresh the status for cluster {cluster_name!r}. It is '\n            f'not fatal, but {operation} might hang if the cluster is not up.\\n'\n            f'Detailed reason: {e}')\n        if record is None:\n            cluster_status, handle = None, None\n        else:\n            cluster_status, handle = record['status'], record['handle']\n\n    bright = colorama.Style.BRIGHT\n    reset = colorama.Style.RESET_ALL\n    if handle is None:\n        if previous_cluster_status is None:\n            error_msg = f'Cluster {cluster_name!r} does not exist.'\n        else:\n            error_msg = (f'Cluster {cluster_name!r} not found on the cloud '\n                         'provider.')\n            assert record is not None, previous_cluster_status\n            actions = []\n            if record['handle'].launched_resources.use_spot:\n                actions.append('preempted')\n            if record['autostop'] > 0 and record['to_down']:\n                actions.append('autodowned')\n            actions.append('manually terminated in console')\n            if len(actions) > 1:\n                actions[-1] = 'or ' + actions[-1]\n            actions_str = ', '.join(actions)\n            message = f' It was likely {actions_str}.'\n            if len(actions) > 1:\n                message = message.replace('likely', 'either')\n            error_msg += message\n\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.ClusterDoesNotExist(\n                f'{colorama.Fore.YELLOW}{error_msg}{reset}')\n    assert cluster_status is not None, 'handle is not None but status is None'\n    backend = get_backend_from_handle(handle)\n    if check_cloud_vm_ray_backend and not isinstance(\n            backend, backends.CloudVmRayBackend):\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.NotSupportedError(\n                f'{colorama.Fore.YELLOW}{operation.capitalize()}: skipped for '\n                f'cluster {cluster_name!r}. It is only supported by backend: '\n                f'{backends.CloudVmRayBackend.NAME}.'\n                f'{reset}')\n    if cluster_status != status_lib.ClusterStatus.UP:\n        with ux_utils.print_exception_no_traceback():\n            hint_for_init = ''\n            if cluster_status == status_lib.ClusterStatus.INIT:\n                hint_for_init = (\n                    f'{reset} Wait for a launch to finish, or use this command '\n                    f'to try to transition the cluster to UP: {bright}sky '\n                    f'start {cluster_name}{reset}')\n            raise exceptions.ClusterNotUpError(\n                f'{colorama.Fore.YELLOW}{operation.capitalize()}: skipped for '\n                f'cluster {cluster_name!r} (status: {cluster_status.value}). '\n                'It is only allowed for '\n                f'{status_lib.ClusterStatus.UP.value} clusters.'\n                f'{hint_for_init}'\n                f'{reset}',\n                cluster_status=cluster_status,\n                handle=handle)\n\n    if handle.head_ip is None:\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.ClusterNotUpError(\n                f'Cluster {cluster_name!r} has been stopped or not properly '\n                'set up. Please re-launch it with `sky start`.',\n                cluster_status=cluster_status,\n                handle=handle)\n    return handle\n\n\n# TODO(tian): Refactor to controller_utils. Current blocker: circular import.\ndef is_controller_accessible(\n    controller: controller_utils.Controllers,\n    stopped_message: str,\n    non_existent_message: Optional[str] = None,\n    exit_if_not_accessible: bool = False,\n) -> 'backends.CloudVmRayResourceHandle':\n    \"\"\"Check if the jobs/serve controller is up.\n\n    The controller is accessible when it is in UP or INIT state, and the ssh\n    connection is successful.\n\n    It can be used to check if the controller is accessible (since the autostop\n    is set for the controller) before the jobs/serve commands interact with the\n    controller.\n\n    ClusterNotUpError will be raised whenever the controller cannot be accessed.\n\n    Args:\n        type: Type of the controller.\n        stopped_message: Message to print if the controller is STOPPED.\n        non_existent_message: Message to show if the controller does not exist.\n        exit_if_not_accessible: Whether to exit directly if the controller is not\n          accessible. If False, the function will raise ClusterNotUpError.\n\n    Returns:\n        handle: The ResourceHandle of the controller.\n\n    Raises:\n        exceptions.ClusterOwnerIdentityMismatchError: if the current user is not\n          the same as the user who created the cluster.\n        exceptions.CloudUserIdentityError: if we fail to get the current user\n          identity.\n        exceptions.ClusterNotUpError: if the controller is not accessible, or\n          failed to be connected.\n    \"\"\"\n    if (managed_job_utils.is_consolidation_mode() and\n            controller == controller_utils.Controllers.JOBS_CONTROLLER\n       ) or (serve_utils.is_consolidation_mode() and\n             controller == controller_utils.Controllers.SKY_SERVE_CONTROLLER):\n        cn = 'local-controller-consolidation'\n        return backends.LocalResourcesHandle(\n            cluster_name=cn,\n            cluster_name_on_cloud=cn,\n            cluster_yaml=None,\n            launched_nodes=1,\n            launched_resources=sky.Resources(cloud=clouds.Cloud(),\n                                             instance_type=cn),\n        )\n    if non_existent_message is None:\n        non_existent_message = controller.value.default_hint_if_non_existent\n    cluster_name = controller.value.cluster_name\n    need_connection_check = False\n    controller_status, handle = None, None\n    try:\n        # Set force_refresh_statuses=[INIT] to make sure the refresh happens\n        # when the controller is INIT/UP (triggered in these statuses as the\n        # autostop is always set for the controller). The controller can be in\n        # following cases:\n        # * (UP, autostop set): it will be refreshed without force_refresh set.\n        # * (UP, no autostop): very rare (a user ctrl-c when the controller is\n        #   launching), does not matter if refresh or not, since no autostop. We\n        #   don't include UP in force_refresh_statuses to avoid overheads.\n        # * (INIT, autostop set)\n        # * (INIT, no autostop): very rare (_update_cluster_status_no_lock may\n        #   reset local autostop config), but force_refresh will make sure\n        #   status is refreshed.\n        #\n        # We avoids unnecessary costly refresh when the controller is already\n        # STOPPED. This optimization is based on the assumption that the user\n        # will not start the controller manually from the cloud console.\n        #\n        # The acquire_lock_timeout is set to 0 to avoid hanging the command when\n        # multiple jobs.launch commands are running at the same time. Our later\n        # code will check if the controller is accessible by directly checking\n        # the ssh connection to the controller, if it fails to get accurate\n        # status of the controller.\n        controller_status, handle = refresh_cluster_status_handle(\n            cluster_name,\n            force_refresh_statuses=[status_lib.ClusterStatus.INIT],\n            cluster_status_lock_timeout=0)\n    except exceptions.ClusterStatusFetchingError as e:\n        # We do not catch the exceptions related to the cluster owner identity\n        # mismatch, please refer to the comment in\n        # `backend_utils.check_cluster_available`.\n        controller_name = controller.value.name.replace(' controller', '')\n        logger.warning(\n            'Failed to get the status of the controller. It is not '\n            f'fatal, but {controller_name} commands/calls may hang or return '\n            'stale information, when the controller is not up.\\n'\n            f'  Details: {common_utils.format_exception(e, use_bracket=True)}')\n        record = global_user_state.get_cluster_from_name(\n            cluster_name, include_user_info=False, summary_response=True)\n        if record is not None:\n            controller_status, handle = record['status'], record['handle']\n            # We check the connection even if the cluster has a cached status UP\n            # to make sure the controller is actually accessible, as the cached\n            # status might be stale.\n            need_connection_check = True\n\n    error_msg = None\n    if controller_status == status_lib.ClusterStatus.STOPPED:\n        error_msg = stopped_message\n    elif controller_status is None or handle is None or handle.head_ip is None:\n        # We check the controller is STOPPED before the check for handle.head_ip\n        # None because when the controller is STOPPED, handle.head_ip can also\n        # be None, but we only want to catch the case when the controller is\n        # being provisioned at the first time and have no head_ip.\n        error_msg = non_existent_message\n    elif (controller_status == status_lib.ClusterStatus.INIT or\n          need_connection_check):\n        # Check ssh connection if (1) controller is in INIT state, or (2) we failed to fetch the\n        # status, both of which can happen when controller's status lock is held by another `sky jobs launch` or\n        # `sky serve up`. If we have controller's head_ip available and it is ssh-reachable,\n        # we can allow access to the controller.\n        ssh_credentials = ssh_credential_from_yaml(handle.cluster_yaml,\n                                                   handle.docker_user,\n                                                   handle.ssh_user)\n\n        runner = command_runner.SSHCommandRunner(node=(handle.head_ip,\n                                                       handle.head_ssh_port),\n                                                 **ssh_credentials)\n        if not runner.check_connection():\n            error_msg = controller.value.connection_error_hint\n    else:\n        assert controller_status == status_lib.ClusterStatus.UP, handle\n\n    if error_msg is not None:\n        if exit_if_not_accessible:\n            sky_logging.print(error_msg)\n            sys.exit(1)\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.ClusterNotUpError(error_msg,\n                                               cluster_status=controller_status,\n                                               handle=handle)\n    assert handle is not None and handle.head_ip is not None, (\n        handle, controller_status)\n    return handle\n\n\nclass CloudFilter(enum.Enum):\n    # Filter for all types of clouds.\n    ALL = 'all'\n    # Filter for Sky's main clouds (aws, gcp, azure, docker).\n    CLOUDS_AND_DOCKER = 'clouds-and-docker'\n    # Filter for only local clouds.\n    LOCAL = 'local'\n\n\ndef _get_glob_clusters(\n        clusters: List[str],\n        silent: bool = False,\n        workspaces_filter: Optional[Dict[str, Any]] = None) -> List[str]:\n    \"\"\"Returns a list of clusters that match the glob pattern.\"\"\"\n    glob_clusters = []\n    for cluster in clusters:\n        glob_cluster = global_user_state.get_glob_cluster_names(\n            cluster, workspaces_filter=workspaces_filter)\n        if len(glob_cluster) == 0 and not silent:\n            logger.info(f'Cluster {cluster} not found.')\n        glob_clusters.extend(glob_cluster)\n    return list(set(glob_clusters))\n\n\ndef _refresh_cluster(\n        cluster_name: str,\n        force_refresh_statuses: Optional[Set[status_lib.ClusterStatus]],\n        include_user_info: bool = True,\n        summary_response: bool = False) -> Optional[Dict[str, Any]]:\n    try:\n        record = refresh_cluster_record(\n            cluster_name,\n            force_refresh_statuses=force_refresh_statuses,\n            cluster_lock_already_held=False,\n            include_user_info=include_user_info,\n            summary_response=summary_response)\n    except (exceptions.ClusterStatusFetchingError,\n            exceptions.CloudUserIdentityError,\n            exceptions.ClusterOwnerIdentityMismatchError) as e:\n        # Do not fail the entire refresh process. The caller will\n        # handle the 'UNKNOWN' status, and collect the errors into\n        # a table.\n        record = {'status': 'UNKNOWN', 'error': e}\n    return record\n\n\ndef refresh_cluster_records() -> None:\n    \"\"\"Refreshes the status of all clusters, except managed clusters.\n\n    Used by the background status refresh daemon.\n    This function is a stripped-down version of get_clusters, with only the\n    bare bones refresh logic.\n\n    Returns:\n        None\n\n    Raises:\n        None\n    \"\"\"\n    # We force to exclude managed clusters to avoid multiple sources\n    # manipulating them. For example, SkyServe assumes the replica manager\n    # is the only source of truth for the cluster status.\n    cluster_names = set(\n        global_user_state.get_cluster_names(exclude_managed_clusters=True))\n\n    # TODO(syang): we should try not to leak\n    # request info in backend_utils.py.\n    # Refactor this to use some other info to\n    # determine if a launch is in progress.\n    cluster_names_with_launch_request = {\n        request.cluster_name for request in requests_lib.get_request_tasks(\n            req_filter=requests_lib.RequestTaskFilter(\n                status=[requests_lib.RequestStatus.RUNNING],\n                include_request_names=['sky.launch'],\n                fields=['cluster_name']))\n    }\n    cluster_names_without_launch_request = (cluster_names -\n                                            cluster_names_with_launch_request)\n\n    def _refresh_cluster_record(cluster_name):\n        return _refresh_cluster(cluster_name,\n                                force_refresh_statuses=set(\n                                    status_lib.ClusterStatus),\n                                include_user_info=False,\n                                summary_response=True)\n\n    if len(cluster_names_without_launch_request) > 0:\n        # Do not refresh the clusters that have an active launch request.\n        subprocess_utils.run_in_parallel(_refresh_cluster_record,\n                                         cluster_names_without_launch_request)\n\n\ndef _get_records_with_handle(\n        records: List[Optional[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n    \"\"\"Filter for records that have a handle\"\"\"\n    return [\n        record for record in records\n        if record is not None and record['handle'] is not None\n    ]\n\n\ndef _update_records_with_handle_info(records_with_handle: List[Dict[str, Any]],\n                                     summary_response: bool = False) -> None:\n    \"\"\"Add resource str to record\"\"\"\n    for record in records_with_handle:\n        handle = record['handle']\n        resource_str_simple, resource_str_full = (\n            resources_utils.get_readable_resources_repr(handle,\n                                                        simplified_only=False))\n        record['resources_str'] = resource_str_simple\n        record['resources_str_full'] = resource_str_full\n        if not summary_response:\n            record['cluster_name_on_cloud'] = handle.cluster_name_on_cloud\n\n\ndef get_clusters(\n    refresh: common.StatusRefreshMode,\n    cluster_names: Optional[Union[str, List[str]]] = None,\n    all_users: bool = True,\n    include_credentials: bool = False,\n    summary_response: bool = False,\n    include_handle: bool = True,\n    # Internal only:\n    # pylint: disable=invalid-name\n    _include_is_managed: bool = False,\n) -> List[Dict[str, Any]]:\n    \"\"\"Returns a list of cached or optionally refreshed cluster records.\n\n    Combs through the database (in ~/.sky/state.db) to get a list of records\n    corresponding to launched clusters (filtered by `cluster_names` if it is\n    specified). The refresh flag can be used to force a refresh of the status\n    of the clusters.\n\n    Args:\n        refresh: Whether to refresh the status of the clusters. (Refreshing will\n            set the status to STOPPED if the cluster cannot be pinged.)\n        cluster_names: If provided, only return records for the given cluster\n            names.\n        all_users: If True, return clusters from all users. If False, only\n            return clusters from the current user.\n        include_credentials: If True, include cluster ssh credentials in the\n            return value.\n        _include_is_managed: Whether to force include clusters created by the\n            controller.\n\n    Returns:\n        A list of cluster records. If the cluster does not exist or has been\n        terminated, the record will be omitted from the returned list.\n    \"\"\"\n    accessible_workspaces = workspaces_core.get_workspaces()\n    if cluster_names is not None:\n        if isinstance(cluster_names, str):\n            cluster_names = [cluster_names]\n        non_glob_cluster_names = []\n        glob_cluster_names = []\n        for cluster_name in cluster_names:\n            if ux_utils.is_glob_pattern(cluster_name):\n                glob_cluster_names.append(cluster_name)\n            else:\n                non_glob_cluster_names.append(cluster_name)\n        cluster_names = non_glob_cluster_names\n        if glob_cluster_names:\n            cluster_names += _get_glob_clusters(\n                glob_cluster_names,\n                silent=True,\n                workspaces_filter=accessible_workspaces)\n\n    exclude_managed_clusters = False\n    if not (_include_is_managed or env_options.Options.SHOW_DEBUG_INFO.get()):\n        exclude_managed_clusters = True\n    user_hashes_filter = None\n    if not all_users:\n        user_hashes_filter = {common_utils.get_current_user().id}\n    records = global_user_state.get_clusters(\n        exclude_managed_clusters=exclude_managed_clusters,\n        user_hashes_filter=user_hashes_filter,\n        workspaces_filter=accessible_workspaces,\n        cluster_names=cluster_names,\n        summary_response=summary_response)\n\n    yellow = colorama.Fore.YELLOW\n    bright = colorama.Style.BRIGHT\n    reset = colorama.Style.RESET_ALL\n\n    if cluster_names is not None:\n        record_names = {record['name'] for record in records}\n        not_found_clusters = ux_utils.get_non_matched_query(\n            cluster_names, record_names)\n        if not_found_clusters:\n            clusters_str = ', '.join(not_found_clusters)\n            logger.info(f'Cluster(s) not found: {bright}{clusters_str}{reset}.')\n\n    def _update_records_with_credentials(\n            records: List[Optional[Dict[str, Any]]]) -> None:\n        \"\"\"Add the credentials to the record.\n\n        This is useful for the client side to setup the ssh config of the\n        cluster.\n        \"\"\"\n        records_with_handle = _get_records_with_handle(records)\n        if len(records_with_handle) == 0:\n            return\n\n        handles = [record['handle'] for record in records_with_handle]\n        credentials = ssh_credentials_from_handles(handles)\n        cached_private_keys: Dict[str, str] = {}\n        for record, credential in zip(records_with_handle, credentials):\n            if not credential:\n                continue\n            ssh_private_key_path = credential.get('ssh_private_key', None)\n            if ssh_private_key_path is not None:\n                expanded_private_key_path = os.path.expanduser(\n                    ssh_private_key_path)\n                if not os.path.exists(expanded_private_key_path):\n                    success = auth_utils.create_ssh_key_files_from_db(\n                        ssh_private_key_path)\n                    if not success:\n                        # If the ssh key files are not found, we do not\n                        # update the record with credentials.\n                        logger.debug(\n                            f'SSH keys not found for cluster {record[\"name\"]} '\n                            f'at key path {ssh_private_key_path}')\n                        continue\n            else:\n                private_key_path, _ = auth_utils.get_or_generate_keys()\n                expanded_private_key_path = os.path.expanduser(private_key_path)\n            if expanded_private_key_path in cached_private_keys:\n                credential['ssh_private_key_content'] = cached_private_keys[\n                    expanded_private_key_path]\n            else:\n                with open(expanded_private_key_path, 'r',\n                          encoding='utf-8') as f:\n                    credential['ssh_private_key_content'] = f.read()\n                    cached_private_keys[expanded_private_key_path] = credential[\n                        'ssh_private_key_content']\n            record['credentials'] = credential\n\n    def _update_records_with_resources(\n        records: List[Optional[Dict[str, Any]]],) -> None:\n        \"\"\"Add the resources to the record.\"\"\"\n        for record in _get_records_with_handle(records):\n            handle = record['handle']\n            record['nodes'] = handle.launched_nodes\n            if handle.launched_resources is None:\n                # Set default values when launched_resources is None\n                record['labels'] = {}\n                continue\n            record['cloud'] = (f'{handle.launched_resources.cloud}'\n                               if handle.launched_resources.cloud else None)\n            record['region'] = (f'{handle.launched_resources.region}'\n                                if handle.launched_resources.region else None)\n            record['cpus'] = (f'{handle.launched_resources.cpus}'\n                              if handle.launched_resources.cpus else None)\n            record['memory'] = (f'{handle.launched_resources.memory}'\n                                if handle.launched_resources.memory else None)\n            record['accelerators'] = (\n                f'{handle.launched_resources.accelerators}'\n                if handle.launched_resources.accelerators else None)\n            record['labels'] = (handle.launched_resources.labels\n                                if handle.launched_resources.labels else {})\n            if not include_handle:\n                record.pop('handle', None)\n\n    # Add handle info to the records\n    _update_records_with_handle_info(_get_records_with_handle(records),\n                                     summary_response=summary_response)\n    if include_credentials:\n        _update_records_with_credentials(records)\n    if refresh == common.StatusRefreshMode.NONE:\n        # Add resources to the records\n        _update_records_with_resources(records)\n        return records\n\n    plural = 's' if len(records) > 1 else ''\n    progress = rich_progress.Progress(transient=True,\n                                      redirect_stdout=False,\n                                      redirect_stderr=False)\n    task = progress.add_task(ux_utils.spinner_message(\n        f'Refreshing status for {len(records)} cluster{plural}'),\n                             total=len(records))\n\n    if refresh == common.StatusRefreshMode.FORCE:\n        force_refresh_statuses = set(status_lib.ClusterStatus)\n    else:\n        force_refresh_statuses = None\n\n    def _refresh_cluster_record(cluster_name):\n        record = _refresh_cluster(cluster_name,\n                                  force_refresh_statuses=force_refresh_statuses,\n                                  include_user_info=True,\n                                  summary_response=summary_response)\n        # record may be None if the cluster is deleted during refresh,\n        # e.g. all the Pods of a cluster on Kubernetes have been\n        # deleted before refresh.\n        if record is not None and 'error' not in record:\n            if record['handle'] is not None:\n                _update_records_with_handle_info(\n                    [record], summary_response=summary_response)\n            if include_credentials:\n                _update_records_with_credentials([record])\n            progress.update(task, advance=1)\n        return record\n\n    cluster_names = [record['name'] for record in records]\n    # TODO(syang): we should try not to leak\n    # request info in backend_utils.py.\n    # Refactor this to use some other info to\n    # determine if a launch is in progress.\n    cluster_names_with_launch_request = {\n        request.cluster_name for request in requests_lib.get_request_tasks(\n            req_filter=requests_lib.RequestTaskFilter(\n                status=[requests_lib.RequestStatus.RUNNING],\n                include_request_names=['sky.launch'],\n                cluster_names=cluster_names,\n                fields=['cluster_name']))\n    }\n    # Preserve the index of the cluster name as it appears on \"records\"\n    cluster_names_without_launch_request = [\n        (i, cluster_name)\n        for i, cluster_name in enumerate(cluster_names)\n        if cluster_name not in cluster_names_with_launch_request\n    ]\n    # for clusters that have an active launch request, we do not refresh the status\n    updated_records = []\n    if len(cluster_names_without_launch_request) > 0:\n        with progress:\n            updated_records = subprocess_utils.run_in_parallel(\n                _refresh_cluster_record, [\n                    cluster_name\n                    for _, cluster_name in cluster_names_without_launch_request\n                ])\n    # Preserve the index of the cluster name as it appears on \"records\"\n    # before filtering for clusters being launched.\n    updated_records_dict: Dict[int, Optional[Dict[str, Any]]] = {\n        cluster_names_without_launch_request[i][0]: updated_records[i]\n        for i in range(len(cluster_names_without_launch_request))\n    }\n    # Show information for removed clusters.\n    kept_records = []\n    autodown_clusters, remaining_clusters, failed_clusters = [], [], []\n    for i, record in enumerate(records):\n        if i not in updated_records_dict:\n            # record was not refreshed, keep the original record\n            kept_records.append(record)\n            continue\n        updated_record = updated_records_dict[i]\n        if updated_record is None:\n            if record['to_down']:\n                autodown_clusters.append(record['name'])\n            else:\n                remaining_clusters.append(record['name'])\n        elif updated_record['status'] == 'UNKNOWN':\n            failed_clusters.append((record['name'], updated_record['error']))\n            # Keep the original record if the status is unknown,\n            # so that the user can still see the cluster.\n            kept_records.append(record)\n        else:\n            kept_records.append(updated_record)\n\n    if autodown_clusters:\n        plural = 's' if len(autodown_clusters) > 1 else ''\n        cluster_str = ', '.join(autodown_clusters)\n        logger.info(f'Autodowned cluster{plural}: '\n                    f'{bright}{cluster_str}{reset}')\n    if remaining_clusters:\n        plural = 's' if len(remaining_clusters) > 1 else ''\n        cluster_str = ', '.join(name for name in remaining_clusters)\n        logger.warning(f'{yellow}Cluster{plural} terminated on '\n                       f'the cloud: {reset}{bright}{cluster_str}{reset}')\n\n    if failed_clusters:\n        plural = 's' if len(failed_clusters) > 1 else ''\n        logger.warning(f'{yellow}Failed to refresh status for '\n                       f'{len(failed_clusters)} cluster{plural}:{reset}')\n        for cluster_name, e in failed_clusters:\n            logger.warning(f'  {bright}{cluster_name}{reset}: {e}')\n\n    # Add resources to the records\n    _update_records_with_resources(kept_records)\n    return kept_records\n\n\n@typing.overload\ndef get_backend_from_handle(\n    handle: 'cloud_vm_ray_backend.CloudVmRayResourceHandle'\n) -> 'cloud_vm_ray_backend.CloudVmRayBackend':\n    ...\n\n\n@typing.overload\ndef get_backend_from_handle(\n    handle: 'local_docker_backend.LocalDockerResourceHandle'\n) -> 'local_docker_backend.LocalDockerBackend':\n    ...\n\n\n@typing.overload\ndef get_backend_from_handle(\n        handle: backends.ResourceHandle) -> backends.Backend:\n    ...\n\n\ndef get_backend_from_handle(\n        handle: backends.ResourceHandle) -> backends.Backend:\n    \"\"\"Gets a Backend object corresponding to a handle.\n\n    Inspects handle type to infer the backend used for the resource.\n    \"\"\"\n    backend: backends.Backend\n    if isinstance(handle, backends.CloudVmRayResourceHandle):\n        backend = backends.CloudVmRayBackend()\n    elif isinstance(handle, backends.LocalDockerResourceHandle):\n        backend = backends.LocalDockerBackend()\n    else:\n        raise NotImplementedError(\n            f'Handle type {type(handle)} is not supported yet.')\n    return backend\n\n\ndef get_task_demands_dict(task: 'task_lib.Task') -> Dict[str, float]:\n    \"\"\"Returns the resources dict of the task.\n\n    Returns:\n        A dict of the resources of the task. The keys are the resource names\n        and the values are the number of the resources. It always contains\n        the CPU resource (to control the maximum number of tasks), and\n        optionally accelerator demands.\n    \"\"\"\n    # TODO: Custom CPU and other memory resources are not supported yet.\n    # For sky jobs/serve controller task, we set the CPU resource to a smaller\n    # value to support a larger number of managed jobs and services.\n    resources_dict = {\n        'CPU': (constants.CONTROLLER_PROCESS_CPU_DEMAND\n                if task.is_controller_task() else DEFAULT_TASK_CPU_DEMAND)\n    }\n    if task.best_resources is not None:\n        resources = task.best_resources\n    else:\n        # Task may (e.g., sky launch) or may not (e.g., sky exec) have undergone\n        # sky.optimize(), so best_resources may be None.\n        assert len(task.resources) == 1, task.resources\n        resources = list(task.resources)[0]\n    if resources is not None and resources.accelerators is not None:\n        resources_dict.update(resources.accelerators)\n    return resources_dict\n\n\ndef get_task_resources_str(task: 'task_lib.Task',\n                           is_managed_job: bool = False) -> str:\n    \"\"\"Returns the resources string of the task.\n\n    The resources string is only used as a display purpose, so we only show\n    the accelerator demands (if any). Otherwise, the CPU demand is shown.\n    \"\"\"\n    spot_str = ''\n    is_controller_task = task.is_controller_task()\n    task_cpu_demand = (str(constants.CONTROLLER_PROCESS_CPU_DEMAND)\n                       if is_controller_task else str(DEFAULT_TASK_CPU_DEMAND))\n    if is_controller_task:\n        resources_str = f'CPU:{task_cpu_demand}'\n    elif task.best_resources is not None:\n        accelerator_dict = task.best_resources.accelerators\n        if is_managed_job:\n            if task.best_resources.use_spot:\n                spot_str = '[Spot]'\n            assert task.best_resources.cpus is not None\n            task_cpu_demand = task.best_resources.cpus\n        if accelerator_dict is None:\n            resources_str = f'CPU:{task_cpu_demand}'\n        else:\n            resources_str = ', '.join(\n                f'{k}:{v}' for k, v in accelerator_dict.items())\n    else:\n        resource_accelerators = []\n        min_cpus = float('inf')\n        spot_type: Set[str] = set()\n        for resource in task.resources:\n            task_cpu_demand = '1+'\n            if resource.cpus is not None:\n                task_cpu_demand = resource.cpus\n            min_cpus = min(min_cpus, float(task_cpu_demand.strip('+ ')))\n            if resource.use_spot:\n                spot_type.add('Spot')\n            else:\n                spot_type.add('On-demand')\n\n            if resource.accelerators is None:\n                continue\n            for k, v in resource.accelerators.items():\n                resource_accelerators.append(f'{k}:{v}')\n\n        if is_managed_job:\n            if len(task.resources) > 1:\n                task_cpu_demand = f'{min_cpus}+'\n            if 'Spot' in spot_type:\n                spot_str = '|'.join(sorted(spot_type))\n                spot_str = f'[{spot_str}]'\n        if resource_accelerators:\n            resources_str = ', '.join(set(resource_accelerators))\n        else:\n            resources_str = f'CPU:{task_cpu_demand}'\n    resources_str = f'{task.num_nodes}x[{resources_str}]{spot_str}'\n    return resources_str\n\n\n# Handle ctrl-c\ndef interrupt_handler(signum, frame):\n    del signum, frame\n    subprocess_utils.kill_children_processes()\n    # Avoid using logger here, as it will print the stack trace for broken\n    # pipe, when the output is piped to another program.\n    print(f'{colorama.Style.DIM}Tip: The job will keep '\n          f'running after Ctrl-C.{colorama.Style.RESET_ALL}')\n    with ux_utils.print_exception_no_traceback():\n        raise KeyboardInterrupt(exceptions.KEYBOARD_INTERRUPT_CODE)\n\n\n# Handle ctrl-z\ndef stop_handler(signum, frame):\n    del signum, frame\n    subprocess_utils.kill_children_processes()\n    # Avoid using logger here, as it will print the stack trace for broken\n    # pipe, when the output is piped to another program.\n    print(f'{colorama.Style.DIM}Tip: The job will keep '\n          f'running after Ctrl-Z.{colorama.Style.RESET_ALL}')\n    with ux_utils.print_exception_no_traceback():\n        raise KeyboardInterrupt(exceptions.SIGTSTP_CODE)\n\n\ndef check_rsync_installed() -> None:\n    \"\"\"Checks if rsync is installed.\n\n    Raises:\n        RuntimeError: if rsync is not installed in the machine.\n    \"\"\"\n    try:\n        subprocess.run('rsync --version',\n                       shell=True,\n                       check=True,\n                       stdout=subprocess.PIPE,\n                       stderr=subprocess.PIPE)\n    except subprocess.CalledProcessError:\n        with ux_utils.print_exception_no_traceback():\n            raise RuntimeError(\n                '`rsync` is required for provisioning and'\n                ' it is not installed. For Debian/Ubuntu system, '\n                'install it with:\\n'\n                '  $ sudo apt install rsync') from None\n\n\ndef check_stale_runtime_on_remote(returncode: int, stderr: str,\n                                  cluster_name: str) -> None:\n    \"\"\"Raises RuntimeError if remote SkyPilot runtime needs to be updated.\n\n    We detect this by parsing certain backward-incompatible error messages from\n    `stderr`. Typically due to the local client version just got updated, and\n    the remote runtime is an older version.\n    \"\"\"\n    if returncode != 0:\n        if 'SkyPilot runtime is too old' in stderr:\n            with ux_utils.print_exception_no_traceback():\n                raise RuntimeError(\n                    f'{colorama.Fore.RED}SkyPilot runtime needs to be updated '\n                    f'on the remote cluster: {cluster_name}. To update, run '\n                    '(existing jobs will not be interrupted): '\n                    f'{colorama.Style.BRIGHT}sky start -f -y '\n                    f'{cluster_name}{colorama.Style.RESET_ALL}'\n                    f'\\n--- Details ---\\n{stderr.strip()}\\n') from None\n\n\ndef get_endpoints(cluster: str,\n                  port: Optional[Union[int, str]] = None,\n                  skip_status_check: bool = False) -> Dict[int, str]:\n    \"\"\"Gets the endpoint for a given cluster and port number (endpoint).\n\n    Args:\n        cluster: The name of the cluster.\n        port: The port number to get the endpoint for. If None, endpoints\n            for all ports are returned.\n        skip_status_check: Whether to skip the status check for the cluster.\n            This is useful when the cluster is known to be in a INIT state\n            and the caller wants to query the endpoints. Used by serve\n            controller to query endpoints during cluster launch when multiple\n            services may be getting launched in parallel (and as a result,\n            the controller may be in INIT status due to a concurrent launch).\n\n    Returns: A dictionary of port numbers to endpoints. If endpoint is None,\n        the dictionary will contain all ports:endpoints exposed on the cluster.\n        If the endpoint is not exposed yet (e.g., during cluster launch or\n        waiting for cloud provider to expose the endpoint), an empty dictionary\n        is returned.\n\n    Raises:\n        ValueError: if the port is invalid or the cloud provider does not\n            support querying endpoints.\n        exceptions.ClusterNotUpError: if the cluster is not in UP status.\n    \"\"\"\n    # Cast endpoint to int if it is not None\n    if port is not None:\n        try:\n            port = int(port)\n        except ValueError:\n            with ux_utils.print_exception_no_traceback():\n                raise ValueError(f'Invalid endpoint {port!r}.') from None\n    cluster_records = get_clusters(refresh=common.StatusRefreshMode.NONE,\n                                   cluster_names=[cluster],\n                                   _include_is_managed=True)\n    if not cluster_records:\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.ClusterNotUpError(\n                f'Cluster {cluster!r} not found.', cluster_status=None)\n    assert len(cluster_records) == 1, cluster_records\n    cluster_record = cluster_records[0]\n    if (not skip_status_check and\n            cluster_record['status'] != status_lib.ClusterStatus.UP):\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.ClusterNotUpError(\n                f'Cluster {cluster_record[\"name\"]!r} '\n                'is not in UP status.',\n                cluster_status=cluster_record['status'],\n                handle=cluster_record['handle'])\n    handle = cluster_record['handle']\n    if not isinstance(handle, backends.CloudVmRayResourceHandle):\n        with ux_utils.print_exception_no_traceback():\n            raise ValueError('Querying IP address is not supported '\n                             f'for cluster {cluster!r} with backend '\n                             f'{get_backend_from_handle(handle).NAME}.')\n\n    launched_resources = handle.launched_resources.assert_launchable()\n    cloud = launched_resources.cloud\n    try:\n        cloud.check_features_are_supported(\n            launched_resources, {clouds.CloudImplementationFeatures.OPEN_PORTS})\n    except exceptions.NotSupportedError:\n        with ux_utils.print_exception_no_traceback():\n            raise ValueError('Querying endpoints is not supported '\n                             f'for {cluster!r} on {cloud}.') from None\n\n    config = global_user_state.get_cluster_yaml_dict(handle.cluster_yaml)\n    port_details = provision_lib.query_ports(repr(cloud),\n                                             handle.cluster_name_on_cloud,\n                                             handle.launched_resources.ports,\n                                             head_ip=handle.head_ip,\n                                             provider_config=config['provider'])\n\n    launched_resources = handle.launched_resources.assert_launchable()\n    # Validation before returning the endpoints\n    if port is not None:\n        # If the requested endpoint was not to be exposed\n        port_set = resources_utils.port_ranges_to_set(launched_resources.ports)\n        if port not in port_set:\n            logger.warning(f'Port {port} is not exposed on '\n                           f'cluster {cluster!r}.')\n            return {}\n        # If the user requested a specific port endpoint, check if it is exposed\n        if port not in port_details:\n            error_msg = (f'Port {port} not exposed yet. '\n                         f'{_ENDPOINTS_RETRY_MESSAGE} ')\n            if launched_resources.cloud.is_same_cloud(clouds.Kubernetes()):\n                # Add Kubernetes specific debugging info\n                error_msg += kubernetes_utils.get_endpoint_debug_message(\n                    launched_resources.region)\n            logger.warning(error_msg)\n            return {}\n        return {port: port_details[port][0].url()}\n    else:\n        if not port_details:\n            # If cluster had no ports to be exposed\n            if launched_resources.ports is None:\n                logger.warning(f'Cluster {cluster!r} does not have any '\n                               'ports to be exposed.')\n                return {}\n            # Else ports have not been exposed even though they exist.\n            # In this case, ask the user to retry.\n            else:\n                error_msg = (f'No endpoints exposed yet. '\n                             f'{_ENDPOINTS_RETRY_MESSAGE} ')\n                if launched_resources.cloud.is_same_cloud(clouds.Kubernetes()):\n                    # Add Kubernetes specific debugging info\n                    error_msg += kubernetes_utils.get_endpoint_debug_message(\n                        launched_resources.region)\n                logger.warning(error_msg)\n                return {}\n        return {\n            port_num: urls[0].url() for port_num, urls in port_details.items()\n        }\n\n\ndef cluster_status_lock_id(cluster_name: str) -> str:\n    \"\"\"Get the lock ID for cluster status operations.\"\"\"\n    return f'{cluster_name}_status'\n\n\ndef cluster_file_mounts_lock_id(cluster_name: str) -> str:\n    \"\"\"Get the lock ID for cluster file mounts operations.\"\"\"\n    return f'{cluster_name}_file_mounts'\n\n\ndef workspace_lock_id(workspace_name: str) -> str:\n    \"\"\"Get the lock ID for workspace operations.\"\"\"\n    return f'{workspace_name}_workspace'\n\n\ndef cluster_tunnel_lock_id(cluster_name: str) -> str:\n    \"\"\"Get the lock ID for cluster tunnel operations.\"\"\"\n    return f'{cluster_name}_ssh_tunnel'\n\n\ndef open_ssh_tunnel(head_runner: Union[command_runner.SSHCommandRunner,\n                                       command_runner.KubernetesCommandRunner],\n                    port_forward: Tuple[int, int]) -> subprocess.Popen:\n    local_port, remote_port = port_forward\n    if isinstance(head_runner, command_runner.SSHCommandRunner):\n        # Disabling ControlMaster makes things easier to reason about\n        # with respect to resource management/ownership,\n        # as killing the process will close the tunnel too.\n        head_runner.disable_control_master = True\n        head_runner.port_forward_execute_remote_command = True\n\n    # The default connect_timeout of 1s is too short for\n    # connecting to clusters using a jump server.\n    # We use NON_INTERACTIVE mode to avoid allocating a pseudo-tty,\n    # which is counted towards non-idleness.\n    cmd: List[str] = head_runner.port_forward_command(\n        [(local_port, remote_port)],\n        connect_timeout=5,\n        ssh_mode=command_runner.SshMode.NON_INTERACTIVE)\n    if isinstance(head_runner, command_runner.SSHCommandRunner):\n        # cat so the command doesn't exit until we kill it\n        cmd += [f'\"echo {_ACK_MESSAGE} && cat\"']\n    cmd_str = ' '.join(cmd)\n    logger.debug(f'Running port forward command: {cmd_str}')\n    ssh_tunnel_proc = subprocess.Popen(cmd_str,\n                                       shell=True,\n                                       stdin=subprocess.PIPE,\n                                       stdout=subprocess.PIPE,\n                                       stderr=subprocess.PIPE,\n                                       start_new_session=True,\n                                       text=True)\n    # Wait until we receive an ack from the remote cluster or\n    # the SSH connection times out.\n    queue: queue_lib.Queue = queue_lib.Queue()\n    stdout_thread = threading.Thread(\n        target=lambda queue, stdout: queue.put(stdout.readline()),\n        args=(queue, ssh_tunnel_proc.stdout),\n        daemon=True)\n    stdout_thread.start()\n    while ssh_tunnel_proc.poll() is None:\n        try:\n            ack = queue.get_nowait()\n        except queue_lib.Empty:\n            ack = None\n            time.sleep(0.1)\n            continue\n        assert ack is not None\n        if isinstance(\n                head_runner,\n                command_runner.SSHCommandRunner) and ack == f'{_ACK_MESSAGE}\\n':\n            break\n        elif isinstance(head_runner, command_runner.KubernetesCommandRunner\n                       ) and _FORWARDING_FROM_MESSAGE in ack:\n            # On kind clusters, this error occurs if we make a request\n            # immediately after the port-forward is established on a new pod:\n            # \"Unhandled Error\" err=\"an error occurred forwarding ... -> 46590:\n            # failed to execute portforward in network namespace\n            # \"/var/run/netns/cni-...\": failed to connect to localhost:46590\n            # inside namespace \"...\", IPv4: dial tcp4 127.0.0.1:46590:\n            # connect: connection refused\n            # So we need to poll the port on the pod to check if it is open.\n            # We did not observe this with real Kubernetes clusters.\n            timeout = 5\n            port_check_cmd = (\n                # We install netcat in our ray-node container,\n                # so we can use it here.\n                # (See kubernetes-ray.yml.j2)\n                f'end=$((SECONDS+{timeout})); '\n                f'while ! nc -z -w 1 localhost {remote_port}; do '\n                'if (( SECONDS >= end )); then exit 1; fi; '\n                'sleep 0.1; '\n                'done')\n            returncode, stdout, stderr = head_runner.run(port_check_cmd,\n                                                         require_outputs=True,\n                                                         stream_logs=False)\n            if returncode != 0:\n                try:\n                    ssh_tunnel_proc.terminate()\n                    ssh_tunnel_proc.wait(timeout=5)\n                except subprocess.TimeoutExpired:\n                    ssh_tunnel_proc.kill()\n                    ssh_tunnel_proc.wait()\n                finally:\n                    error_msg = (f'Failed to check remote port {remote_port}')\n                    if stdout:\n                        error_msg += f'\\n-- stdout --\\n{stdout}\\n'\n                    raise exceptions.CommandError(returncode=returncode,\n                                                  command=cmd_str,\n                                                  error_msg=error_msg,\n                                                  detailed_reason=stderr)\n            break\n\n    if ssh_tunnel_proc.poll() is not None:\n        stdout, stderr = ssh_tunnel_proc.communicate()\n        error_msg = 'Port forward failed'\n        if stdout:\n            error_msg += f'\\n-- stdout --\\n{stdout}\\n'\n        raise exceptions.CommandError(returncode=ssh_tunnel_proc.returncode,\n                                      command=cmd_str,\n                                      error_msg=error_msg,\n                                      detailed_reason=stderr)\n    return ssh_tunnel_proc\n\n\nT = TypeVar('T')\n\n\ndef invoke_skylet_with_retries(func: Callable[..., T]) -> T:\n    \"\"\"Generic helper for making Skylet gRPC requests.\n\n    This method handles the common pattern of:\n    1. Try the gRPC request\n    2. If SSH tunnel is closed, recreate it and retry\n    \"\"\"\n    max_attempts = 5\n    backoff = common_utils.Backoff(initial_backoff=0.5)\n    last_exception: Optional[Exception] = None\n\n    for _ in range(max_attempts):\n        try:\n            return func()\n        except grpc.RpcError as e:\n            last_exception = e\n            _handle_grpc_error(e, backoff.current_backoff())\n\n    raise RuntimeError(\n        f'Failed to invoke Skylet after {max_attempts} attempts: {last_exception}'\n    ) from last_exception\n\n\ndef invoke_skylet_streaming_with_retries(\n        stream_func: Callable[..., Iterator[T]]) -> Iterator[T]:\n    \"\"\"Generic helper for making Skylet streaming gRPC requests.\"\"\"\n    max_attempts = 3\n    backoff = common_utils.Backoff(initial_backoff=0.5)\n    last_exception: Optional[Exception] = None\n\n    for _ in range(max_attempts):\n        try:\n            for response in stream_func():\n                yield response\n            return\n        except grpc.RpcError as e:\n            last_exception = e\n            _handle_grpc_error(e, backoff.current_backoff())\n\n    raise RuntimeError(\n        f'Failed to stream Skylet response after {max_attempts} attempts'\n    ) from last_exception\n\n\ndef _handle_grpc_error(e: 'grpc.RpcError', current_backoff: float) -> None:\n    if e.code() == grpc.StatusCode.INTERNAL:\n        with ux_utils.print_exception_no_traceback():\n            raise exceptions.SkyletInternalError(e.details())\n    elif e.code() == grpc.StatusCode.UNAVAILABLE:\n        time.sleep(current_backoff)\n    elif e.code() == grpc.StatusCode.UNIMPLEMENTED or e.code(\n    ) == grpc.StatusCode.UNKNOWN:\n        # Handle backwards compatibility: old server doesn't implement this RPC.\n        # Let the caller fall back to legacy execution.\n        raise exceptions.SkyletMethodNotImplementedError(\n            f'gRPC method not implemented on server, falling back to legacy execution: {e.details()}'\n        )\n    else:\n        raise e\n"
        }
    ]
}