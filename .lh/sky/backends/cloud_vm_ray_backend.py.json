{
    "sourceFile": "sky/backends/cloud_vm_ray_backend.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1768543313347,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1768543313347,
            "name": "Commit-0",
            "content": "\"\"\"Backend: runs on cloud virtual machines, managed by Ray.\"\"\"\nimport copy\nimport dataclasses\nimport enum\nimport json\nimport math\nimport os\nimport pathlib\nimport random\nimport re\nimport shlex\nimport signal\nimport socket\nimport subprocess\nimport sys\nimport tempfile\nimport textwrap\nimport threading\nimport time\nimport typing\nfrom typing import (Any, Callable, Dict, Iterable, Iterator, List, Optional,\n                    Set, Tuple, Union)\n\nimport colorama\nimport psutil\n\nfrom sky import backends\nfrom sky import catalog\nfrom sky import check as sky_check\nfrom sky import cloud_stores\nfrom sky import clouds\nfrom sky import exceptions\nfrom sky import global_user_state\nfrom sky import jobs as managed_jobs\nfrom sky import optimizer\nfrom sky import provision as provision_lib\nfrom sky import resources as resources_lib\nfrom sky import sky_logging\nfrom sky import skypilot_config\nfrom sky import task as task_lib\nfrom sky.adaptors import common as adaptors_common\nfrom sky.backends import backend_utils\nfrom sky.backends import task_codegen\nfrom sky.backends import wheel_utils\nfrom sky.clouds import cloud as sky_cloud\nfrom sky.clouds.utils import gcp_utils\nfrom sky.data import data_utils\nfrom sky.data import storage as storage_lib\nfrom sky.provision import common as provision_common\nfrom sky.provision import instance_setup\nfrom sky.provision import metadata_utils\nfrom sky.provision import provisioner\nfrom sky.provision.kubernetes import config as config_lib\nfrom sky.provision.kubernetes import utils as kubernetes_utils\nfrom sky.serve import constants as serve_constants\nfrom sky.server.requests import requests as requests_lib\nfrom sky.skylet import autostop_lib\nfrom sky.skylet import constants\nfrom sky.skylet import job_lib\nfrom sky.skylet import log_lib\nfrom sky.usage import usage_lib\nfrom sky.utils import annotations\nfrom sky.utils import cluster_utils\nfrom sky.utils import command_runner\nfrom sky.utils import common\nfrom sky.utils import common_utils\nfrom sky.utils import context_utils\nfrom sky.utils import controller_utils\nfrom sky.utils import directory_utils\nfrom sky.utils import env_options\nfrom sky.utils import lock_events\nfrom sky.utils import locks\nfrom sky.utils import log_utils\nfrom sky.utils import message_utils\nfrom sky.utils import registry\nfrom sky.utils import resources_utils\nfrom sky.utils import rich_utils\nfrom sky.utils import status_lib\nfrom sky.utils import subprocess_utils\nfrom sky.utils import timeline\nfrom sky.utils import ux_utils\nfrom sky.utils import volume as volume_lib\nfrom sky.utils import yaml_utils\nfrom sky.utils.plugin_extensions import ExternalFailureSource\n\nif typing.TYPE_CHECKING:\n    import grpc\n\n    from sky import dag\n    from sky.schemas.generated import autostopv1_pb2\n    from sky.schemas.generated import autostopv1_pb2_grpc\n    from sky.schemas.generated import jobsv1_pb2\n    from sky.schemas.generated import jobsv1_pb2_grpc\n    from sky.schemas.generated import managed_jobsv1_pb2\n    from sky.schemas.generated import managed_jobsv1_pb2_grpc\n    from sky.schemas.generated import servev1_pb2\n    from sky.schemas.generated import servev1_pb2_grpc\nelse:\n    # To avoid requiring grpcio to be installed on the client side.\n    grpc = adaptors_common.LazyImport(\n        'grpc',\n        # https://github.com/grpc/grpc/issues/37642 to avoid spam in console\n        set_loggers=lambda: os.environ.update({'GRPC_VERBOSITY': 'NONE'})\n        if not env_options.Options.SHOW_DEBUG_INFO.get() else None)\n    autostopv1_pb2 = adaptors_common.LazyImport(\n        'sky.schemas.generated.autostopv1_pb2')\n    autostopv1_pb2_grpc = adaptors_common.LazyImport(\n        'sky.schemas.generated.autostopv1_pb2_grpc')\n    jobsv1_pb2 = adaptors_common.LazyImport('sky.schemas.generated.jobsv1_pb2')\n    jobsv1_pb2_grpc = adaptors_common.LazyImport(\n        'sky.schemas.generated.jobsv1_pb2_grpc')\n    servev1_pb2 = adaptors_common.LazyImport(\n        'sky.schemas.generated.servev1_pb2')\n    servev1_pb2_grpc = adaptors_common.LazyImport(\n        'sky.schemas.generated.servev1_pb2_grpc')\n    managed_jobsv1_pb2 = adaptors_common.LazyImport(\n        'sky.schemas.generated.managed_jobsv1_pb2')\n    managed_jobsv1_pb2_grpc = adaptors_common.LazyImport(\n        'sky.schemas.generated.managed_jobsv1_pb2_grpc')\n\nPath = str\n\nSKY_REMOTE_APP_DIR = backend_utils.SKY_REMOTE_APP_DIR\nSKY_REMOTE_WORKDIR = constants.SKY_REMOTE_WORKDIR\n\nlogger = sky_logging.init_logger(__name__)\n\n_PATH_SIZE_MEGABYTES_WARN_THRESHOLD = 256\n\n# Timeout (seconds) for provision progress: if in this duration no new nodes\n# are launched, abort and failover.\n_NODES_LAUNCHING_PROGRESS_TIMEOUT = {\n    clouds.AWS: 90,\n    clouds.Azure: 90,\n    clouds.GCP: 240,\n    clouds.Lambda: 300,\n    clouds.IBM: 160,\n    clouds.OCI: 300,\n    clouds.Paperspace: 600,\n    clouds.Kubernetes: 300,\n    clouds.Shadeform: 300,\n    clouds.Vsphere: 240,\n}\n\n# Time gap between retries after failing to provision in all possible places.\n# Used only if --retry-until-up is set.\n_RETRY_UNTIL_UP_INIT_GAP_SECONDS = 30\n\n# The maximum retry count for fetching IP address.\n_FETCH_IP_MAX_ATTEMPTS = 3\n\n# How many times to query the cloud provider to make sure instances are\n# stopping/terminating, and how long to wait between each query.\n_TEARDOWN_WAIT_MAX_ATTEMPTS = 10\n_TEARDOWN_WAIT_BETWEEN_ATTEMPS_SECONDS = 1\n\n_TEARDOWN_FAILURE_MESSAGE = (\n    f'\\n{colorama.Fore.RED}Failed to terminate '\n    '{cluster_name}. {extra_reason}'\n    'If you want to ignore this error and remove the cluster '\n    'from the status table, use `sky down --purge`.'\n    f'{colorama.Style.RESET_ALL}\\n'\n    '**** STDOUT ****\\n'\n    '{stdout}\\n'\n    '**** STDERR ****\\n'\n    '{stderr}')\n\n_TEARDOWN_PURGE_WARNING = (\n    f'{colorama.Fore.YELLOW}'\n    'WARNING: Received non-zero exit code from {reason}. '\n    'Make sure resources are manually deleted.\\n'\n    'Details: {details}'\n    f'{colorama.Style.RESET_ALL}')\n\n_RSYNC_NOT_FOUND_MESSAGE = (\n    '`rsync` command is not found in the specified image. '\n    'Please use an image with rsync installed.')\n\n_TPU_NOT_FOUND_ERROR = 'ERROR: (gcloud.compute.tpus.delete) NOT_FOUND'\n\n_MAX_RAY_UP_RETRY = 5\n\n# Number of retries for getting zones.\n_MAX_GET_ZONE_RETRY = 3\n\n_JOB_ID_PATTERN = re.compile(r'Job ID: ([0-9]+)')\n_LOG_DIR_PATTERN = re.compile(r'Log Dir: ([^ ]+)')\n\n# Path to the monkey-patched ray up script.\n# We don't do import then __file__ because that script needs to be filled in\n# (so import would fail).\n_RAY_UP_WITH_MONKEY_PATCHED_HASH_LAUNCH_CONF_PATH = (\n    pathlib.Path(directory_utils.get_sky_dir()) / 'backends' /\n    'monkey_patches' / 'monkey_patch_ray_up.py')\n\n_EXCEPTION_MSG_AND_RETURNCODE_FOR_DUMP_INLINE_SCRIPT = [\n    ('too long', 255),\n    ('request-uri too large', 1),\n    ('request header fields too large', 1),\n    ('400 bad request', 1),  # CloudFlare 400 error\n]\n\n_RESOURCES_UNAVAILABLE_LOG = (\n    'Reasons for provision failures (for details, please check the log above):')\n\n# Number of seconds to wait locking the cluster before communicating with user.\n_CLUSTER_LOCK_TIMEOUT = 5.0\n\n\ndef _is_message_too_long(returncode: int,\n                         output: Optional[str] = None,\n                         file_path: Optional[str] = None) -> bool:\n    \"\"\"Check if the message sent to the remote is too long.\n\n    We use inline script to run the setup or run command, i.e. the script will\n    be part of the message sent to the remote cluster. There is a chance that\n    the command is too long, when people has very long run or setup commands, or\n    there is a cloudflare proxy in front of the remote blocking the long\n    message. Several common causes are:\n    - SSH returning: `too long` in the error message.\n    - Cloudflare proxy returning: `414 Request-URI Too Large` or\n      `431 Request Header Fields Too Large` error.\n\n    We use a general length limit check before but it could be inaccurate on\n    some systems, e.g. cloudflare proxy, so this is necessary.\n\n    Args:\n        returncode: The return code of the setup command.\n        output: The output of the setup command.\n        file_path: The path to the setup log file.\n    \"\"\"\n    assert (output is None) != (file_path is None), (\n        'Either output or file_path must be provided.', output, file_path)\n    to_check = []\n    for (match_str,\n         desired_rc) in _EXCEPTION_MSG_AND_RETURNCODE_FOR_DUMP_INLINE_SCRIPT:\n        if desired_rc == returncode:\n            to_check.append(match_str)\n    if not to_check:\n        return False\n\n    def _check_output_for_match_str(output: str) -> bool:\n        for match_str in to_check:\n            if match_str.lower() in output.lower():\n                return True\n        return False\n\n    if file_path is not None:\n        try:\n            with open(os.path.expanduser(file_path), 'r',\n                      encoding='utf-8') as f:\n                content = f.read()\n                return _check_output_for_match_str(content)\n        except Exception as e:  # pylint: disable=broad-except\n            # We don't crash the setup if we cannot read the log file.\n            # Instead, we should retry the setup with dumping the script\n            # to a file to be safe.\n            logger.debug(f'Failed to read setup log file {file_path}: {e}')\n            return True\n    else:\n        assert output is not None, (output, file_path)\n        return _check_output_for_match_str(output)\n\n\ndef _get_cluster_config_template(cloud):\n    cloud_to_template = {\n        clouds.AWS: 'aws-ray.yml.j2',\n        clouds.Azure: 'azure-ray.yml.j2',\n        clouds.Cudo: 'cudo-ray.yml.j2',\n        clouds.GCP: 'gcp-ray.yml.j2',\n        clouds.Lambda: 'lambda-ray.yml.j2',\n        clouds.IBM: 'ibm-ray.yml.j2',\n        clouds.SCP: 'scp-ray.yml.j2',\n        clouds.Slurm: 'slurm-ray.yml.j2',\n        clouds.OCI: 'oci-ray.yml.j2',\n        clouds.Paperspace: 'paperspace-ray.yml.j2',\n        clouds.PrimeIntellect: 'primeintellect-ray.yml.j2',\n        clouds.DO: 'do-ray.yml.j2',\n        clouds.RunPod: 'runpod-ray.yml.j2',\n        clouds.Kubernetes: 'kubernetes-ray.yml.j2',\n        clouds.SSH: 'kubernetes-ray.yml.j2',\n        clouds.Shadeform: 'shadeform-ray.yml.j2',\n        clouds.Vsphere: 'vsphere-ray.yml.j2',\n        clouds.Vast: 'vast-ray.yml.j2',\n        clouds.Fluidstack: 'fluidstack-ray.yml.j2',\n        clouds.Nebius: 'nebius-ray.yml.j2',\n        clouds.Hyperbolic: 'hyperbolic-ray.yml.j2',\n        clouds.Seeweb: 'seeweb-ray.yml.j2',\n        clouds.Yotta: 'yotta-ray.yml.j2',\n    }\n    return cloud_to_template[type(cloud)]\n\n\ndef write_ray_up_script_with_patched_launch_hash_fn(\n    cluster_config_path: Optional[str],\n    ray_up_kwargs: Dict[str, bool],\n) -> str:\n    \"\"\"Writes a Python script that runs `ray up` with our launch hash func.\n\n    Our patched launch hash has one difference from the non-patched version: it\n    does not include any `ssh_proxy_command` under `auth` as part of the hash\n    calculation.\n    \"\"\"\n    with open(_RAY_UP_WITH_MONKEY_PATCHED_HASH_LAUNCH_CONF_PATH,\n              'r',\n              encoding='utf-8') as f:\n        ray_up_no_restart_script = f.read().format(\n            ray_yaml_path=repr(cluster_config_path),\n            ray_up_kwargs=ray_up_kwargs)\n    with tempfile.NamedTemporaryFile('w',\n                                     prefix='skypilot_ray_up_',\n                                     suffix='.py',\n                                     delete=False) as f:\n        f.write(ray_up_no_restart_script)\n        logger.debug(f'`ray up` script: {f.name}')\n    return f.name\n\n\nclass GangSchedulingStatus(enum.Enum):\n    \"\"\"Enum for gang scheduling status.\"\"\"\n    CLUSTER_READY = 0\n    GANG_FAILED = 1\n    HEAD_FAILED = 2\n\n\ndef _add_to_blocked_resources(blocked_resources: Set['resources_lib.Resources'],\n                              resources: 'resources_lib.Resources') -> None:\n    # If the resources is already blocked by blocked_resources, we don't need to\n    # add it again to avoid duplicated entries.\n    for r in blocked_resources:\n        if resources.should_be_blocked_by(r):\n            return\n    blocked_resources.add(resources)\n\n\nclass FailoverCloudErrorHandlerV1:\n    \"\"\"Handles errors during provisioning and updates the blocked_resources.\n\n    Deprecated: Newly added cloud should use the FailoverCloudErrorHandlerV2,\n    which is more robust by parsing the errors raised by the cloud's API in a\n    more structured way, instead of directly based on the stdout and stderr.\n    \"\"\"\n\n    @staticmethod\n    def _handle_errors(stdout: str, stderr: str,\n                       is_error_str_known: Callable[[str], bool]) -> List[str]:\n        stdout_splits = stdout.split('\\n')\n        stderr_splits = stderr.split('\\n')\n        errors = [\n            s.strip()\n            for s in stdout_splits + stderr_splits\n            if is_error_str_known(s.strip())\n        ]\n        if errors:\n            return errors\n        if 'rsync: command not found' in stderr:\n            with ux_utils.print_exception_no_traceback():\n                e = RuntimeError(_RSYNC_NOT_FOUND_MESSAGE)\n                setattr(e, 'detailed_reason',\n                        f'stdout: {stdout}\\nstderr: {stderr}')\n                raise e\n        detailed_reason = textwrap.dedent(f\"\"\"\\\n        ====== stdout ======\n        {stdout}\n        ====== stderr ======\n        {stderr}\n        \"\"\")\n        logger.info('====== stdout ======')\n        print(stdout)\n        logger.info('====== stderr ======')\n        print(stderr)\n        with ux_utils.print_exception_no_traceback():\n            e = RuntimeError('Errors occurred during provision; '\n                             'check logs above.')\n            setattr(e, 'detailed_reason', detailed_reason)\n            raise e\n\n    @staticmethod\n    def _ibm_handler(blocked_resources: Set['resources_lib.Resources'],\n                     launchable_resources: 'resources_lib.Resources',\n                     region: 'clouds.Region',\n                     zones: Optional[List['clouds.Zone']], stdout: str,\n                     stderr: str):\n\n        errors = FailoverCloudErrorHandlerV1._handle_errors(\n            stdout, stderr,\n            lambda x: 'ERR' in x.strip() or 'PANIC' in x.strip())\n\n        logger.warning(f'Got error(s) on IBM cluster, in {region.name}:')\n        messages = '\\n\\t'.join(errors)\n        style = colorama.Style\n        logger.warning(f'{style.DIM}\\t{messages}{style.RESET_ALL}')\n\n        for zone in zones:  # type: ignore[union-attr]\n            _add_to_blocked_resources(blocked_resources,\n                                      launchable_resources.copy(zone=zone.name))\n\n    @staticmethod\n    def update_blocklist_on_error(\n            blocked_resources: Set['resources_lib.Resources'],\n            launchable_resources: 'resources_lib.Resources',\n            region: 'clouds.Region', zones: Optional[List['clouds.Zone']],\n            stdout: Optional[str], stderr: Optional[str]) -> bool:\n        \"\"\"Handles cloud-specific errors and updates the block list.\n\n        This parses textual stdout/stderr because we don't directly use the\n        underlying clouds' SDKs.  If we did that, we could catch proper\n        exceptions instead.\n\n        Returns:\n          definitely_no_nodes_launched: bool, True if definitely no nodes\n            launched (e.g., due to VPC errors we have never sent the provision\n            request), False otherwise.\n        \"\"\"\n        assert launchable_resources.region == region.name, (\n            launchable_resources, region)\n        if stdout is None:\n            # Gang scheduling failure (head node is definitely up, but some\n            # workers' provisioning failed).  Simply block the zones.\n            assert stderr is None, stderr\n            if zones is not None:\n                for zone in zones:\n                    _add_to_blocked_resources(\n                        blocked_resources,\n                        launchable_resources.copy(zone=zone.name))\n            return False  # definitely_no_nodes_launched\n        assert stdout is not None and stderr is not None, (stdout, stderr)\n\n        # TODO(zongheng): refactor into Cloud interface?\n        cloud = launchable_resources.cloud\n        handler = getattr(FailoverCloudErrorHandlerV1,\n                          f'_{str(cloud).lower()}_handler')\n        if handler is None:\n            raise NotImplementedError(\n                f'Cloud {cloud} unknown, or has not added '\n                'support for parsing and handling provision failures. '\n                'Please implement a handler in FailoverCloudErrorHandlerV1 when'\n                'ray-autoscaler-based provisioner is used for the cloud.')\n        handler(blocked_resources, launchable_resources, region, zones, stdout,\n                stderr)\n\n        stdout_splits = stdout.split('\\n')\n        stderr_splits = stderr.split('\\n')\n        # Determining whether head node launch *may* have been requested based\n        # on outputs is tricky. We are conservative here by choosing an \"early\n        # enough\" output line in the following:\n        # https://github.com/ray-project/ray/blob/03b6bc7b5a305877501110ec04710a9c57011479/python/ray/autoscaler/_private/commands.py#L704-L737  # pylint: disable=line-too-long\n        # This is okay, because we mainly want to use the return value of this\n        # func to skip cleaning up never-launched clusters that encountered VPC\n        # errors; their launch should not have printed any such outputs.\n        head_node_launch_may_have_been_requested = any(\n            'Acquiring an up-to-date head node' in line\n            for line in stdout_splits + stderr_splits)\n        # If head node request has definitely not been sent (this happens when\n        # there are errors during node provider \"bootstrapping\", e.g.,\n        # VPC-not-found errors), then definitely no nodes are launched.\n        definitely_no_nodes_launched = (\n            not head_node_launch_may_have_been_requested)\n\n        return definitely_no_nodes_launched\n\n\nclass FailoverCloudErrorHandlerV2:\n    \"\"\"Handles errors during provisioning and updates the blocked_resources.\n\n    This is a more robust version of FailoverCloudErrorHandlerV1. V2 parses\n    the errors raised by the cloud's API using the exception, instead of the\n    stdout and stderr.\n    \"\"\"\n\n    @staticmethod\n    def _azure_handler(blocked_resources: Set['resources_lib.Resources'],\n                       launchable_resources: 'resources_lib.Resources',\n                       region: 'clouds.Region', zones: List['clouds.Zone'],\n                       err: Exception):\n        del region, zones  # Unused.\n        if '(ReadOnlyDisabledSubscription)' in str(err):\n            logger.info(\n                f'{colorama.Style.DIM}Azure subscription is read-only. '\n                'Skip provisioning on Azure. Please check the subscription set '\n                'with az account set -s <subscription_id>.'\n                f'{colorama.Style.RESET_ALL}')\n            _add_to_blocked_resources(\n                blocked_resources,\n                resources_lib.Resources(cloud=clouds.Azure()))\n        elif 'ClientAuthenticationError' in str(err):\n            _add_to_blocked_resources(\n                blocked_resources,\n                resources_lib.Resources(cloud=clouds.Azure()))\n        else:\n            _add_to_blocked_resources(blocked_resources,\n                                      launchable_resources.copy(zone=None))\n\n    @staticmethod\n    def _gcp_handler(blocked_resources: Set['resources_lib.Resources'],\n                     launchable_resources: 'resources_lib.Resources',\n                     region: 'clouds.Region', zones: List['clouds.Zone'],\n                     err: Exception):\n        assert zones and len(zones) == 1, zones\n        zone = zones[0]\n\n        if not isinstance(err, provision_common.ProvisionerError):\n            logger.warning(f'{colorama.Style.DIM}Got an unparsed error: {err}; '\n                           f'blocking resources by its zone {zone.name}'\n                           f'{colorama.Style.RESET_ALL}')\n            _add_to_blocked_resources(blocked_resources,\n                                      launchable_resources.copy(zone=zone.name))\n            return\n        errors = err.errors\n\n        for e in errors:\n            code = e['code']\n            message = e['message']\n\n            if code in ('QUOTA_EXCEEDED', 'quotaExceeded'):\n                if '\\'GPUS_ALL_REGIONS\\' exceeded' in message:\n                    # Global quota.  All regions in GCP will fail.  Ex:\n                    # Quota 'GPUS_ALL_REGIONS' exceeded.  Limit: 1.0\n                    # globally.\n                    # This skip is only correct if we implement \"first\n                    # retry the region/zone of an existing cluster with the\n                    # same name\" correctly.\n                    _add_to_blocked_resources(\n                        blocked_resources,\n                        launchable_resources.copy(region=None, zone=None))\n                else:\n                    # Per region.  Ex: Quota 'CPUS' exceeded.  Limit: 24.0\n                    # in region us-west1.\n                    _add_to_blocked_resources(\n                        blocked_resources, launchable_resources.copy(zone=None))\n            elif code in [\n                    'ZONE_RESOURCE_POOL_EXHAUSTED',\n                    'ZONE_RESOURCE_POOL_EXHAUSTED_WITH_DETAILS',\n                    'UNSUPPORTED_OPERATION',\n                    'insufficientCapacity',\n            ]:  # Per zone.\n                # Return codes can be found at https://cloud.google.com/compute/docs/troubleshooting/troubleshooting-vm-creation # pylint: disable=line-too-long\n                # However, UNSUPPORTED_OPERATION is observed empirically\n                # when VM is preempted during creation.  This seems to be\n                # not documented by GCP.\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(zone=zone.name))\n            elif code in ['RESOURCE_NOT_READY']:\n                # This code is returned when the VM is still STOPPING.\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(zone=zone.name))\n            elif code in ['RESOURCE_OPERATION_RATE_EXCEEDED']:\n                # This code can happen when the VM is being created with a\n                # machine image, and the VM and the machine image are on\n                # different zones. We already have the retry when calling the\n                # insert API, but if it still fails, we should block the zone\n                # to avoid infinite retry.\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(zone=zone.name))\n            elif code in [3, 8, 9]:\n                # Error code 3 means TPU is preempted during creation.\n                # Example:\n                # {'code': 3, 'message': 'Cloud TPU received a bad request. update is not supported while in state PREEMPTED [EID: 0x73013519f5b7feb2]'} # pylint: disable=line-too-long\n                # Error code 8 means TPU resources is out of\n                # capacity. Example:\n                # {'code': 8, 'message': 'There is no more capacity in the zone \"europe-west4-a\"; you can try in another zone where Cloud TPU Nodes are offered (see https://cloud.google.com/tpu/docs/regions) [EID: 0x1bc8f9d790be9142]'} # pylint: disable=line-too-long\n                # Error code 9 means TPU resources is insufficient reserved\n                # capacity. Example:\n                # {'code': 9, 'message': 'Insufficient reserved capacity. Contact customer support to increase your reservation. [EID: 0x2f8bc266e74261a]'} # pylint: disable=line-too-long\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(zone=zone.name))\n            elif code == 'RESOURCE_NOT_FOUND':\n                # https://github.com/skypilot-org/skypilot/issues/1797\n                # In the inner provision loop we have used retries to\n                # recover but failed. This indicates this zone is most\n                # likely out of capacity. The provision loop will terminate\n                # any potentially live VMs before moving onto the next\n                # zone.\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(zone=zone.name))\n            elif code == 'VPC_NOT_FOUND':\n                # User has specified a VPC that does not exist. On GCP, VPC is\n                # global. So we skip the entire cloud.\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(region=None, zone=None))\n            elif code == 'SUBNET_NOT_FOUND_FOR_VPC':\n                if (launchable_resources.accelerators is not None and any(\n                        acc.lower().startswith('tpu-v4')\n                        for acc in launchable_resources.accelerators.keys()) and\n                        region.name == 'us-central2'):\n                    # us-central2 is a TPU v4 only region. The subnet for\n                    # this region may not exist when the user does not have\n                    # the TPU v4 quota. We should skip this region.\n                    logger.warning('Please check if you have TPU v4 quotas '\n                                   f'in {region.name}.')\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(region=region.name, zone=None))\n            elif code == 'type.googleapis.com/google.rpc.QuotaFailure':\n                # TPU VM pod specific error.\n                if 'in region' in message:\n                    # Example:\n                    # \"Quota 'TPUV2sPreemptiblePodPerProjectPerRegionForTPUAPI'\n                    # exhausted. Limit 32 in region europe-west4\"\n                    _add_to_blocked_resources(\n                        blocked_resources,\n                        launchable_resources.copy(region=region.name,\n                                                  zone=None))\n                elif 'in zone' in message:\n                    # Example:\n                    # \"Quota 'TPUV2sPreemptiblePodPerProjectPerZoneForTPUAPI'\n                    # exhausted. Limit 32 in zone europe-west4-a\"\n                    _add_to_blocked_resources(\n                        blocked_resources,\n                        launchable_resources.copy(zone=zone.name))\n\n            elif 'Requested disk size cannot be smaller than the image size' in message:\n                logger.info('Skipping all regions due to disk size issue.')\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(region=None, zone=None))\n            elif 'Policy update access denied.' in message or code == 'IAM_PERMISSION_DENIED':\n                logger.info(\n                    'Skipping all regions due to service account not '\n                    'having the required permissions and the user '\n                    'account does not have enough permission to '\n                    'update it. Please contact your administrator and '\n                    'check out: https://docs.skypilot.co/en/latest/cloud-setup/cloud-permissions/gcp.html\\n'  # pylint: disable=line-too-long\n                    f'Details: {message}')\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(region=None, zone=None))\n            elif 'is not found or access is unauthorized' in message:\n                # Parse HttpError for unauthorized regions. Example:\n                # googleapiclient.errors.HttpError: <HttpError 403 when requesting ... returned \"Location us-east1-d is not found or access is unauthorized.\". # pylint: disable=line-too-long\n                # Details: \"Location us-east1-d is not found or access is\n                # unauthorized.\">\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(zone=zone.name))\n            else:\n                logger.debug('Got unparsed error blocking resources by zone: '\n                             f'{e}.')\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(zone=zone.name))\n\n    @staticmethod\n    def _lambda_handler(blocked_resources: Set['resources_lib.Resources'],\n                        launchable_resources: 'resources_lib.Resources',\n                        region: 'clouds.Region',\n                        zones: Optional[List['clouds.Zone']], error: Exception):\n        output = str(error)\n        # Sometimes, lambda cloud error will list available regions.\n        if output.find('Regions with capacity available:') != -1:\n            for r in catalog.regions('lambda'):\n                if output.find(r.name) == -1:\n                    _add_to_blocked_resources(\n                        blocked_resources,\n                        launchable_resources.copy(region=r.name, zone=None))\n        else:\n            FailoverCloudErrorHandlerV2._default_handler(\n                blocked_resources, launchable_resources, region, zones, error)\n\n    @staticmethod\n    def _aws_handler(blocked_resources: Set['resources_lib.Resources'],\n                     launchable_resources: 'resources_lib.Resources',\n                     region: 'clouds.Region',\n                     zones: Optional[List['clouds.Zone']],\n                     error: Exception) -> None:\n        logger.info(f'AWS handler error: {error}')\n        # Block AWS if the credential has expired.\n        if isinstance(error, exceptions.InvalidCloudCredentials):\n            _add_to_blocked_resources(\n                blocked_resources, resources_lib.Resources(cloud=clouds.AWS()))\n        else:\n            FailoverCloudErrorHandlerV2._default_handler(\n                blocked_resources, launchable_resources, region, zones, error)\n\n    @staticmethod\n    def _scp_handler(blocked_resources: Set['resources_lib.Resources'],\n                     launchable_resources: 'resources_lib.Resources',\n                     region: 'clouds.Region',\n                     zones: Optional[List['clouds.Zone']],\n                     error: Exception) -> None:\n        logger.info(f'SCP handler error: {error}')\n        # Block SCP if the credential has expired.\n        if isinstance(error, exceptions.InvalidCloudCredentials):\n            _add_to_blocked_resources(\n                blocked_resources, resources_lib.Resources(cloud=clouds.SCP()))\n        else:\n            FailoverCloudErrorHandlerV2._default_handler(\n                blocked_resources, launchable_resources, region, zones, error)\n\n    @staticmethod\n    def _default_handler(blocked_resources: Set['resources_lib.Resources'],\n                         launchable_resources: 'resources_lib.Resources',\n                         region: 'clouds.Region',\n                         zones: Optional[List['clouds.Zone']],\n                         error: Exception) -> None:\n        \"\"\"Handles cloud-specific errors and updates the block list.\"\"\"\n        del region  # Unused.\n        logger.debug(\n            f'Got error(s) in {launchable_resources.cloud}:'\n            f'{common_utils.format_exception(error, use_bracket=True)}')\n        if zones is None:\n            _add_to_blocked_resources(blocked_resources,\n                                      launchable_resources.copy(zone=None))\n        else:\n            for zone in zones:\n                _add_to_blocked_resources(\n                    blocked_resources,\n                    launchable_resources.copy(zone=zone.name))\n\n    @staticmethod\n    def update_blocklist_on_error(\n            blocked_resources: Set['resources_lib.Resources'],\n            launchable_resources: 'resources_lib.Resources',\n            region: 'clouds.Region', zones: Optional[List['clouds.Zone']],\n            error: Exception) -> None:\n        \"\"\"Handles cloud-specific errors and updates the block list.\"\"\"\n        cloud = launchable_resources.cloud\n        handler = getattr(FailoverCloudErrorHandlerV2,\n                          f'_{str(cloud).lower()}_handler',\n                          FailoverCloudErrorHandlerV2._default_handler)\n        handler(blocked_resources, launchable_resources, region, zones, error)\n\n\nclass RetryingVmProvisioner(object):\n    \"\"\"A provisioner that retries different cloud/regions/zones.\"\"\"\n\n    class ToProvisionConfig:\n        \"\"\"Resources to be provisioned.\"\"\"\n\n        def __init__(\n            self,\n            cluster_name: str,\n            resources: resources_lib.Resources,\n            num_nodes: int,\n            prev_cluster_status: Optional[status_lib.ClusterStatus],\n            prev_handle: Optional['CloudVmRayResourceHandle'],\n            prev_cluster_ever_up: bool,\n            prev_config_hash: Optional[str],\n        ) -> None:\n            assert cluster_name is not None, 'cluster_name must be specified.'\n            self.cluster_name = cluster_name\n            self.resources = resources\n            self.num_nodes = num_nodes\n            self.prev_cluster_status = prev_cluster_status\n            self.prev_handle = prev_handle\n            self.prev_cluster_ever_up = prev_cluster_ever_up\n            self.prev_config_hash = prev_config_hash\n\n    def __init__(self,\n                 log_dir: str,\n                 dag: 'dag.Dag',\n                 optimize_target: 'common.OptimizeTarget',\n                 requested_features: Set[clouds.CloudImplementationFeatures],\n                 local_wheel_path: pathlib.Path,\n                 wheel_hash: str,\n                 blocked_resources: Optional[Iterable[\n                     resources_lib.Resources]] = None,\n                 is_managed: Optional[bool] = None):\n        self._blocked_resources: Set[resources_lib.Resources] = set()\n        if blocked_resources:\n            # blocked_resources is not None and not empty.\n            self._blocked_resources.update(blocked_resources)\n\n        self.log_dir = os.path.expanduser(log_dir)\n        self._dag = dag\n        self._optimize_target = optimize_target\n        self._requested_features = requested_features\n        self._local_wheel_path = local_wheel_path\n        self._wheel_hash = wheel_hash\n        self._is_managed = is_managed\n\n    def _yield_zones(\n            self, to_provision: resources_lib.Resources, num_nodes: int,\n            cluster_name: str,\n            prev_cluster_status: Optional[status_lib.ClusterStatus],\n            prev_cluster_ever_up: bool\n    ) -> Iterable[Optional[List[clouds.Zone]]]:\n        \"\"\"Yield zones within the given region to try for provisioning.\n\n        Yields:\n            Zones to try for provisioning within the given to_provision.region.\n              - None means the cloud does not support zones, but the region does\n                offer the requested resources (so the outer loop should issue a\n                request to that region).\n              - Non-empty list means the cloud supports zones, and the zones\n                do offer the requested resources. If a list is yielded, it is\n                guaranteed to be non-empty.\n              - Nothing yielded means the region does not offer the requested\n                resources.\n        \"\"\"\n        assert (to_provision.cloud is not None and\n                to_provision.region is not None and to_provision.instance_type\n                is not None), (to_provision,\n                               'cloud, region and instance_type must have been '\n                               'set by optimizer')\n        cloud = to_provision.cloud\n        region = clouds.Region(to_provision.region)\n        zones = None\n\n        def _get_previously_launched_zones() -> Optional[List[clouds.Zone]]:\n            # When the cluster exists, the to_provision should have been set\n            # to the previous cluster's resources.\n            zones = [\n                clouds.Zone(name=to_provision.zone),\n            ] if to_provision.zone is not None else None\n            if zones is None:\n                # Reuse the zone field in the ray yaml as the\n                # prev_resources.zone field may not be set before the previous\n                # cluster is launched.\n                handle = global_user_state.get_handle_from_cluster_name(\n                    cluster_name)\n                assert isinstance(handle, CloudVmRayResourceHandle), (\n                    'handle should be CloudVmRayResourceHandle (found: '\n                    f'{type(handle)}) {cluster_name!r}')\n                config = global_user_state.get_cluster_yaml_dict(\n                    handle.cluster_yaml)\n                # This is for the case when the zone field is not set in the\n                # launched resources in a previous launch (e.g., ctrl-c during\n                # launch and multi-node cluster before PR #1700).\n                zones_str = config.get('provider', {}).get('availability_zone')\n                if zones_str is not None:\n                    zones = [\n                        clouds.Zone(name=zone) for zone in zones_str.split(',')\n                    ]\n            return zones\n\n        if prev_cluster_status is not None:\n            # If the cluster is previously launched, we should relaunch in the\n            # same region and zone.\n            zones = _get_previously_launched_zones()\n\n            if prev_cluster_status != status_lib.ClusterStatus.UP:\n                logger.info(\n                    f'{colorama.Style.DIM}Cluster {cluster_name!r} (status: '\n                    f'{prev_cluster_status.value}) was previously in '\n                    f'{cloud} ({region.name}). Restarting.'\n                    f'{colorama.Style.RESET_ALL}')\n            yield zones\n\n            # If it reaches here: the cluster status in the database gets\n            # set to either STOPPED or None, since a launch request was issued\n            # but failed, and the provisioning loop (_retry_zones()) stopped the\n            # cluster if `cluster_ever_up` is True; or terminated the cluster\n            # otherwise.\n            if prev_cluster_ever_up:\n                message = (f'Failed to launch cluster {cluster_name!r} '\n                           f'(previous status: {prev_cluster_status.value}). '\n                           'To retry launching the cluster, run: '\n                           f'sky start {cluster_name}')\n                with ux_utils.print_exception_no_traceback():\n                    raise exceptions.ResourcesUnavailableError(message,\n                                                               no_failover=True)\n\n            assert (prev_cluster_status == status_lib.ClusterStatus.INIT\n                   ), prev_cluster_status\n            message = (f'Failed to launch cluster {cluster_name!r} '\n                       f'(previous status: {prev_cluster_status.value}) '\n                       f'with the original resources: {to_provision}.')\n            # We attempted re-launching a previously INIT cluster with the\n            # same cloud/region/resources, but failed. Here no_failover=False,\n            # so we will retry provisioning it with the current requested\n            # resources in the outer loop.\n            #\n            # This condition can be triggered for previously INIT cluster by\n            # (1) launch, after answering prompt immediately ctrl-c;\n            # (2) launch again.\n            # After (1), the cluster exists with INIT, and may or may not be\n            # live.  And if it hits here, it's definitely not alive (because\n            # step (2) failed).  Hence it's ok to retry with different\n            # cloud/region and with current resources.\n            with ux_utils.print_exception_no_traceback():\n                raise exceptions.ResourcesUnavailableError(message)\n\n        # If it reaches here, it means the cluster did not exist, as all the\n        # cases when the cluster exists have been handled above (either the\n        # provision succeeded in the caller and no need to retry, or this\n        # function raised an ResourcesUnavailableError).\n        for zones in cloud.zones_provision_loop(\n                region=to_provision.region,\n                num_nodes=num_nodes,\n                instance_type=to_provision.instance_type,\n                accelerators=to_provision.accelerators,\n                use_spot=to_provision.use_spot,\n        ):\n            if zones is None:\n                yield None\n            else:\n                assert zones, (\n                    'Either None or a non-empty list of zones should '\n                    'be yielded')\n                # Only retry requested region/zones or all if not specified.\n                zone_names = [zone.name for zone in zones]\n                if not to_provision.valid_on_region_zones(\n                        region.name, zone_names):\n                    continue\n                if to_provision.zone is not None:\n                    zones = [clouds.Zone(name=to_provision.zone)]\n                yield zones\n\n    def _insufficient_resources_msg(\n        self,\n        to_provision: resources_lib.Resources,\n        requested_resources: Set[resources_lib.Resources],\n        insufficient_resources: Optional[List[str]],\n    ) -> str:\n        insufficent_resource_msg = ('' if insufficient_resources is None else\n                                    f' ({\", \".join(insufficient_resources)})')\n        message = f'Failed to acquire resources{insufficent_resource_msg} '\n        if to_provision.zone is not None:\n            message += (f'in {to_provision.zone} for {requested_resources}. ')\n        elif to_provision.region is not None and to_provision.cloud is not None:\n            # For public clouds, provision.region is always set.\n            if clouds.SSH().is_same_cloud(to_provision.cloud):\n                ssh_node_pool_name = common_utils.removeprefix(\n                    to_provision.region, 'ssh-')\n                message += (\n                    f'in SSH Node Pool ({ssh_node_pool_name}) '\n                    f'for {requested_resources}. The SSH Node Pool may not '\n                    'have enough resources.')\n            elif clouds.Kubernetes().is_same_cloud(to_provision.cloud):\n                message += (f'in context {to_provision.region} for '\n                            f'{requested_resources}. ')\n            else:\n                message += (f'in all zones in {to_provision.region} for '\n                            f'{requested_resources}. ')\n        else:\n            message += (f'{to_provision.cloud} for {requested_resources}. ')\n        return message\n\n    def _retry_zones(\n        self,\n        to_provision: resources_lib.Resources,\n        num_nodes: int,\n        requested_resources: Set[resources_lib.Resources],\n        dryrun: bool,\n        stream_logs: bool,\n        cluster_name: str,\n        cloud_user_identity: Optional[List[str]],\n        prev_cluster_status: Optional[status_lib.ClusterStatus],\n        prev_handle: Optional['CloudVmRayResourceHandle'],\n        prev_cluster_ever_up: bool,\n        skip_if_config_hash_matches: Optional[str],\n        volume_mounts: Optional[List[volume_lib.VolumeMount]],\n    ) -> Dict[str, Any]:\n        \"\"\"The provision retry loop.\n\n        Returns a config_dict with the following fields:\n        All fields from backend_utils.write_cluster_config(). See its\n          docstring.\n        - 'provisioning_skipped': True if provisioning was short-circuited\n          by skip_if_config_hash_matches, False otherwise.\n        - 'handle': The provisioned cluster handle.\n        - 'provision_record': (Only if using the new skypilot provisioner) The\n          record returned by provisioner.bulk_provision().\n        - 'resources_vars': (Only if using the new skypilot provisioner) The\n          resources variables given by make_deploy_resources_variables().\n        \"\"\"\n        # Get log_path name\n        log_path = os.path.join(self.log_dir, 'provision.log')\n        log_abs_path = os.path.abspath(log_path)\n        if not dryrun:\n            os.makedirs(os.path.expanduser(self.log_dir), exist_ok=True)\n            os.system(f'touch {log_path}')\n\n        rich_utils.force_update_status(\n            ux_utils.spinner_message('Launching',\n                                     log_path,\n                                     cluster_name=cluster_name))\n\n        # Get previous cluster status\n        cluster_exists = prev_cluster_status is not None\n\n        to_provision = to_provision.assert_launchable()\n\n        assert to_provision.region is not None, (\n            to_provision, 'region should have been set by the optimizer.')\n        region = clouds.Region(to_provision.region)\n\n        # Optimization - check if user has non-zero quota for\n        # the instance type in the target region. If not, fail early\n        # instead of trying to provision and failing later.\n        try:\n            need_provision = to_provision.cloud.check_quota_available(\n                to_provision)\n\n        except Exception as e:  # pylint: disable=broad-except\n            need_provision = True\n            logger.info(f'Error occurred when trying to check quota. '\n                        f'Proceeding assuming quotas are available. Error: '\n                        f'{common_utils.format_exception(e, use_bracket=True)}')\n\n        if not need_provision:\n            # if quota is found to be zero, raise exception and skip to\n            # the next region\n            if to_provision.use_spot:\n                instance_descriptor = 'spot'\n            else:\n                instance_descriptor = 'on-demand'\n            raise exceptions.ResourcesUnavailableError(\n                f'{colorama.Fore.YELLOW}Found no quota for '\n                f'{to_provision.instance_type} {instance_descriptor} '\n                f'instances in region {to_provision.region} '\n                f'in {to_provision.cloud}. '\n                f'{colorama.Style.RESET_ALL}'\n                f'To request quotas, check the instruction: '\n                f'https://docs.skypilot.co/en/latest/cloud-setup/quota.html.')\n\n        insufficient_resources = None\n        for zones in self._yield_zones(to_provision, num_nodes, cluster_name,\n                                       prev_cluster_status,\n                                       prev_cluster_ever_up):\n            # Filter out zones that are blocked, if any.\n            # This optimize the provision loop by skipping zones that are\n            # indicated to be unavailable from previous provision attempts.\n            # It can happen for the provisioning on GCP, as the\n            # yield_region_zones will return zones from a region one by one,\n            # but the optimizer that does the filtering will not be involved\n            # until the next region.\n            if zones is not None:\n                remaining_unblocked_zones = copy.deepcopy(zones)\n                for zone in zones:\n                    for blocked_resources in self._blocked_resources:\n                        if to_provision.copy(\n                                region=region.name,\n                                zone=zone.name).should_be_blocked_by(\n                                    blocked_resources):\n                            remaining_unblocked_zones.remove(zone)\n                            break\n                if not remaining_unblocked_zones:\n                    # Skip the region if all zones are blocked.\n                    continue\n                zones = remaining_unblocked_zones\n\n            if zones is None:\n                # For clouds that don't have a zone concept or cloud\n                # provisioners that do not support zone-based provisioning\n                # (e.g., Azure, Lambda).\n                zone_str = ''\n            else:\n                zone_str = ','.join(z.name for z in zones)\n                zone_str = f' ({zone_str})'\n            try:\n                config_dict = backend_utils.write_cluster_config(\n                    to_provision,\n                    num_nodes,\n                    _get_cluster_config_template(to_provision.cloud),\n                    cluster_name,\n                    self._local_wheel_path,\n                    self._wheel_hash,\n                    region=region,\n                    zones=zones,\n                    dryrun=dryrun,\n                    keep_launch_fields_in_existing_config=cluster_exists,\n                    volume_mounts=volume_mounts,\n                )\n            except exceptions.ResourcesUnavailableError as e:\n                # Failed due to catalog issue, e.g. image not found, or\n                # GPUs are requested in a Kubernetes cluster but the cluster\n                # does not have nodes labeled with GPU types.\n                logger.info(f'{e}')\n                continue\n            except exceptions.InvalidCloudCredentials as e:\n                # Failed due to invalid cloud credentials.\n                logger.warning(f'{common_utils.format_exception(e)}')\n                # We should block the entire cloud for invalid cloud credentials\n                _add_to_blocked_resources(\n                    self._blocked_resources,\n                    to_provision.copy(region=None, zone=None))\n                raise exceptions.ResourcesUnavailableError(\n                    f'Failed to provision on cloud {to_provision.cloud} due to '\n                    f'invalid cloud credentials: '\n                    f'{common_utils.format_exception(e)}')\n            except exceptions.InvalidCloudConfigs as e:\n                # Failed due to invalid user configs in ~/.sky/config.yaml.\n                logger.warning(f'{common_utils.format_exception(e)}')\n                # We should block the entire cloud if the user config is\n                # invalid.\n                _add_to_blocked_resources(\n                    self._blocked_resources,\n                    to_provision.copy(region=None, zone=None))\n                raise exceptions.ResourcesUnavailableError(\n                    f'Failed to provision on cloud {to_provision.cloud} due to '\n                    f'invalid cloud config: {common_utils.format_exception(e)}')\n\n            if ('config_hash' in config_dict and\n                    skip_if_config_hash_matches == config_dict['config_hash']):\n                logger.debug('Skipping provisioning of cluster with matching '\n                             'config hash.')\n                config_dict['provisioning_skipped'] = True\n                return config_dict\n            config_dict['provisioning_skipped'] = False\n\n            if dryrun:\n                return config_dict\n\n            cluster_config_file = config_dict['ray']\n\n            launched_resources = to_provision.copy(region=region.name)\n            if zones and len(zones) == 1:\n                launched_resources = launched_resources.copy(zone=zones[0].name)\n\n            prev_cluster_ips, prev_ssh_ports, prev_cluster_info = (None, None,\n                                                                   None)\n            if prev_handle is not None:\n                prev_cluster_ips = prev_handle.stable_internal_external_ips\n                prev_ssh_ports = prev_handle.stable_ssh_ports\n                prev_cluster_info = prev_handle.cached_cluster_info\n            # Record early, so if anything goes wrong, 'sky status' will show\n            # the cluster name and users can appropriately 'sky down'.  It also\n            # means a second 'sky launch -c <name>' will attempt to reuse.\n            handle = CloudVmRayResourceHandle(\n                cluster_name=cluster_name,\n                # Backward compatibility will be guaranteed by the underlying\n                # backend_utils.write_cluster_config, which gets the cluster\n                # name on cloud from the ray yaml file, if the previous cluster\n                # exists.\n                cluster_name_on_cloud=config_dict['cluster_name_on_cloud'],\n                cluster_yaml=cluster_config_file,\n                launched_nodes=num_nodes,\n                # OK for this to be shown in CLI as status == INIT.\n                launched_resources=launched_resources,\n                # Use the previous cluster's IPs and ports if available to\n                # optimize the case where the cluster is restarted, i.e., no\n                # need to query IPs and ports from the cloud provider.\n                stable_internal_external_ips=prev_cluster_ips,\n                stable_ssh_ports=prev_ssh_ports,\n                cluster_info=prev_cluster_info,\n            )\n            usage_lib.messages.usage.update_final_cluster_status(\n                status_lib.ClusterStatus.INIT)\n\n            # This sets the status to INIT (even for a normal, UP cluster).\n            global_user_state.add_or_update_cluster(\n                cluster_name,\n                cluster_handle=handle,\n                requested_resources=requested_resources,\n                ready=False,\n                is_managed=self._is_managed,\n                provision_log_path=log_abs_path,\n            )\n\n            # Add cluster event for actual provisioning start.\n            global_user_state.add_cluster_event(\n                cluster_name, status_lib.ClusterStatus.INIT,\n                f'Provisioning on {to_provision.cloud.display_name()} ' +\n                f'in {to_provision.region}',\n                global_user_state.ClusterEventType.STATUS_CHANGE)\n\n            global_user_state.set_owner_identity_for_cluster(\n                cluster_name, cloud_user_identity)\n\n            if (to_provision.cloud.PROVISIONER_VERSION ==\n                    clouds.ProvisionerVersion.SKYPILOT):\n                # TODO (suquark): Gradually move the other clouds to\n                #  the new provisioner once they are ready.\n                assert to_provision.region == region.name, (to_provision,\n                                                            region)\n                num_nodes = handle.launched_nodes\n                # Some clouds, like RunPod, only support exposing ports during\n                # launch. For those clouds, we pass the ports to open in the\n                # `bulk_provision` to expose the ports during provisioning.\n                # If the `bulk_provision` is to apply on an existing cluster,\n                # it should be ignored by the underlying provisioner impl\n                # as it will only apply to newly-created instances.\n                ports_to_open_on_launch = (\n                    list(resources_utils.port_ranges_to_set(to_provision.ports))\n                    if to_provision.cloud.OPEN_PORTS_VERSION <=\n                    clouds.OpenPortsVersion.LAUNCH_ONLY else None)\n                try:\n                    controller = controller_utils.Controllers.from_name(\n                        cluster_name)\n                    controller_str = ('' if controller is None else\n                                      f' {controller.value.name}')\n                    if isinstance(to_provision.cloud, clouds.Kubernetes):\n                        suffix = '.'\n                        if region.name.startswith('ssh-'):\n                            ssh_node_pool_name = common_utils.removeprefix(\n                                region.name, 'ssh-')\n                            suffix = f' ({ssh_node_pool_name})'\n                        logger.info(\n                            ux_utils.starting_message(\n                                f'Launching{controller_str} on '\n                                f'{to_provision.cloud}{suffix}'))\n                    else:\n                        logger.info(\n                            ux_utils.starting_message(\n                                f'Launching{controller_str} on '\n                                f'{to_provision.cloud} '\n                                f'{region.name}{colorama.Style.RESET_ALL}'\n                                f'{zone_str}.'))\n                    assert handle.cluster_yaml is not None\n                    provision_record = provisioner.bulk_provision(\n                        to_provision.cloud,\n                        region,\n                        zones,\n                        resources_utils.ClusterName(\n                            cluster_name, handle.cluster_name_on_cloud),\n                        num_nodes=num_nodes,\n                        cluster_yaml=handle.cluster_yaml,\n                        prev_cluster_ever_up=prev_cluster_ever_up,\n                        log_dir=self.log_dir,\n                        ports_to_open_on_launch=ports_to_open_on_launch)\n                    # NOTE: We will handle the logic of '_ensure_cluster_ray_started' #pylint: disable=line-too-long\n                    # in 'provision_utils.post_provision_runtime_setup()' in the\n                    # caller.\n                    resources_vars = (\n                        to_provision.cloud.make_deploy_resources_variables(\n                            to_provision,\n                            resources_utils.ClusterName(\n                                cluster_name, handle.cluster_name_on_cloud),\n                            region, zones, num_nodes))\n                    config_dict['provision_record'] = provision_record\n                    config_dict['resources_vars'] = resources_vars\n                    config_dict['handle'] = handle\n                    return config_dict\n                except provision_common.StopFailoverError:\n                    with ux_utils.print_exception_no_traceback():\n                        raise\n                except exceptions.InconsistentHighAvailabilityError:\n                    # No teardown happens for this error.\n                    with ux_utils.print_exception_no_traceback():\n                        raise\n                except config_lib.KubernetesError as e:\n                    if e.insufficent_resources:\n                        insufficient_resources = e.insufficent_resources\n                    # NOTE: We try to cleanup the cluster even if the previous\n                    # cluster does not exist. Also we are fast at\n                    # cleaning up clusters now if there is no existing node.\n                    CloudVmRayBackend().post_teardown_cleanup(\n                        handle,\n                        terminate=not prev_cluster_ever_up,\n                        remove_from_db=False,\n                        failover=True,\n                    )\n                    # TODO(suquark): other clouds may have different zone\n                    #  blocking strategy. See '_update_blocklist_on_error'\n                    #  for details.\n                    FailoverCloudErrorHandlerV2.update_blocklist_on_error(\n                        self._blocked_resources, to_provision, region, zones, e)\n                    continue\n                except Exception as e:  # pylint: disable=broad-except\n                    # NOTE: We try to cleanup the cluster even if the previous\n                    # cluster does not exist. Also we are fast at\n                    # cleaning up clusters now if there is no existing node..\n                    CloudVmRayBackend().post_teardown_cleanup(\n                        handle,\n                        terminate=not prev_cluster_ever_up,\n                        remove_from_db=False,\n                        failover=True)\n                    # TODO(suquark): other clouds may have different zone\n                    #  blocking strategy. See '_update_blocklist_on_error'\n                    #  for details.\n                    FailoverCloudErrorHandlerV2.update_blocklist_on_error(\n                        self._blocked_resources, to_provision, region, zones, e)\n                    continue\n                # NOTE: The code below in the loop should not be reachable\n                # with the new provisioner.\n\n            logging_info = {\n                'cluster_name': cluster_name,\n                'region_name': region.name,\n                'zone_str': zone_str,\n            }\n\n            status, stdout, stderr, head_internal_ip, head_external_ip = (\n                self._gang_schedule_ray_up(to_provision.cloud,\n                                           cluster_config_file, handle,\n                                           log_abs_path, stream_logs,\n                                           logging_info, to_provision.use_spot))\n\n            if status == GangSchedulingStatus.CLUSTER_READY:\n                # We must query the IPs from the cloud provider, when the\n                # provisioning is done, to make sure the cluster IPs are\n                # up-to-date.\n                # The staled IPs may be caused by the node being restarted\n                # manually or by the cloud provider.\n                # Optimize the case where the cluster's head IPs can be parsed\n                # from the output of 'ray up'.\n                if handle.launched_nodes == 1:\n                    handle.update_cluster_ips(\n                        max_attempts=_FETCH_IP_MAX_ATTEMPTS,\n                        internal_ips=[head_internal_ip],\n                        external_ips=[head_external_ip])\n                else:\n                    handle.update_cluster_ips(\n                        max_attempts=_FETCH_IP_MAX_ATTEMPTS)\n                handle.update_ssh_ports(max_attempts=_FETCH_IP_MAX_ATTEMPTS)\n                if cluster_exists:\n                    # Guard against the case where there's an existing cluster\n                    # with ray runtime messed up (e.g., manually killed) by (1)\n                    # querying ray status (2) restarting ray if needed.\n                    #\n                    # The above 'ray up' will not restart it automatically due\n                    # to 'ray up # --no-restart' flag.\n                    #\n                    # NOTE: this is performance sensitive and has been observed\n                    # to take 9s. Only do this for existing clusters, not\n                    # freshly launched ones (which should have ray runtime\n                    # started).\n                    self._ensure_cluster_ray_started(handle, log_abs_path)\n\n                config_dict['handle'] = handle\n                logger.info(\n                    ux_utils.finishing_message(\n                        f'Cluster launched: {cluster_name!r}.',\n                        log_path,\n                        cluster_name=cluster_name))\n                return config_dict\n\n            # The cluster is not ready. We must perform error recording and/or\n            # cleanup.\n\n            # If cluster was ever up, stop it; otherwise terminate.\n            terminate_or_stop = not prev_cluster_ever_up\n            definitely_no_nodes_launched = False\n            if status == GangSchedulingStatus.HEAD_FAILED:\n                # ray up failed for the head node.\n                definitely_no_nodes_launched = (\n                    FailoverCloudErrorHandlerV1.update_blocklist_on_error(\n                        self._blocked_resources, to_provision, region, zones,\n                        stdout, stderr))\n            else:\n                # gang scheduling failed.\n                assert status == GangSchedulingStatus.GANG_FAILED, status\n                # The stdout/stderr of ray up is not useful here, since\n                # head node is successfully provisioned.\n                definitely_no_nodes_launched = (\n                    FailoverCloudErrorHandlerV1.update_blocklist_on_error(\n                        self._blocked_resources,\n                        to_provision,\n                        region,\n                        zones=zones,\n                        stdout=None,\n                        stderr=None))\n                # GANG_FAILED means head is up, workers failed.\n                assert definitely_no_nodes_launched is False, (\n                    definitely_no_nodes_launched)\n\n                # Only log the errors for GANG_FAILED, since HEAD_FAILED may\n                # not have created any resources (it can happen however) and\n                # HEAD_FAILED can happen in \"normal\" failover cases.\n                logger.error('*** Failed provisioning the cluster. ***')\n                terminate_str = ('Terminating'\n                                 if terminate_or_stop else 'Stopping')\n                logger.error(f'*** {terminate_str} the failed cluster. ***')\n\n            # If these conditions hold, it *should* be safe to skip the cleanup\n            # action. This is a UX optimization.\n            #\n            # We want to skip mainly for VPC/subnets errors thrown during node\n            # provider bootstrapping: if users encountered \"No VPC with name\n            # 'xxx' is found in <region>.\", then going ahead to down the\n            # non-existent cluster will itself print out a (caught, harmless)\n            # error with the same message.  This was found to be\n            # confusing. Thus we skip termination.\n            skip_cleanup = not cluster_exists and definitely_no_nodes_launched\n            if skip_cleanup:\n                continue\n\n            # There may exist partial nodes (e.g., head node) so we must\n            # terminate or stop before moving on to other regions.\n            #\n            # NOTE: even HEAD_FAILED could've left a live head node there,\n            # so we must terminate/stop here too. E.g., node is up, and ray\n            # autoscaler proceeds to setup commands, which may fail:\n            #   ERR updater.py:138 -- New status: update-failed\n            CloudVmRayBackend().teardown_no_lock(handle,\n                                                 terminate=terminate_or_stop,\n                                                 remove_from_db=False)\n\n        message = self._insufficient_resources_msg(to_provision,\n                                                   requested_resources,\n                                                   insufficient_resources)\n        # Do not failover to other locations if the cluster was ever up, since\n        # the user can have some data on the cluster.\n        raise exceptions.ResourcesUnavailableError(\n            message, no_failover=prev_cluster_ever_up)\n\n    # TODO(suquark): Deprecate this method\n    # once the `provision_utils` is adopted for all the clouds.\n    @timeline.event\n    def _gang_schedule_ray_up(\n        self, to_provision_cloud: clouds.Cloud, cluster_config_file: str,\n        cluster_handle: 'backends.CloudVmRayResourceHandle', log_abs_path: str,\n        stream_logs: bool, logging_info: dict, use_spot: bool\n    ) -> Tuple[GangSchedulingStatus, str, str, Optional[str], Optional[str]]:\n        \"\"\"Provisions a cluster via 'ray up' and wait until fully provisioned.\n\n        Returns:\n            (GangSchedulingStatus; stdout; stderr;\n                optional head_internal_ip; optional head_external_ip).\n        \"\"\"\n        # FIXME(zhwu,zongheng): ray up on multiple nodes ups the head node then\n        # waits for all workers; turn it into real gang scheduling.\n        # FIXME: refactor code path to remove use of stream_logs\n        del stream_logs\n\n        def ray_up():\n            # Runs `ray up <kwargs>` with our monkey-patched launch hash\n            # calculation. See the monkey patch file for why.\n            #\n            # NOTE: --no-restart solves the following bug.  Without it, if 'ray\n            # up' (sky launch) twice on a cluster with >1 node, the worker node\n            # gets disconnected/killed by ray autoscaler; the whole task will\n            # just freeze.  (Doesn't affect 1-node clusters.)  With this flag,\n            # ray processes no longer restart and this bug doesn't show.\n            # Downside is existing tasks on the cluster will keep running\n            # (which may be ok with the semantics of 'sky launch' twice).\n            # Tracked in https://github.com/ray-project/ray/issues/20402.\n            # Ref: https://github.com/ray-project/ray/blob/releases/2.4.0/python/ray/autoscaler/sdk/sdk.py#L16-L49  # pylint: disable=line-too-long\n            script_path = write_ray_up_script_with_patched_launch_hash_fn(\n                cluster_config_file, ray_up_kwargs={'no_restart': True})\n\n            # Redirect stdout/err to the file and streaming (if stream_logs).\n            # With stdout/err redirected, 'ray up' will have no color and\n            # different order from directly running in the console. The\n            # `--log-style` and `--log-color` flags do not work. To reproduce,\n            # `ray up --log-style pretty --log-color true | tee tmp.out`.\n            returncode, stdout, stderr = log_lib.run_with_log(\n                [sys.executable, script_path],\n                log_abs_path,\n                stream_logs=False,\n                start_streaming_at='Shared connection to',\n                line_processor=log_utils.RayUpLineProcessor(\n                    log_abs_path, cluster_name=cluster_handle.cluster_name),\n                # Reduce BOTO_MAX_RETRIES from 12 to 5 to avoid long hanging\n                # time during 'ray up' if insufficient capacity occurs.\n                env=dict(\n                    os.environ,\n                    BOTO_MAX_RETRIES='5',\n                    # Use environment variables to disable the ray usage collection\n                    # (to avoid overheads and potential issues with the usage)\n                    # as sdk does not take the argument for disabling the usage\n                    # collection.\n                    RAY_USAGE_STATS_ENABLED='0'),\n                require_outputs=True,\n                # Disable stdin to avoid ray outputs mess up the terminal with\n                # misaligned output when multithreading/multiprocessing are used\n                # Refer to: https://github.com/ray-project/ray/blob/d462172be7c5779abf37609aed08af112a533e1e/python/ray/autoscaler/_private/subprocess_output_util.py#L264  # pylint: disable=line-too-long\n                stdin=subprocess.DEVNULL)\n            return returncode, stdout, stderr\n\n        region_name = logging_info['region_name']\n        zone_str = logging_info['zone_str']\n        if isinstance(to_provision_cloud, clouds.Kubernetes):\n            logger.info(\n                ux_utils.starting_message(\n                    f'Launching on {to_provision_cloud}.'))\n        else:\n            logger.info(\n                ux_utils.starting_message(f'Launching on {to_provision_cloud} '\n                                          f'{region_name}{zone_str}.'))\n        start = time.time()\n\n        # Edge case: /tmp/ray does not exist, so autoscaler can't create/store\n        # cluster lock and cluster state.\n        os.makedirs('/tmp/ray', exist_ok=True)\n\n        # Launch the cluster with ray up\n\n        # Retry if the any of the following happens:\n        # 1. Failed due to timeout when fetching head node for Azure.\n        # 2. Failed due to file mounts, because it is probably has too\n        # many ssh connections and can be fixed by retrying.\n        # This is required when using custom image for GCP.\n        def need_ray_up(\n                ray_up_return_value: Optional[Tuple[int, str, str]]) -> bool:\n\n            # Indicates the first ray up.\n            if ray_up_return_value is None:\n                return True\n\n            returncode, stdout, stderr = ray_up_return_value\n            if returncode == 0:\n                return False\n\n            if isinstance(to_provision_cloud, clouds.Lambda):\n                if 'Your API requests are being rate limited.' in stderr:\n                    logger.info(\n                        'Retrying due to Lambda API rate limit exceeded.')\n                    return True\n\n            if 'rsync: command not found' in stderr:\n                logger.info('Skipping retry due to `rsync` not found in '\n                            'the specified image.')\n                return False\n\n            if ('Processing file mounts' in stdout and\n                    'Running setup commands' not in stdout and\n                    'Failed to setup head node.' in stderr):\n                logger.info(\n                    'Retrying runtime setup due to ssh connection issue.')\n                return True\n\n            if ('ConnectionResetError: [Errno 54] Connection reset by peer'\n                    in stderr):\n                logger.info('Retrying due to Connection reset by peer.')\n                return True\n            return False\n\n        retry_cnt = 0\n        ray_up_return_value = None\n        # 5 seconds to 180 seconds. We need backoff for e.g., rate limit per\n        # minute errors.\n        backoff = common_utils.Backoff(initial_backoff=5,\n                                       max_backoff_factor=180 // 5)\n        while (retry_cnt < _MAX_RAY_UP_RETRY and\n               need_ray_up(ray_up_return_value)):\n            retry_cnt += 1\n            if retry_cnt > 1:\n                sleep = backoff.current_backoff()\n                logger.info(\n                    'Retrying launching in {:.1f} seconds.'.format(sleep))\n                time.sleep(sleep)\n            # TODO(zhwu): when we retry ray up, it is possible that the ray\n            # cluster fail to start because --no-restart flag is used.\n            ray_up_return_value = ray_up()\n\n        assert ray_up_return_value is not None\n        returncode, stdout, stderr = ray_up_return_value\n\n        logger.debug(f'`ray up` takes {time.time() - start:.1f} seconds with '\n                     f'{retry_cnt} retries.')\n        if returncode != 0:\n            return GangSchedulingStatus.HEAD_FAILED, stdout, stderr, None, None\n\n        # Only 1 node or head node provisioning failure.\n        if cluster_handle.launched_nodes == 1 and returncode == 0:\n            # Optimization: Try parse head ip from 'ray up' stdout.\n            # Last line looks like: 'ssh ... <user>@<public head_ip>\\n'\n            position = stdout.rfind('@')\n            # Use a regex to extract the IP address.\n            external_ip_list = re.findall(backend_utils.IP_ADDR_REGEX,\n                                          stdout[position + 1:])\n            head_internal_ip, head_external_ip = None, None\n            if len(external_ip_list) == 1:\n                head_external_ip = external_ip_list[0]\n\n            # Optimization: Try parse internal head ip from 'ray start' stdout.\n            # The line looks like: 'Local node IP: <internal head_ip>\\n'\n            position = stdout.rfind('Local node IP')\n            line = stdout[position:].partition('\\n')[0]\n            internal_ip_list = re.findall(backend_utils.IP_ADDR_REGEX,\n                                          common_utils.remove_color(line))\n            if len(internal_ip_list) == 1:\n                head_internal_ip = internal_ip_list[0]\n\n            logger.debug(f'Get head ips from ray up stdout: {head_internal_ip} '\n                         f'{head_external_ip}')\n            return (GangSchedulingStatus.CLUSTER_READY, stdout, stderr,\n                    head_internal_ip, head_external_ip)\n\n        # All code below is handling num_nodes > 1.\n        # FIXME(zongheng): the below requires ray processes are up on head. To\n        # repro it failing: launch a 2-node cluster, log into head and ray\n        # stop, then launch again.\n        cluster_ready = backend_utils.wait_until_ray_cluster_ready(\n            cluster_config_file,\n            num_nodes=cluster_handle.launched_nodes,\n            log_path=log_abs_path,\n            nodes_launching_progress_timeout=_NODES_LAUNCHING_PROGRESS_TIMEOUT[\n                type(to_provision_cloud)])\n        if cluster_ready:\n            cluster_status = GangSchedulingStatus.CLUSTER_READY\n            # ray up --no-restart again with upscaling_speed=0 after cluster is\n            # ready to ensure cluster will not scale up after preemption (spot).\n            # Skip for non-spot as this takes extra time to provision (~1min).\n            if use_spot:\n                ray_config = global_user_state.get_cluster_yaml_dict(\n                    cluster_config_file)\n                ray_config['upscaling_speed'] = 0\n                yaml_utils.dump_yaml(cluster_config_file, ray_config)\n                start = time.time()\n                returncode, stdout, stderr = ray_up()\n                logger.debug(\n                    f'Upscaling reset takes {time.time() - start} seconds.')\n                if returncode != 0:\n                    return (GangSchedulingStatus.GANG_FAILED, stdout, stderr,\n                            None, None)\n        else:\n            cluster_status = GangSchedulingStatus.GANG_FAILED\n\n        # Do not need stdout/stderr if gang scheduling failed.\n        # gang_succeeded = False, if head OK, but workers failed.\n        return cluster_status, '', '', None, None\n\n    def _ensure_cluster_ray_started(self, handle: 'CloudVmRayResourceHandle',\n                                    log_abs_path) -> None:\n        \"\"\"Ensures ray processes are up on a just-provisioned cluster.\"\"\"\n        if handle.launched_nodes > 1:\n            # FIXME(zongheng): this has NOT been tested with multinode\n            # clusters; mainly because this function will not be reached in\n            # that case.  See #140 for details.  If it were reached, the\n            # following logic might work:\n            #   - get all node ips\n            #   - for all nodes: ray stop\n            #   - ray up --restart-only\n            return\n        backend = CloudVmRayBackend()\n\n        returncode, output, _ = backend.run_on_head(\n            handle,\n            instance_setup.RAY_STATUS_WITH_SKY_RAY_PORT_COMMAND,\n            require_outputs=True)\n        while returncode == 0 and 'No cluster status' in output:\n            # Retry until ray status is ready. This is to avoid the case where\n            # ray cluster is just started but the ray status is not ready yet.\n            logger.info('Waiting for ray cluster to be ready remotely.')\n            time.sleep(1)\n            returncode, output, _ = backend.run_on_head(\n                handle,\n                instance_setup.RAY_STATUS_WITH_SKY_RAY_PORT_COMMAND,\n                require_outputs=True)\n        if returncode == 0:\n            return\n        backend.run_on_head(handle, f'{constants.SKY_RAY_CMD} stop')\n\n        # Runs `ray up <kwargs>` with our monkey-patched launch hash\n        # calculation. See the monkey patch file for why.\n        script_path = write_ray_up_script_with_patched_launch_hash_fn(\n            handle.cluster_yaml, ray_up_kwargs={'restart_only': True})\n        log_lib.run_with_log(\n            [sys.executable, script_path],\n            log_abs_path,\n            stream_logs=False,\n            # Use environment variables to disable the ray usage collection\n            # (to avoid overheads and potential issues with the usage)\n            # as sdk does not take the argument for disabling the usage\n            # collection.\n            env=dict(os.environ, RAY_USAGE_STATS_ENABLED='0'),\n            # Disable stdin to avoid ray outputs mess up the terminal with\n            # misaligned output when multithreading/multiprocessing is used.\n            # Refer to: https://github.com/ray-project/ray/blob/d462172be7c5779abf37609aed08af112a533e1e/python/ray/autoscaler/_private/subprocess_output_util.py#L264 # pylint: disable=line-too-long\n            stdin=subprocess.DEVNULL)\n\n    @timeline.event\n    def provision_with_retries(\n        self,\n        task: task_lib.Task,\n        to_provision_config: ToProvisionConfig,\n        dryrun: bool,\n        stream_logs: bool,\n        skip_unnecessary_provisioning: bool,\n    ) -> Dict[str, Any]:\n        \"\"\"Provision with retries for all launchable resources.\n\n        Returns the config_dict from _retry_zones() - see its docstring for\n        details.\n        \"\"\"\n        cluster_name = to_provision_config.cluster_name\n        to_provision = to_provision_config.resources\n        num_nodes = to_provision_config.num_nodes\n        prev_cluster_status = to_provision_config.prev_cluster_status\n        prev_handle = to_provision_config.prev_handle\n        prev_cluster_ever_up = to_provision_config.prev_cluster_ever_up\n        launchable_retries_disabled = (self._dag is None or\n                                       self._optimize_target is None)\n        skip_if_config_hash_matches = (to_provision_config.prev_config_hash if\n                                       skip_unnecessary_provisioning else None)\n\n        failover_history: List[Exception] = list()\n        resource_exceptions: Dict[resources_lib.Resources, Exception] = dict()\n        # If the user is using local credentials which may expire, the\n        # controller may leak resources if the credentials expire while a job\n        # is running. Here we check the enabled clouds and expiring credentials\n        # and raise a warning to the user.\n        if task.is_controller_task():\n            enabled_clouds = sky_check.get_cached_enabled_clouds_or_refresh(\n                sky_cloud.CloudCapability.COMPUTE)\n            expirable_clouds = backend_utils.get_expirable_clouds(\n                enabled_clouds)\n\n            if len(expirable_clouds) > 0:\n                warnings = (f'\\033[93mWarning: Credentials used for '\n                            f'{expirable_clouds} may expire. Clusters may be '\n                            f'leaked if the credentials expire while jobs '\n                            f'are running. It is recommended to use credentials'\n                            f' that never expire or a service account.\\033[0m')\n                logger.warning(warnings)\n\n        to_provision = to_provision.assert_launchable()\n        # Retrying launchable resources.\n        while True:\n            try:\n                # Recheck cluster name as the 'except:' block below may\n                # change the cloud assignment.\n                common_utils.check_cluster_name_is_valid(cluster_name)\n\n                if dryrun:\n                    cloud_user = None\n                else:\n                    cloud_user = to_provision.cloud.get_active_user_identity()\n\n                requested_features = self._requested_features.copy()\n                # Skip stop feature for Kubernetes and RunPod controllers.\n                if (isinstance(to_provision.cloud,\n                               (clouds.Kubernetes, clouds.RunPod)) and\n                        controller_utils.Controllers.from_name(cluster_name)\n                        is not None):\n                    # If autostop is disabled in config, the feature may not be\n                    # requested, so use discard() instead of remove().\n                    requested_features.discard(\n                        clouds.CloudImplementationFeatures.AUTOSTOP)\n\n                # Skip if to_provision.cloud does not support requested features\n                to_provision.cloud.check_features_are_supported(\n                    to_provision, requested_features)\n\n                config_dict = self._retry_zones(\n                    to_provision,\n                    num_nodes,\n                    requested_resources=set(task.resources),\n                    dryrun=dryrun,\n                    stream_logs=stream_logs,\n                    cluster_name=cluster_name,\n                    cloud_user_identity=cloud_user,\n                    prev_cluster_status=prev_cluster_status,\n                    prev_handle=prev_handle,\n                    prev_cluster_ever_up=prev_cluster_ever_up,\n                    skip_if_config_hash_matches=skip_if_config_hash_matches,\n                    volume_mounts=task.volume_mounts,\n                )\n                if dryrun:\n                    return config_dict\n            except (exceptions.InvalidClusterNameError,\n                    exceptions.NotSupportedError,\n                    exceptions.CloudUserIdentityError) as e:\n                # InvalidClusterNameError: cluster name is invalid,\n                # NotSupportedError: cloud does not support requested features,\n                # CloudUserIdentityError: cloud user identity is invalid.\n                # The exceptions above should be applicable to the whole\n                # cloud, so we do add the cloud to the blocked resources.\n                logger.warning(common_utils.format_exception(e))\n                _add_to_blocked_resources(\n                    self._blocked_resources,\n                    resources_lib.Resources(cloud=to_provision.cloud))\n                failover_history.append(e)\n            except exceptions.ResourcesUnavailableError as e:\n                failover_history.append(e)\n                if e.no_failover:\n                    raise e.with_failover_history(failover_history)\n                if launchable_retries_disabled:\n                    logger.warning(\n                        'DAG and optimize_target needs to be registered first '\n                        'to enable cross-cloud retry. '\n                        'To fix, call backend.register_info(dag=dag, '\n                        'optimize_target=sky.OptimizeTarget.COST)')\n                    raise e.with_failover_history(failover_history)\n\n                logger.warning(common_utils.format_exception(e))\n            else:\n                # Provisioning succeeded.\n                break\n\n            if prev_cluster_status is None:\n                # Add failed resources to the blocklist, only when it\n                # is in fallback mode.\n                _add_to_blocked_resources(self._blocked_resources, to_provision)\n                assert len(failover_history) > 0\n                resource_exceptions[to_provision] = failover_history[-1]\n            else:\n                # If we reach here, it means that the existing cluster must have\n                # a previous status of INIT, because other statuses (UP,\n                # STOPPED) will not trigger the failover due to `no_failover`\n                # flag; see _yield_zones(). Also, the cluster should have been\n                # terminated by _retry_zones().\n                assert (prev_cluster_status == status_lib.ClusterStatus.INIT\n                       ), prev_cluster_status\n                logger.info(\n                    ux_utils.retry_message(\n                        f'Retrying provisioning with requested resources: '\n                        f'{task.num_nodes}x {task.resources}'))\n                # Retry with the current, potentially \"smaller\" resources:\n                # to_provision == the current new resources (e.g., V100:1),\n                # which may be \"smaller\" than the original (V100:8).\n                # num_nodes is not part of a Resources so must be updated\n                # separately.\n                num_nodes = task.num_nodes\n                prev_cluster_status = None\n                prev_handle = None\n\n            retry_message = ux_utils.retry_message(\n                'Trying other potential resources.')\n            logger.warning(f'\\n{retry_message}')\n            log_path = os.path.join(self.log_dir, 'provision.log')\n            rich_utils.force_update_status(\n                ux_utils.spinner_message('Looking for resources', log_path))\n            # Set to None so that sky.optimize() will assign a new one\n            # (otherwise will skip re-optimizing this task).\n            # TODO: set all remaining tasks' best_resources to None.\n            task.best_resources = None\n            try:\n                self._dag = optimizer.Optimizer.optimize(\n                    self._dag,\n                    minimize=self._optimize_target,\n                    blocked_resources=self._blocked_resources)\n            except exceptions.ResourcesUnavailableError as e:\n                # Optimizer failed to find a feasible resources for the task,\n                # either because the previous failovers have blocked all the\n                # possible resources or the requested resources is too\n                # restrictive. If we reach here, our failover logic finally\n                # ends here.\n                table = log_utils.create_table(['INFRA', 'RESOURCES', 'REASON'])\n                for (resource, exception) in resource_exceptions.items():\n                    table.add_row([\n                        resource.infra.formatted_str(),\n                        resources_utils.format_resource(\n                            resource, simplified_only=True)[0], exception\n                    ])\n                # Set the max width of REASON column to 80 to avoid the table\n                # being wrapped in a unreadable way.\n                # pylint: disable=protected-access\n                table._max_width = {'REASON': 80}\n                raise exceptions.ResourcesUnavailableError(\n                    _RESOURCES_UNAVAILABLE_LOG + '\\n' + table.get_string(),\n                    failover_history=failover_history)\n            best_resources = task.best_resources\n            assert task in self._dag.tasks, 'Internal logic error.'\n            assert best_resources is not None, task\n            to_provision = best_resources\n        return config_dict\n\n\n@dataclasses.dataclass\nclass SSHTunnelInfo:\n    port: int\n    pid: int\n\n\ndef _is_tunnel_healthy(tunnel: SSHTunnelInfo) -> bool:\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(0.5)\n            s.connect(('localhost', tunnel.port))\n        return True\n    except socket.error as e:\n        logger.warning(f'Failed to connect to tunnel on port {tunnel.port}: '\n                       f'{common_utils.format_exception(e)}')\n        return False\n\n\nclass CloudVmRayResourceHandle(backends.backend.ResourceHandle):\n    \"\"\"A pickle-able handle to a cluster created by CloudVmRayBackend.\n\n    The handle object will last for the whole lifecycle of the cluster.\n\n    - (required) Cluster name.\n    - (required) Cluster name on cloud (different from the cluster name, as we\n        append user hash to avoid conflict b/t multiple users in the same\n        organization/account, and truncate the name for length limit). See\n        design_docs/cluster_name.md for details.\n    - (required) Path to a cluster.yaml file.\n    - (optional) A cached head node public IP.  Filled in after a\n        successful provision().\n    - (optional) A cached stable list of (internal IP, external IP) tuples\n        for all nodes in a cluster. Filled in after successful task execution.\n    - (optional) Launched num nodes\n    - (optional) Launched resources\n    - (optional) Docker user name\n    - (optional) If TPU(s) are managed, a path to a deletion script.\n    - (optional) Skylet SSH tunnel info.\n    \"\"\"\n    # Bump if any fields get added/removed/changed, and add backward\n    # compatibility logic in __setstate__ and/or __getstate__.\n    _VERSION = 12\n\n    def __init__(\n            self,\n            *,\n            cluster_name: str,\n            cluster_name_on_cloud: str,\n            cluster_yaml: Optional[str],\n            launched_nodes: int,\n            launched_resources: resources_lib.Resources,\n            stable_internal_external_ips: Optional[List[Tuple[str,\n                                                              str]]] = None,\n            stable_ssh_ports: Optional[List[int]] = None,\n            cluster_info: Optional[provision_common.ClusterInfo] = None\n    ) -> None:\n        self._version = self._VERSION\n        self.cluster_name = cluster_name\n        self.cluster_name_on_cloud = cluster_name_on_cloud\n        # Replace the home directory with ~ for better robustness across systems\n        # with different home directories.\n        if cluster_yaml is not None and cluster_yaml.startswith(\n                os.path.expanduser('~')):\n            cluster_yaml = cluster_yaml.replace(os.path.expanduser('~'), '~', 1)\n        self._cluster_yaml = cluster_yaml\n        # List of (internal_ip, feasible_ip) tuples for all the nodes in the\n        # cluster, sorted by the feasible ips. The feasible ips can be either\n        # internal or external ips, depending on the use_internal_ips flag.\n        self.stable_internal_external_ips = stable_internal_external_ips\n        self.stable_ssh_ports = stable_ssh_ports\n        self.cached_cluster_info = cluster_info\n        self.launched_nodes = launched_nodes\n        self.launched_resources = launched_resources\n        self.docker_user: Optional[str] = None\n        self.is_grpc_enabled = True\n\n    def __repr__(self):\n        return (f'ResourceHandle('\n                f'\\n\\tcluster_name={self.cluster_name},'\n                f'\\n\\tcluster_name_on_cloud={self.cluster_name_on_cloud},'\n                f'\\n\\thead_ip={self.head_ip},'\n                '\\n\\tstable_internal_external_ips='\n                f'{self.stable_internal_external_ips},'\n                '\\n\\tstable_ssh_ports='\n                f'{self.stable_ssh_ports},'\n                '\\n\\tcluster_yaml='\n                f'{self.cluster_yaml}, '\n                f'\\n\\tlaunched_resources={self.launched_nodes}x '\n                f'{self.launched_resources}, '\n                f'\\n\\tdocker_user={self.docker_user},'\n                f'\\n\\tssh_user={self.ssh_user},'\n                f'\\n\\tis_grpc_enabled={self.is_grpc_enabled},')\n\n    def get_cluster_name(self):\n        return self.cluster_name\n\n    def get_cluster_name_on_cloud(self):\n        return self.cluster_name_on_cloud\n\n    def _use_internal_ips(self):\n        \"\"\"Returns whether to use internal IPs for SSH connections.\"\"\"\n        # Directly load the `use_internal_ips` flag from the cluster yaml\n        # instead of `skypilot_config` as the latter can be changed after the\n        # cluster is UP.\n        return global_user_state.get_cluster_yaml_dict(self.cluster_yaml).get(\n            'provider', {}).get('use_internal_ips', False)\n\n    def update_ssh_ports(self, max_attempts: int = 1) -> None:\n        \"\"\"Fetches and sets the SSH ports for the cluster nodes.\n\n        Use this method to use any cloud-specific port fetching logic.\n        \"\"\"\n        del max_attempts  # Unused.\n        if self.cached_cluster_info is not None:\n            self.stable_ssh_ports = self.cached_cluster_info.get_ssh_ports()\n            return\n\n        head_ssh_port = 22\n        self.stable_ssh_ports = (\n            [head_ssh_port] + [22] *\n            (self.num_ips_per_node * self.launched_nodes - 1))\n\n    def _update_cluster_info(self):\n        # When a cluster is on a cloud that does not support the new\n        # provisioner, we should skip updating cluster_info.\n        if (self.launched_resources.cloud is not None and\n                self.launched_resources.cloud.PROVISIONER_VERSION >=\n                clouds.ProvisionerVersion.SKYPILOT):\n            provider_name = str(self.launched_resources.cloud).lower()\n            config = {}\n            # It is possible that the cluster yaml is not available when\n            # the handle is unpickled for service replicas from the\n            # controller with older version.\n            yaml_str = global_user_state.get_cluster_yaml_str(self.cluster_yaml)\n            if yaml_str is None:\n                # If the cluster yaml is not available,\n                # we skip updating the cluster info.\n                return\n            config = yaml_utils.safe_load(yaml_str)\n            try:\n                cluster_info = provision_lib.get_cluster_info(\n                    provider_name,\n                    region=self.launched_resources.region,\n                    cluster_name_on_cloud=self.cluster_name_on_cloud,\n                    provider_config=config.get('provider', None))\n            except Exception as e:  # pylint: disable=broad-except\n                # This could happen when the VM is not fully launched, and a\n                # user is trying to terminate it with `sky down`.\n                logger.debug('Failed to get cluster info for '\n                             f'{self.cluster_name} from the new provisioner '\n                             f'with {common_utils.format_exception(e)}.')\n                raise exceptions.FetchClusterInfoError(\n                    exceptions.FetchClusterInfoError.Reason.HEAD) from e\n            if cluster_info.num_instances != self.launched_nodes:\n                logger.debug(\n                    f'Available nodes in the cluster {self.cluster_name} '\n                    'do not match the number of nodes requested ('\n                    f'{cluster_info.num_instances} != '\n                    f'{self.launched_nodes}).')\n                raise exceptions.FetchClusterInfoError(\n                    exceptions.FetchClusterInfoError.Reason.HEAD)\n            self.cached_cluster_info = cluster_info\n\n    def update_cluster_ips(\n            self,\n            max_attempts: int = 1,\n            internal_ips: Optional[List[Optional[str]]] = None,\n            external_ips: Optional[List[Optional[str]]] = None,\n            cluster_info: Optional[provision_common.ClusterInfo] = None\n    ) -> None:\n        \"\"\"Updates the cluster IPs cached in the handle.\n\n        We cache the cluster IPs in the handle to avoid having to retrieve\n        them from the cloud provider every time we need them. This method\n        updates the cached IPs.\n\n        Optimizations:\n            1) If the external IPs are provided (e.g. from the provision logs),\n                we use them instead of retrieving them from the cloud provider.\n            2) If the cached external IPs match the provided (fetched) external\n                IPs, we don't need to update the internal IPs.\n            3) If the internal IPs are provided (e.g. from the provision logs),\n                we use them instead of retrieving them from the cloud provider.\n\n        Args:\n            max_attempts: The maximum number of attempts to get the head IP.\n            internal_ips: The internal IPs to use for the cluster. It is an\n                optimization to avoid retrieving the internal IPs from the\n                cloud provider. Typically, it can be parsed from the provision\n                logs.\n            external_ips: The external IPs to use for the cluster. Similar to\n                internal_ips, it is an optimization to avoid retrieving the\n                external IPs from the cloud provider.\n\n        Raises:\n            exceptions.FetchClusterInfoError: if we failed to get the cluster\n                infos. e.reason is HEAD or WORKER.\n        \"\"\"\n        if cluster_info is not None:\n            self.cached_cluster_info = cluster_info\n            cluster_feasible_ips = self.cached_cluster_info.get_feasible_ips()\n            cluster_internal_ips = self.cached_cluster_info.get_feasible_ips(\n                force_internal_ips=True)\n        else:\n            # For clouds that do not support the SkyPilot Provisioner API.\n            # TODO(zhwu): once all the clouds are migrated to SkyPilot\n            # Provisioner API, we should remove this else block\n            def is_provided_ips_valid(\n                    ips: Optional[List[Optional[str]]]) -> bool:\n                return (ips is not None and len(ips)\n                        == self.num_ips_per_node * self.launched_nodes and\n                        all(ip is not None for ip in ips))\n\n            use_internal_ips = self._use_internal_ips()\n\n            # cluster_feasible_ips is the list of IPs of the nodes in the\n            # cluster which can be used to connect to the cluster. It is a list\n            # of external IPs if the cluster is assigned public IPs, otherwise\n            # it is a list of internal IPs.\n            if is_provided_ips_valid(external_ips):\n                logger.debug(f'Using provided external IPs: {external_ips}')\n                cluster_feasible_ips = typing.cast(List[str], external_ips)\n            else:\n                cluster_feasible_ips = backend_utils.get_node_ips(\n                    self.cluster_yaml,\n                    self.launched_nodes,\n                    head_ip_max_attempts=max_attempts,\n                    worker_ip_max_attempts=max_attempts,\n                    get_internal_ips=use_internal_ips)\n\n            if self.cached_external_ips == cluster_feasible_ips:\n                logger.debug(\n                    'Skipping the fetching of internal IPs as the cached '\n                    'external IPs matches the newly fetched ones.')\n                # Optimization: If the cached external IPs are the same as the\n                # retrieved feasible IPs, then we can skip retrieving internal\n                # IPs since the cached IPs are up-to-date.\n                return\n\n            logger.debug(\n                'Cached external IPs do not match with the newly fetched ones: '\n                f'cached ({self.cached_external_ips}), new '\n                f'({cluster_feasible_ips})')\n\n            if use_internal_ips:\n                # Optimization: if we know use_internal_ips is True (currently\n                # only exposed for AWS and GCP), then our provisioner is\n                # guaranteed to not assign public IPs, thus the first list of\n                # IPs returned above are already private IPs. So skip the second\n                # query.\n                cluster_internal_ips = list(cluster_feasible_ips)\n            elif is_provided_ips_valid(internal_ips):\n                logger.debug(f'Using provided internal IPs: {internal_ips}')\n                cluster_internal_ips = typing.cast(List[str], internal_ips)\n            else:\n                cluster_internal_ips = backend_utils.get_node_ips(\n                    self.cluster_yaml,\n                    self.launched_nodes,\n                    head_ip_max_attempts=max_attempts,\n                    worker_ip_max_attempts=max_attempts,\n                    get_internal_ips=True)\n\n        assert len(cluster_feasible_ips) == len(cluster_internal_ips), (\n            f'Cluster {self.cluster_name!r}:'\n            f'Expected same number of internal IPs {cluster_internal_ips}'\n            f' and external IPs {cluster_feasible_ips}.')\n\n        # List of (internal_ip, feasible_ip) tuples for all the nodes in the\n        # cluster, sorted by the feasible ips. The feasible ips can be either\n        # internal or external ips, depending on the use_internal_ips flag.\n        internal_external_ips: List[Tuple[str, str]] = list(\n            zip(cluster_internal_ips, cluster_feasible_ips))\n\n        # Ensure head node is the first element, then sort based on the\n        # external IPs for stableness. Skip for k8s nodes since pods\n        # worker ids are already mapped.\n        if (cluster_info is not None and\n                cluster_info.provider_name == 'kubernetes'):\n            stable_internal_external_ips = internal_external_ips\n        else:\n            stable_internal_external_ips = [internal_external_ips[0]] + sorted(\n                internal_external_ips[1:], key=lambda x: x[1])\n        self.stable_internal_external_ips = stable_internal_external_ips\n\n    @context_utils.cancellation_guard\n    # we expect different request to be acting on different clusters\n    # (= different handles) so we have no real expectation of cache hit\n    # across requests.\n    # Do not change this cache to global scope\n    # without understanding https://github.com/skypilot-org/skypilot/pull/6908\n    @annotations.lru_cache(scope='request', maxsize=10)\n    @timeline.event\n    def get_command_runners(self,\n                            force_cached: bool = False,\n                            avoid_ssh_control: bool = False\n                           ) -> List[command_runner.CommandRunner]:\n        \"\"\"Returns a list of command runners for the cluster.\"\"\"\n        ssh_credentials = backend_utils.ssh_credential_from_yaml(\n            self.cluster_yaml, self.docker_user, self.ssh_user)\n        if avoid_ssh_control:\n            ssh_credentials.pop('ssh_control_name', None)\n\n        launched_resources = self.launched_resources.assert_launchable()\n        updated_to_skypilot_provisioner_after_provisioned = (\n            launched_resources.cloud.PROVISIONER_VERSION >=\n            clouds.ProvisionerVersion.SKYPILOT and\n            self.cached_external_ips is not None and\n            self.cached_cluster_info is None)\n        if updated_to_skypilot_provisioner_after_provisioned:\n            logger.debug(\n                f'{launched_resources.cloud} has been updated to the new '\n                f'provisioner after cluster {self.cluster_name} was '\n                f'provisioned. Cached IPs are used for connecting to the '\n                'cluster.')\n        if (clouds.ProvisionerVersion.RAY_PROVISIONER_SKYPILOT_TERMINATOR >=\n                launched_resources.cloud.PROVISIONER_VERSION or\n                updated_to_skypilot_provisioner_after_provisioned):\n            ip_list = (self.cached_external_ips\n                       if force_cached else self.external_ips())\n            if ip_list is None:\n                return []\n            # Potentially refresh the external SSH ports, in case the existing\n            # cluster before #2491 was launched without external SSH ports\n            # cached.\n            port_list = self.external_ssh_ports()\n            runners = command_runner.SSHCommandRunner.make_runner_list(\n                zip(ip_list, port_list), **ssh_credentials)\n            return runners\n        if self.cached_cluster_info is None:\n            # We have `and self.cached_external_ips is None` here, because\n            # when a cluster's cloud is just upgraded to the new provsioner,\n            # although it has the cached_external_ips, the cached_cluster_info\n            # can be None. We need to update it here, even when force_cached is\n            # set to True.\n            # TODO: We can remove `self.cached_external_ips is None` after\n            # all clouds moved to new provisioner.\n            if force_cached and self.cached_external_ips is None:\n                raise RuntimeError(\n                    'Tried to use cached cluster info, but it\\'s missing for '\n                    f'cluster \"{self.cluster_name}\"')\n            self._update_cluster_info()\n        # For Kubernetes, `KubernetesCommandRunner` want to get the pod names\n        # to run the command. But for high availability serve controller,\n        # the controller pod is part of a deployment, and once the pod is\n        # killed and a new one is created, the pod name changes, so we need\n        # to manually update the cluster info here.\n        # TODO(andyl): See if we can prevent this refresh. Like pass in\n        # deployment name as identifier for KubernetesCommandRunner. Now this\n        # is required for rsync as using deployment in rsync seems to cause\n        # some unknown issues.\n        # TODO(andyl): Should check through the real cluster info. Same as\n        # the TODO in kubernetes/instance.py:terminate_instances\n        if (isinstance(self.launched_resources.cloud, clouds.Kubernetes) and\n                controller_utils.high_availability_specified(\n                    self.cluster_name)):\n            self._update_cluster_info()\n\n        assert self.cached_cluster_info is not None, self\n        runners = provision_lib.get_command_runners(\n            self.cached_cluster_info.provider_name, self.cached_cluster_info,\n            **ssh_credentials)\n        return runners\n\n    @property\n    def cached_internal_ips(self) -> Optional[List[str]]:\n        if self.stable_internal_external_ips is not None:\n            return [ips[0] for ips in self.stable_internal_external_ips]\n        return None\n\n    def internal_ips(self,\n                     max_attempts: int = _FETCH_IP_MAX_ATTEMPTS) -> List[str]:\n        internal_ips = self.cached_internal_ips\n        if internal_ips is not None:\n            return internal_ips\n        self.update_cluster_ips(max_attempts=max_attempts)\n        internal_ips = self.cached_internal_ips\n        assert internal_ips is not None, 'update_cluster_ips failed.'\n        return internal_ips\n\n    @property\n    def cached_external_ips(self) -> Optional[List[str]]:\n        if self.stable_internal_external_ips is not None:\n            return [ips[1] for ips in self.stable_internal_external_ips]\n        return None\n\n    def external_ips(self,\n                     max_attempts: int = _FETCH_IP_MAX_ATTEMPTS) -> List[str]:\n        external_ips = self.cached_external_ips\n        if external_ips is not None:\n            return external_ips\n        self.update_cluster_ips(max_attempts=max_attempts)\n        external_ips = self.cached_external_ips\n        assert external_ips is not None, 'update_cluster_ips failed.'\n        return external_ips\n\n    @property\n    def cached_external_ssh_ports(self) -> Optional[List[int]]:\n        if self.stable_ssh_ports is not None:\n            return self.stable_ssh_ports\n        return None\n\n    def external_ssh_ports(self,\n                           max_attempts: int = _FETCH_IP_MAX_ATTEMPTS\n                          ) -> List[int]:\n        cached_ssh_ports = self.cached_external_ssh_ports\n        if cached_ssh_ports is not None:\n            return cached_ssh_ports\n        self.update_ssh_ports(max_attempts=max_attempts)\n        cached_ssh_ports = self.cached_external_ssh_ports\n        assert cached_ssh_ports is not None, 'update_ssh_ports failed.'\n        return cached_ssh_ports\n\n    def get_hourly_price(self) -> float:\n        hourly_cost = (self.launched_resources.get_cost(3600) *\n                       self.launched_nodes)\n        return hourly_cost\n\n    def setup_docker_user(self, cluster_config_file: str):\n        ip_list = self.external_ips()\n        assert ip_list is not None\n        docker_user = backend_utils.get_docker_user(ip_list[0],\n                                                    cluster_config_file)\n        self.docker_user = docker_user\n\n    def _get_skylet_ssh_tunnel(self) -> Optional[SSHTunnelInfo]:\n        metadata = global_user_state.get_cluster_skylet_ssh_tunnel_metadata(\n            self.cluster_name)\n        if metadata is None:\n            return None\n        return SSHTunnelInfo(port=metadata[0], pid=metadata[1])\n\n    def _set_skylet_ssh_tunnel(self, tunnel: Optional[SSHTunnelInfo]) -> None:\n        global_user_state.set_cluster_skylet_ssh_tunnel_metadata(\n            self.cluster_name,\n            (tunnel.port, tunnel.pid) if tunnel is not None else None)\n\n    def close_skylet_ssh_tunnel(self) -> None:\n        \"\"\"Terminate the SSH tunnel process and clear its metadata.\"\"\"\n        tunnel = self._get_skylet_ssh_tunnel()\n        if tunnel is None:\n            return\n        logger.debug('Closing Skylet SSH tunnel for cluster %r on port %d',\n                     self.cluster_name, tunnel.port)\n        try:\n            self._terminate_ssh_tunnel_process(tunnel)\n        finally:\n            self._set_skylet_ssh_tunnel(None)\n\n    def get_grpc_channel(self) -> 'grpc.Channel':\n        grpc_options = [\n            # The task YAMLs can be large, so the default\n            # max_receive_message_length of 4MB might not be enough.\n            ('grpc.max_receive_message_length', -1),\n        ]\n        # It's fine to not grab the lock here, as we're only reading,\n        # and writes are very rare.\n        # It's acceptable to read while another process is opening a tunnel,\n        # because it will only happen on:\n        # 1. A new cluster who has no tunnel yet, or\n        # 2. A cluster with an unhealthy tunnel\n        # For (2), for processes that read the \"stale\" tunnel, it will fail\n        # and on the next retry, it will call get_grpc_channel again\n        # and get the new tunnel.\n        tunnel = self._get_skylet_ssh_tunnel()\n        if tunnel is not None:\n            if _is_tunnel_healthy(tunnel):\n                return grpc.insecure_channel(f'localhost:{tunnel.port}',\n                                             options=grpc_options)\n            logger.debug('Failed to connect to SSH tunnel for cluster '\n                         f'{self.cluster_name!r} on port {tunnel.port}')\n\n        lock_id = backend_utils.cluster_tunnel_lock_id(self.cluster_name)\n        remaining_timeout = backend_utils.CLUSTER_TUNNEL_LOCK_TIMEOUT_SECONDS\n        start_time = time.perf_counter()\n        attempt = 1\n\n        def _get_remaining_timeout() -> float:\n            return max(0.0,\n                       remaining_timeout - (time.perf_counter() - start_time))\n\n        while remaining_timeout > 0:\n            logger.debug(\n                'Attempting to acquire exclusive lock for %s (attempt %d)',\n                lock_id, attempt)\n            exclusive_lock = locks.get_lock(lock_id, remaining_timeout)\n            try:\n                with exclusive_lock.acquire(blocking=False):\n                    wait_elapsed = time.perf_counter() - start_time\n                    logger.debug(f'Acquired exclusive lock for {lock_id} after '\n                                 f'{wait_elapsed:.2f}s')\n                    try:\n                        tunnel = self._open_and_update_skylet_tunnel()\n                        return grpc.insecure_channel(f'localhost:{tunnel.port}',\n                                                     options=grpc_options)\n                    except Exception as e:  # pylint: disable=broad-except\n                        # Failed to open tunnel, release the lock and retry.\n                        logger.warning(f'Failed to open tunnel for cluster '\n                                       f'{self.cluster_name!r}: '\n                                       f'{common_utils.format_exception(e)}')\n                        remaining_timeout = _get_remaining_timeout()\n                        attempt += 1\n                        continue\n            except locks.LockTimeout:\n                pass\n\n            remaining_timeout = _get_remaining_timeout()\n            logger.debug(f'Could not acquire exclusive lock for {lock_id}, '\n                         f'waiting on shared lock (attempt {attempt})')\n            try:\n                # Use shared lock so that concurrent readers can\n                # proceed in parallel.\n                shared_lock = locks.get_lock(lock_id,\n                                             remaining_timeout,\n                                             shared_lock=True)\n                # Wait for the exclusive lock to be released.\n                shared_lock.acquire(blocking=True)\n                # We only need the lock for signalling that the new tunnel has\n                # been opened, not for checking the tunnel health.\n                # Same reasoning as why we don't need to grab the lock in\n                # the fast path at the start of this function.\n                shared_lock.release()\n                wait_elapsed = time.perf_counter() - start_time\n                logger.debug(f'Acquired shared lock for {lock_id} after '\n                             f'{wait_elapsed:.2f}s')\n            except locks.LockTimeout as e:\n                raise RuntimeError(\n                    f'Failed to get gRPC channel for cluster '\n                    f'{self.cluster_name!r} due to a timeout when waiting '\n                    'for the SSH tunnel to be opened. Please try again or '\n                    f'manually remove the lock at {lock_id}. '\n                    f'{common_utils.format_exception(e)}') from e\n\n            # Add small jitter before probing to smoothen the effects\n            # of many readers waking up simultaneously.\n            jitter = random.uniform(0.01, 0.05)\n            time.sleep(jitter)\n\n            # Re-read the tunnel metadata and verify it's healthy.\n            tunnel = self._get_skylet_ssh_tunnel()\n            if tunnel is not None:\n                if _is_tunnel_healthy(tunnel):\n                    return grpc.insecure_channel(f'localhost:{tunnel.port}',\n                                                 options=grpc_options)\n                logger.debug('Failed to connect to SSH tunnel for cluster '\n                             f'{self.cluster_name!r} on port {tunnel.port}')\n            # Tunnel is still unhealthy or missing, try again with updated\n            # timeout. This could happen in the case where the thread who\n            # held the exclusive lock to open the tunnel crashed.\n            remaining_timeout = _get_remaining_timeout()\n            attempt += 1\n        raise RuntimeError('Timeout waiting for gRPC channel for cluster '\n                           f'{self.cluster_name!r} to be ready.')\n\n    def _terminate_ssh_tunnel_process(self, tunnel_info: SSHTunnelInfo) -> None:\n        \"\"\"Terminate the SSH tunnel process.\"\"\"\n        try:\n            proc = psutil.Process(tunnel_info.pid)\n            if proc.is_running() and proc.status() != psutil.STATUS_ZOMBIE:\n                logger.debug(\n                    f'Terminating SSH tunnel process {tunnel_info.pid}')\n                subprocess_utils.kill_children_processes(proc.pid)\n        except psutil.NoSuchProcess:\n            pass\n        except Exception as e:  # pylint: disable=broad-except\n            logger.warning(\n                f'Failed to cleanup SSH tunnel process {tunnel_info.pid}: {e}')\n\n    def _open_and_update_skylet_tunnel(self) -> SSHTunnelInfo:\n        \"\"\"Opens an SSH tunnel to the Skylet on the head node,\n        updates the cluster handle, and persists it to the database.\"\"\"\n        max_attempts = 3\n        # There could be a race condition here, as multiple processes may\n        # attempt to open the same port at the same time.\n        for attempt in range(max_attempts):\n            runners = self.get_command_runners()\n            head_runner = runners[0]\n            local_port = random.randint(10000, 65535)\n            try:\n                ssh_tunnel_proc = backend_utils.open_ssh_tunnel(\n                    head_runner, (local_port, constants.SKYLET_GRPC_PORT))\n            except exceptions.CommandError as e:\n                # Don't retry if the error is due to timeout,\n                # connection refused, Kubernetes pods not found,\n                # or an in-progress termination.\n                if (e.detailed_reason is not None and\n                    (backend_utils.SSH_CONNECTION_ERROR_PATTERN.search(\n                        e.detailed_reason) or\n                     backend_utils.K8S_PODS_NOT_FOUND_PATTERN.search(\n                         e.detailed_reason) or attempt == max_attempts - 1)):\n                    raise e\n                logger.warning(\n                    f'Failed to open SSH tunnel on port {local_port} '\n                    f'({attempt + 1}/{max_attempts}). '\n                    f'{e.error_msg}\\n{e.detailed_reason}')\n                continue\n            tunnel_info = SSHTunnelInfo(port=local_port,\n                                        pid=ssh_tunnel_proc.pid)\n            break\n\n        try:\n            grpc.channel_ready_future(\n                grpc.insecure_channel(f'localhost:{tunnel_info.port}')).result(\n                    timeout=constants.SKYLET_GRPC_TIMEOUT_SECONDS)\n            # Clean up existing tunnel before setting up the new one.\n            old_tunnel = self._get_skylet_ssh_tunnel()\n            if old_tunnel is not None:\n                self._terminate_ssh_tunnel_process(old_tunnel)\n            self._set_skylet_ssh_tunnel(tunnel_info)\n            return tunnel_info\n        except grpc.FutureTimeoutError as e:\n            self._terminate_ssh_tunnel_process(tunnel_info)\n            logger.warning(\n                f'Skylet gRPC channel for cluster {self.cluster_name} not '\n                f'ready after {constants.SKYLET_GRPC_TIMEOUT_SECONDS}s')\n            raise e\n        except Exception as e:\n            self._terminate_ssh_tunnel_process(tunnel_info)\n            raise e\n\n    @property\n    def cluster_yaml(self) -> Optional[str]:\n        if self._cluster_yaml is None:\n            return None\n        return os.path.expanduser(self._cluster_yaml)\n\n    @cluster_yaml.setter\n    def cluster_yaml(self, value: Optional[str]):\n        self._cluster_yaml = value\n\n    @property\n    def instance_ids(self):\n        if self.cached_cluster_info is not None:\n            return self.cached_cluster_info.instance_ids()\n        return None\n\n    @property\n    def ssh_user(self):\n        if self.cached_cluster_info is not None:\n            # Overload ssh_user with the user stored in cluster_info, which is\n            # useful for kubernetes case, where the ssh_user can depend on the\n            # container image used. For those clusters launched with ray\n            # autoscaler, we directly use the ssh_user in yaml config.\n            return self.cached_cluster_info.ssh_user\n        return None\n\n    @property\n    def head_ip(self):\n        external_ips = self.cached_external_ips\n        if external_ips is not None:\n            return external_ips[0]\n        return None\n\n    @property\n    def head_ssh_port(self):\n        external_ssh_ports = self.cached_external_ssh_ports\n        if external_ssh_ports:\n            return external_ssh_ports[0]\n        return None\n\n    @property\n    def num_ips_per_node(self) -> int:\n        \"\"\"Returns number of IPs per node in the cluster, handling TPU Pod.\"\"\"\n        is_tpu_vm_pod = gcp_utils.is_tpu_vm_pod(self.launched_resources)\n        if is_tpu_vm_pod:\n            num_ips = len(self.internal_ips())\n        else:\n            num_ips = 1\n        return num_ips\n\n    @property\n    def is_grpc_enabled_with_flag(self) -> bool:\n        \"\"\"Returns whether this handle has gRPC enabled and gRPC flag is set.\"\"\"\n        return (env_options.Options.ENABLE_GRPC.get() and\n                self.is_grpc_enabled and\n                not isinstance(self.launched_resources.cloud, clouds.Slurm))\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        # For backwards compatibility. Refer to\n        # https://github.com/skypilot-org/skypilot/pull/7133\n        state.setdefault('skylet_ssh_tunnel', None)\n        return state\n\n    def __setstate__(self, state):\n        self._version = self._VERSION\n\n        version = state.pop('_version', None)\n        if version is None:\n            version = -1\n            state.pop('cluster_region', None)\n        if version < 2:\n            state['_cluster_yaml'] = state.pop('cluster_yaml')\n        if version < 3:\n            head_ip = state.pop('head_ip', None)\n            state['stable_internal_external_ips'] = None\n        if version < 4:\n            # Version 4 adds self.stable_ssh_ports for Kubernetes support\n            state['stable_ssh_ports'] = None\n        if version < 5:\n            state['docker_user'] = None\n\n        if version < 6:\n            state['cluster_name_on_cloud'] = state['cluster_name']\n\n        if version < 8:\n            self.cached_cluster_info = None\n\n        if version < 9:\n            # For backward compatibility, we should update the region of a\n            # SkyPilot cluster on Kubernetes to the actual context it is using.\n            # pylint: disable=import-outside-toplevel\n            launched_resources = state['launched_resources']\n            if isinstance(launched_resources.cloud, clouds.Kubernetes):\n                yaml_config = global_user_state.get_cluster_yaml_dict(\n                    os.path.expanduser(state['_cluster_yaml']))\n                context = kubernetes_utils.get_context_from_config(\n                    yaml_config['provider'])\n                state['launched_resources'] = launched_resources.copy(\n                    region=context)\n\n        if version < 10:\n            # In #4660, we keep the cluster entry in the database even when it\n            # is in the transition from one region to another during the\n            # failover. We allow `handle.cluster_yaml` to be None to indicate\n            # that the cluster yaml is intentionally removed. Before that PR,\n            # the `handle.cluster_yaml` is always not None, even if it is\n            # intentionally removed.\n            #\n            # For backward compatibility, we set the `_cluster_yaml` to None\n            # if the file does not exist, assuming all the removal of the\n            # _cluster_yaml for existing clusters are intentional by SkyPilot.\n            # are intentional by SkyPilot.\n            if state['_cluster_yaml'] is not None and not os.path.exists(\n                    os.path.expanduser(state['_cluster_yaml'])):\n                state['_cluster_yaml'] = None\n\n        if version < 11:\n            state['is_grpc_enabled'] = False\n            state['skylet_ssh_tunnel'] = None\n\n        if version >= 12:\n            # DEPRECATED in favor of skylet_ssh_tunnel_metadata column in the DB\n            state.pop('skylet_ssh_tunnel', None)\n\n        self.__dict__.update(state)\n\n        # Because the update_cluster_ips and update_ssh_ports\n        # functions use the handle, we call it on the current instance\n        # after the state is updated.\n        if version < 3 and head_ip is not None:\n            try:\n                self.update_cluster_ips()\n            except exceptions.FetchClusterInfoError:\n                # This occurs when an old cluster from was autostopped,\n                # so the head IP in the database is not updated.\n                pass\n        if version < 4:\n            self.update_ssh_ports()\n\n        if version < 8:\n            try:\n                self._update_cluster_info()\n            except exceptions.FetchClusterInfoError:\n                # This occurs when an old cluster from was autostopped,\n                # so the head IP in the database is not updated.\n                pass\n\n\nclass LocalResourcesHandle(CloudVmRayResourceHandle):\n    \"\"\"A handle for local resources.\"\"\"\n\n    def __init__(\n            self,\n            *,\n            cluster_name: str,\n            cluster_name_on_cloud: str,\n            cluster_yaml: Optional[str],\n            launched_nodes: int,\n            launched_resources: resources_lib.Resources,\n            stable_internal_external_ips: Optional[List[Tuple[str,\n                                                              str]]] = None,\n            stable_ssh_ports: Optional[List[int]] = None,\n            cluster_info: Optional[provision_common.ClusterInfo] = None\n    ) -> None:\n        super().__init__(\n            cluster_name=cluster_name,\n            cluster_name_on_cloud=cluster_name_on_cloud,\n            cluster_yaml=cluster_yaml,\n            launched_nodes=launched_nodes,\n            launched_resources=launched_resources,\n            stable_internal_external_ips=stable_internal_external_ips,\n            stable_ssh_ports=stable_ssh_ports,\n            cluster_info=cluster_info)\n        # TODO (kyuds): handle jobs consolidation mode. Currently,\n        # jobs consolidation mode will not run a skylet, hence\n        # grpc server will not run. In the future, we should\n        # figure out a way to start grpc in consolidation mode.\n        self.is_grpc_enabled = False\n\n    @context_utils.cancellation_guard\n    # we expect different request to be acting on different clusters\n    # (= different handles) so we have no real expectation of cache hit\n    # across requests.\n    # Do not change this cache to global scope\n    # without understanding https://github.com/skypilot-org/skypilot/pull/6908\n    @annotations.lru_cache(scope='request', maxsize=10)\n    @timeline.event\n    def get_command_runners(self,\n                            force_cached: bool = False,\n                            avoid_ssh_control: bool = False\n                           ) -> List[command_runner.CommandRunner]:\n        \"\"\"Returns a list of local command runners.\"\"\"\n        del force_cached, avoid_ssh_control  # Unused.\n        return [command_runner.LocalProcessCommandRunner()]\n\n\nclass SkyletClient:\n    \"\"\"The client to interact with a remote cluster through Skylet.\"\"\"\n\n    def __init__(self, channel: 'grpc.Channel'):\n        self._autostop_stub = autostopv1_pb2_grpc.AutostopServiceStub(channel)\n        self._jobs_stub = jobsv1_pb2_grpc.JobsServiceStub(channel)\n        self._serve_stub = servev1_pb2_grpc.ServeServiceStub(channel)\n        self._managed_jobs_stub = (\n            managed_jobsv1_pb2_grpc.ManagedJobsServiceStub(channel))\n\n    def set_autostop(\n        self,\n        request: 'autostopv1_pb2.SetAutostopRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'autostopv1_pb2.SetAutostopResponse':\n        return self._autostop_stub.SetAutostop(request, timeout=timeout)\n\n    def is_autostopping(\n        self,\n        request: 'autostopv1_pb2.IsAutostoppingRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'autostopv1_pb2.IsAutostoppingResponse':\n        return self._autostop_stub.IsAutostopping(request, timeout=timeout)\n\n    def add_job(\n        self,\n        request: 'jobsv1_pb2.AddJobRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.AddJobResponse':\n        return self._jobs_stub.AddJob(request, timeout=timeout)\n\n    def queue_job(\n        self,\n        request: 'jobsv1_pb2.QueueJobRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.QueueJobResponse':\n        return self._jobs_stub.QueueJob(request, timeout=timeout)\n\n    def update_status(\n        self,\n        request: 'jobsv1_pb2.UpdateStatusRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.UpdateStatusResponse':\n        return self._jobs_stub.UpdateStatus(request, timeout=timeout)\n\n    def get_job_queue(\n        self,\n        request: 'jobsv1_pb2.GetJobQueueRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.GetJobQueueResponse':\n        return self._jobs_stub.GetJobQueue(request, timeout=timeout)\n\n    def cancel_jobs(\n        self,\n        request: 'jobsv1_pb2.CancelJobsRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.CancelJobsResponse':\n        return self._jobs_stub.CancelJobs(request, timeout=timeout)\n\n    def fail_all_in_progress_jobs(\n        self,\n        request: 'jobsv1_pb2.FailAllInProgressJobsRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.FailAllInProgressJobsResponse':\n        return self._jobs_stub.FailAllInProgressJobs(request, timeout=timeout)\n\n    def get_job_status(\n        self,\n        request: 'jobsv1_pb2.GetJobStatusRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.GetJobStatusResponse':\n        return self._jobs_stub.GetJobStatus(request, timeout=timeout)\n\n    def get_job_submitted_timestamp(\n        self,\n        request: 'jobsv1_pb2.GetJobSubmittedTimestampRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.GetJobSubmittedTimestampResponse':\n        return self._jobs_stub.GetJobSubmittedTimestamp(request,\n                                                        timeout=timeout)\n\n    def get_job_ended_timestamp(\n        self,\n        request: 'jobsv1_pb2.GetJobEndedTimestampRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.GetJobEndedTimestampResponse':\n        return self._jobs_stub.GetJobEndedTimestamp(request, timeout=timeout)\n\n    def get_log_dirs_for_jobs(\n        self,\n        request: 'jobsv1_pb2.GetLogDirsForJobsRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.GetLogDirsForJobsResponse':\n        return self._jobs_stub.GetLogDirsForJobs(request, timeout=timeout)\n\n    def get_job_exit_codes(\n        self,\n        request: 'jobsv1_pb2.GetJobExitCodesRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'jobsv1_pb2.GetJobExitCodesResponse':\n        return self._jobs_stub.GetJobExitCodes(request, timeout=timeout)\n\n    def tail_logs(\n        self,\n        request: 'jobsv1_pb2.TailLogsRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> Iterator['jobsv1_pb2.TailLogsResponse']:\n        return self._jobs_stub.TailLogs(request, timeout=timeout)\n\n    def get_service_status(\n        self,\n        request: 'servev1_pb2.GetServiceStatusRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'servev1_pb2.GetServiceStatusResponse':\n        return self._serve_stub.GetServiceStatus(request, timeout=timeout)\n\n    def add_serve_version(\n        self,\n        request: 'servev1_pb2.AddVersionRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'servev1_pb2.AddVersionResponse':\n        return self._serve_stub.AddVersion(request, timeout=timeout)\n\n    def terminate_services(\n        self,\n        request: 'servev1_pb2.TerminateServicesRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'servev1_pb2.TerminateServicesResponse':\n        return self._serve_stub.TerminateServices(request, timeout=timeout)\n\n    def terminate_replica(\n        self,\n        request: 'servev1_pb2.TerminateReplicaRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'servev1_pb2.TerminateReplicaResponse':\n        return self._serve_stub.TerminateReplica(request, timeout=timeout)\n\n    def wait_service_registration(\n        self,\n        request: 'servev1_pb2.WaitServiceRegistrationRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'servev1_pb2.WaitServiceRegistrationResponse':\n        # set timeout to at least 10 seconds more than service register\n        # constant to make sure that timeouts will not occur.\n        if timeout is not None:\n            timeout = max(timeout,\n                          serve_constants.SERVICE_REGISTER_TIMEOUT_SECONDS + 10)\n        return self._serve_stub.WaitServiceRegistration(request,\n                                                        timeout=timeout)\n\n    def update_service(\n        self,\n        request: 'servev1_pb2.UpdateServiceRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'servev1_pb2.UpdateServiceResponse':\n        return self._serve_stub.UpdateService(request, timeout=timeout)\n\n    def get_managed_job_controller_version(\n        self,\n        request: 'managed_jobsv1_pb2.GetVersionRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'managed_jobsv1_pb2.GetVersionResponse':\n        return self._managed_jobs_stub.GetVersion(request, timeout=timeout)\n\n    def get_managed_job_table(\n        self,\n        request: 'managed_jobsv1_pb2.GetJobTableRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'managed_jobsv1_pb2.GetJobTableResponse':\n        return self._managed_jobs_stub.GetJobTable(request, timeout=timeout)\n\n    def get_all_managed_job_ids_by_name(\n        self,\n        request: 'managed_jobsv1_pb2.GetAllJobIdsByNameRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'managed_jobsv1_pb2.GetAllJobIdsByNameResponse':\n        return self._managed_jobs_stub.GetAllJobIdsByName(request,\n                                                          timeout=timeout)\n\n    def cancel_managed_jobs(\n        self,\n        request: 'managed_jobsv1_pb2.CancelJobsRequest',\n        timeout: Optional[float] = constants.SKYLET_GRPC_TIMEOUT_SECONDS\n    ) -> 'managed_jobsv1_pb2.CancelJobsResponse':\n        return self._managed_jobs_stub.CancelJobs(request, timeout=timeout)\n\n\n@registry.BACKEND_REGISTRY.type_register(name='cloudvmray')\nclass CloudVmRayBackend(backends.Backend['CloudVmRayResourceHandle']):\n    \"\"\"Backend: runs on cloud virtual machines, managed by Ray.\n\n    Changing this class may also require updates to:\n      * Cloud providers' templates under config/\n      * Cloud providers' implementations under clouds/\n    \"\"\"\n\n    NAME = 'cloudvmray'\n\n    # Backward compatibility, with the old name of the handle.\n    ResourceHandle = CloudVmRayResourceHandle  # type: ignore\n\n    def __init__(self):\n        self.run_timestamp = sky_logging.get_run_timestamp()\n        # NOTE: do not expanduser() here, as this '~/...' path is used for\n        # remote as well to be expanded on the remote side.\n        self.log_dir = os.path.join(constants.SKY_LOGS_DIRECTORY,\n                                    self.run_timestamp)\n        # Do not make directories to avoid create folder for commands that\n        # do not need it (`sky status`, `sky logs` ...)\n        # os.makedirs(self.log_dir, exist_ok=True)\n\n        self._dag = None\n        self._optimize_target = None\n        self._requested_features = set()\n        self._dump_final_script = False\n        self._is_managed = False\n        # Optional planner (via register_info): used under the per-cluster lock\n        # to produce a fresh concrete plan when neither a reusable snapshot nor\n        # a caller plan is available.\n        self._planner = None\n\n        # Command for running the setup script. It is only set when the\n        # setup needs to be run outside the self._setup() and as part of\n        # a job (detach_setup, default).\n        self._setup_cmd = None\n\n    # --- Implementation of Backend APIs ---\n\n    def register_info(self, **kwargs) -> None:\n        self._dag = kwargs.pop('dag', self._dag)\n        self._optimize_target = kwargs.pop(\n            'optimize_target',\n            self._optimize_target) or common.OptimizeTarget.COST\n        self._requested_features = kwargs.pop('requested_features',\n                                              self._requested_features)\n        self._dump_final_script = kwargs.pop('dump_final_script', False)\n        self._is_managed = kwargs.pop('is_managed', False)\n        # Optional planner callback for a fresh plan under lock when no\n        # reusable snapshot/caller plan exists. Keeps optimizer in upper layer.\n        self._planner = kwargs.pop('planner', self._planner)\n        assert not kwargs, f'Unexpected kwargs: {kwargs}'\n\n    def check_resources_fit_cluster(\n        self,\n        handle: CloudVmRayResourceHandle,\n        task: task_lib.Task,\n        check_ports: bool = False,\n    ) -> resources_lib.Resources:\n        \"\"\"Check if resources requested by the task fit the cluster.\n\n        The resources requested by the task should be smaller than the existing\n        cluster.\n        If multiple resources are specified, this checking will pass when\n        at least one resource fits the cluster.\n\n        Raises:\n            exceptions.ResourcesMismatchError: If the resources in the task\n                does not match the existing cluster.\n        \"\"\"\n\n        launched_resources = handle.launched_resources\n        cluster_name = handle.cluster_name\n\n        # Usage Collection:\n        usage_lib.messages.usage.update_cluster_resources(\n            handle.launched_nodes, launched_resources)\n        status = global_user_state.get_status_from_cluster_name(cluster_name)\n        if status is not None:\n            usage_lib.messages.usage.update_cluster_status(status)\n\n        assert launched_resources.region is not None, handle\n\n        mismatch_str = (f'To fix: specify a new cluster name, or down the '\n                        f'existing cluster first: sky down {cluster_name}')\n        valid_resource = None\n        requested_resource_list = []\n        for resource in task.resources:\n            if (task.num_nodes <= handle.launched_nodes and\n                    resource.less_demanding_than(\n                        launched_resources,\n                        requested_num_nodes=task.num_nodes,\n                        check_ports=check_ports)):\n                valid_resource = resource\n                break\n            else:\n                requested_resource_list.append(f'{task.num_nodes}x {resource}')\n\n        if valid_resource is None:\n            for example_resource in task.resources:\n                if (example_resource.region is not None and\n                        example_resource.region != launched_resources.region):\n                    with ux_utils.print_exception_no_traceback():\n                        raise exceptions.ResourcesMismatchError(\n                            f'Task requested resources {example_resource} in region '  # pylint: disable=line-too-long\n                            f'{example_resource.region!r}'\n                            ', but the existing cluster '\n                            f'is in region {launched_resources.region!r}.')\n                if (example_resource.zone is not None and\n                        example_resource.zone != launched_resources.zone):\n                    zone_str = (f'is in zone {launched_resources.zone!r}.'\n                                if launched_resources.zone is not None else\n                                'does not have zone specified.')\n                    with ux_utils.print_exception_no_traceback():\n                        raise exceptions.ResourcesMismatchError(\n                            f'Task requested resources {example_resource} in zone '  # pylint: disable=line-too-long\n                            f'{example_resource.zone!r},'\n                            'but the existing cluster '\n                            f'{zone_str}')\n                if (example_resource.requires_fuse and\n                        not launched_resources.requires_fuse):\n                    # Will not be reached for non-k8s case since the\n                    # less_demanding_than only fails fuse requirement when\n                    # the cloud is Kubernetes AND the cluster doesn't have fuse.\n                    with ux_utils.print_exception_no_traceback():\n                        raise exceptions.ResourcesMismatchError(\n                            'Task requires FUSE support for mounting object '\n                            'stores, but the existing cluster with '\n                            f'{launched_resources!r} does not support FUSE '\n                            f'mounting. Launch a new cluster to run this task.')\n            requested_resource_str = ', '.join(requested_resource_list)\n            if isinstance(task.resources, list):\n                requested_resource_str = f'[{requested_resource_str}]'\n            elif isinstance(task.resources, set):\n                requested_resource_str = f'{{{requested_resource_str}}}'\n            with ux_utils.print_exception_no_traceback():\n                raise exceptions.ResourcesMismatchError(\n                    'Requested resources do not match the existing '\n                    'cluster.\\n'\n                    f'  Requested:\\t{requested_resource_str}\\n'\n                    f'  Existing:\\t{handle.launched_nodes}x '\n                    f'{handle.launched_resources}\\n'\n                    f'{mismatch_str}')\n        else:\n            # For fractional acc count clusters, we round up the number of accs\n            # to 1 (sky/utils/resources_utils.py::make_ray_custom_resources_str)\n            # Here we scale the required acc count to (required / launched) * 1\n            # so the total number of accs is the same as the requested number.\n            launched_accs = launched_resources.accelerators\n            if (launched_accs is not None and\n                    valid_resource.accelerators is not None):\n                for _, count in launched_accs.items():\n                    if isinstance(count, float) and not count.is_integer():\n                        valid_resource = valid_resource.copy(\n                            accelerators={\n                                k: v / count\n                                for k, v in valid_resource.accelerators.items()\n                            })\n        return valid_resource\n\n    def _provision(\n        self,\n        task: task_lib.Task,\n        to_provision: Optional[resources_lib.Resources],\n        dryrun: bool,\n        stream_logs: bool,\n        cluster_name: str,\n        retry_until_up: bool = False,\n        skip_unnecessary_provisioning: bool = False,\n    ) -> Tuple[Optional[CloudVmRayResourceHandle], bool]:\n        \"\"\"Provisions the cluster, or re-provisions an existing cluster.\n\n        Use the SKYPILOT provisioner if it's supported by the cloud, otherwise\n        use 'ray up'.\n\n        See also docstring for Backend.provision().\n\n        Raises:\n            exceptions.ClusterOwnerIdentityMismatchError: if the cluster\n                'cluster_name' exists and is owned by another user.\n            exceptions.InvalidClusterNameError: if the cluster name is invalid.\n            exceptions.ResourcesMismatchError: if the requested resources\n                do not match the existing cluster.\n            exceptions.ResourcesUnavailableError: if the requested resources\n                cannot be satisfied. The failover_history of the exception\n                will be set as at least 1 exception from either our pre-checks\n                (e.g., cluster name invalid) or a region/zone throwing\n                resource unavailability.\n            exceptions.CommandError: any ssh command error.\n            RuntimeError: raised when 'rsync' is not installed.\n            # TODO(zhwu): complete the list of exceptions.\n        \"\"\"\n        # FIXME: ray up for Azure with different cluster_names will overwrite\n        # each other.\n        # When rsync is not installed in the user's machine, Ray will\n        # silently retry to up the node for _MAX_RAY_UP_RETRY number\n        # of times. This is time consuming so we fail early.\n        backend_utils.check_rsync_installed()\n        # Check if the cluster is owned by the current user. Raise\n        # exceptions.ClusterOwnerIdentityMismatchError\n        backend_utils.check_owner_identity(cluster_name)\n        lock_id = backend_utils.cluster_status_lock_id(cluster_name)\n        communicated_with_user = False\n\n        while True:\n            try:\n                return self._locked_provision(lock_id, task, to_provision,\n                                              dryrun, stream_logs, cluster_name,\n                                              retry_until_up,\n                                              skip_unnecessary_provisioning)\n            except locks.LockTimeout:\n                if not communicated_with_user:\n                    rich_utils.force_update_status(\n                        ux_utils.spinner_message('Launching - blocked by ' +\n                                                 'other requests ' +\n                                                 colorama.Style.RESET_ALL +\n                                                 colorama.Style.DIM +\n                                                 'Check concurrent requests: ' +\n                                                 'sky api status -v | grep '\n                                                 f'{cluster_name}'))\n\n    def _maybe_clear_external_cluster_failures(\n            self, cluster_name: str,\n            prev_cluster_status: Optional[status_lib.ClusterStatus]) -> None:\n        \"\"\"Clear any existing cluster failures when reusing a cluster.\n\n        Clear any existing cluster failures when reusing a cluster. This ensures\n        that when a cluster failure is detected (causing the cluster to be\n        marked as INIT), the user can recover the cluster via `sky start` or\n        `sky launch` and clear the failure.\n        \"\"\"\n        if prev_cluster_status is not None:\n            failures = ExternalFailureSource.clear(cluster_name=cluster_name)\n            if failures:\n                failure_details = [f'\"{f[\"failure_mode\"]}\"' for f in failures]\n                plural = 's' if len(failures) > 1 else ''\n                logger.info(f'{colorama.Style.DIM}Cleared {len(failures)} '\n                            f'existing cluster failure{plural} for cluster '\n                            f'{cluster_name!r}: {\", \".join(failure_details)}'\n                            f'{colorama.Style.RESET_ALL}')\n\n    def _locked_provision(\n        self,\n        lock_id: str,\n        task: task_lib.Task,\n        to_provision: Optional[resources_lib.Resources],\n        dryrun: bool,\n        stream_logs: bool,\n        cluster_name: str,\n        retry_until_up: bool = False,\n        skip_unnecessary_provisioning: bool = False,\n    ) -> Tuple[Optional[CloudVmRayResourceHandle], bool]:\n        with lock_events.DistributedLockEvent(lock_id, _CLUSTER_LOCK_TIMEOUT):\n            # Reset spinner message to remove any mention of being blocked\n            # by other requests.\n            rich_utils.force_update_status(\n                ux_utils.spinner_message('Launching'))\n\n            # Try to launch the exiting cluster first. If no existing\n            # cluster, this function will create a to_provision_config\n            # with required resources.\n            to_provision_config = self._check_existing_cluster(\n                task, to_provision, cluster_name, dryrun)\n            assert to_provision_config.resources is not None, (\n                'to_provision should not be None', to_provision_config)\n\n            prev_cluster_status = to_provision_config.prev_cluster_status\n            usage_lib.messages.usage.update_cluster_resources(\n                to_provision_config.num_nodes, to_provision_config.resources)\n            usage_lib.messages.usage.update_cluster_status(prev_cluster_status)\n\n            self._maybe_clear_external_cluster_failures(cluster_name,\n                                                        prev_cluster_status)\n\n            # TODO(suquark): once we have sky on PyPI, we should directly\n            # install sky from PyPI.\n            # NOTE: can take ~2s.\n            with timeline.Event('backend.provision.wheel_build'):\n                # TODO(suquark): once we have sky on PyPI, we should directly\n                # install sky from PyPI.\n                local_wheel_path, wheel_hash = wheel_utils.build_sky_wheel()\n            while True:\n                # For on-demand instances, RetryingVmProvisioner will retry\n                # within the given region first, then optionally retry on all\n                # other clouds and regions (if backend.register_info()\n                # has been called).\n                # For spot instances, each provisioning request is made for a\n                # single zone and the provisioner will retry on all other\n                # clouds, regions, and zones.\n                # See optimizer.py#_make_launchables_for_valid_region_zones()\n                # for detailed reasons.\n\n                # After this \"round\" of optimization across clouds, provisioning\n                # may still have not succeeded. This while loop will then kick\n                # in if retry_until_up is set, which will kick off new \"rounds\"\n                # of optimization infinitely.\n                try:\n                    retry_provisioner = RetryingVmProvisioner(\n                        self.log_dir,\n                        self._dag,  # type: ignore[arg-type]\n                        self._optimize_target,  # type: ignore[arg-type]\n                        self._requested_features,\n                        local_wheel_path,\n                        wheel_hash,\n                        blocked_resources=task.blocked_resources,\n                        is_managed=self._is_managed)\n                    log_path = os.path.join(self.log_dir, 'provision.log')\n                    rich_utils.force_update_status(\n                        ux_utils.spinner_message('Launching',\n                                                 log_path,\n                                                 cluster_name=cluster_name))\n                    config_dict = retry_provisioner.provision_with_retries(\n                        task, to_provision_config, dryrun, stream_logs,\n                        skip_unnecessary_provisioning)\n                    break\n                except exceptions.ResourcesUnavailableError as e:\n                    log_path = retry_provisioner.log_dir + '/provision.log'\n\n                    error_message = (\n                        f'{colorama.Fore.RED}Failed to provision all '\n                        f'possible launchable resources.'\n                        f'{colorama.Style.RESET_ALL}'\n                        ' Relax the task\\'s resource requirements: '\n                        f'{task.num_nodes}x {list(task.resources)[0]}')\n                    if e.no_failover:\n                        error_message = str(e)\n\n                    if retry_until_up:\n                        gap_seconds = _RETRY_UNTIL_UP_INIT_GAP_SECONDS\n                        retry_message = ux_utils.retry_message(\n                            f'Retry after {gap_seconds:.0f}s ')\n                        hint_message = (\n                            f'\\n{retry_message} '\n                            f'{ux_utils.provision_hint(cluster_name)}'\n                            f'{colorama.Style.RESET_ALL}')\n\n                        # Add cluster event for retry.\n                        global_user_state.add_cluster_event(\n                            cluster_name, status_lib.ClusterStatus.INIT,\n                            f'Retrying provisioning after {gap_seconds:.0f}s',\n                            global_user_state.ClusterEventType.STATUS_CHANGE)\n\n                        raise exceptions.ExecutionRetryableError(\n                            error_message,\n                            hint=hint_message,\n                            retry_wait_seconds=gap_seconds)\n                    # Clean up the cluster's entry in `sky status`.\n                    # Do not remove the stopped cluster from the global state\n                    # if failed to start.\n                    if not e.no_failover:\n                        global_user_state.add_cluster_event(\n                            cluster_name,\n                            None,\n                            'Provision failed: ' + str(e),\n                            global_user_state.ClusterEventType.STATUS_CHANGE,\n                            nop_if_duplicate=True)\n                        global_user_state.remove_cluster(cluster_name,\n                                                         terminate=True)\n                        usage_lib.messages.usage.update_final_cluster_status(\n                            None)\n                    logger.error(\n                        ux_utils.error_message(\n                            'Failed to provision resources. '\n                            f'{ux_utils.provision_hint(cluster_name)}'))\n                    error_message += (\n                        '\\nTo keep retrying until the cluster is up, use '\n                        'the `--retry-until-up` flag.')\n                    with ux_utils.print_exception_no_traceback():\n                        raise exceptions.ResourcesUnavailableError(\n                            error_message + '\\n' + str(e),\n                            failover_history=e.failover_history) from None\n            if dryrun:\n                handle = global_user_state.get_handle_from_cluster_name(\n                    cluster_name)\n                return handle if handle is not None else None, False\n\n            if config_dict['provisioning_skipped']:\n                # Skip further provisioning.\n                # In this case, we won't have certain fields in the config_dict\n                # ('handle', 'provision_record', 'resources_vars')\n                # We need to return the handle - but it should be the existing\n                # handle for the cluster.\n                handle = global_user_state.get_handle_from_cluster_name(\n                    cluster_name)\n                assert handle is not None, (cluster_name, handle)\n                return handle, True\n\n            if 'provision_record' in config_dict:\n                # New provisioner is used here.\n                handle = config_dict['handle']\n                provision_record = config_dict['provision_record']\n                resources_vars = config_dict['resources_vars']\n                config_hash = config_dict.get('config_hash', None)\n\n                # Setup SkyPilot runtime after the cluster is provisioned\n                # 1. Wait for SSH to be ready.\n                # 2. Mount the cloud credentials, skypilot wheel,\n                #    and other necessary files to the VM.\n                # 3. Run setup commands to install dependencies.\n                # 4. Starting ray cluster and skylet.\n\n                # Add cluster event for runtime setup start\n                global_user_state.add_cluster_event(\n                    handle.cluster_name, status_lib.ClusterStatus.INIT,\n                    'Setting up SkyPilot runtime on cluster',\n                    global_user_state.ClusterEventType.STATUS_CHANGE)\n\n                cluster_info = provisioner.post_provision_runtime_setup(\n                    handle.launched_resources,\n                    resources_utils.ClusterName(handle.cluster_name,\n                                                handle.cluster_name_on_cloud),\n                    handle.cluster_yaml,\n                    provision_record=provision_record,\n                    custom_resource=resources_vars.get('custom_resources'),\n                    log_dir=self.log_dir)\n                # We use the IPs from the cluster_info to update_cluster_ips,\n                # when the provisioning is done, to make sure the cluster IPs\n                # are up-to-date.\n                # The staled IPs may be caused by the node being restarted\n                # manually or by the cloud provider.\n                # Optimize the case where the cluster's IPs can be retrieved\n                # from cluster_info.\n                handle.cached_cluster_info = cluster_info\n                handle.docker_user = cluster_info.docker_user\n                handle.update_cluster_ips(max_attempts=_FETCH_IP_MAX_ATTEMPTS,\n                                          cluster_info=cluster_info)\n                handle.update_ssh_ports(max_attempts=_FETCH_IP_MAX_ATTEMPTS)\n\n                # Update launched resources.\n                handle.launched_resources = handle.launched_resources.copy(\n                    region=provision_record.region, zone=provision_record.zone)\n\n                self._update_after_cluster_provisioned(\n                    handle, to_provision_config.prev_handle, task,\n                    prev_cluster_status, config_hash)\n                return handle, False\n\n            cluster_config_file = config_dict['ray']\n            handle = config_dict['handle']\n\n            ip_list = handle.external_ips()\n            ssh_port_list = handle.external_ssh_ports()\n            assert ip_list is not None, handle\n            assert ssh_port_list is not None, handle\n            config = global_user_state.get_cluster_yaml_dict(\n                cluster_config_file)\n            if 'docker' in config:\n                handle.setup_docker_user(cluster_config_file)\n\n            # Get actual zone info and save it into handle.\n            # NOTE: querying zones is expensive, observed 1node GCP >=4s.\n            zone = handle.launched_resources.zone\n            if zone is None:\n                get_zone_cmd = (\n                    handle.launched_resources.cloud.get_zone_shell_cmd())\n                # zone is None for Azure\n                if get_zone_cmd is not None:\n                    runners = handle.get_command_runners()\n\n                    def _get_zone(runner):\n                        retry_count = 0\n                        backoff = common_utils.Backoff(initial_backoff=1,\n                                                       max_backoff_factor=3)\n                        while True:\n                            returncode, stdout, stderr = runner.run(\n                                get_zone_cmd,\n                                require_outputs=True,\n                                stream_logs=False)\n                            if returncode == 0:\n                                break\n                            retry_count += 1\n                            if retry_count <= _MAX_GET_ZONE_RETRY:\n                                time.sleep(backoff.current_backoff())\n                                continue\n                        subprocess_utils.handle_returncode(\n                            returncode,\n                            get_zone_cmd,\n                            f'Failed to get zone for {cluster_name!r}',\n                            stderr=stderr,\n                            stream_logs=stream_logs)\n                        return stdout.strip()\n\n                    zones = subprocess_utils.run_in_parallel(_get_zone, runners)\n                    if len(set(zones)) == 1:\n                        # zone will be checked during Resources cls\n                        # initialization.\n                        handle.launched_resources = (\n                            handle.launched_resources.copy(zone=zones[0]))\n                    # If the number of zones > 1, nodes in the cluster are\n                    # launched in different zones (legacy clusters before\n                    # #1700), leave the zone field of handle.launched_resources\n                    # to None.\n\n            # For backward compatibility and robustness of skylet, it is checked\n            # and restarted if necessary.\n            logger.debug('Checking if skylet is running on the head node.')\n            with rich_utils.safe_status(\n                    ux_utils.spinner_message('Preparing SkyPilot runtime')):\n                # We need to source bashrc for skylet to make sure the autostop\n                # event can access the path to the cloud CLIs.\n                self.run_on_head(handle,\n                                 instance_setup.MAYBE_SKYLET_RESTART_CMD,\n                                 source_bashrc=True)\n\n            self._update_after_cluster_provisioned(\n                handle, to_provision_config.prev_handle, task,\n                prev_cluster_status, config_hash)\n            return handle, False\n\n    def _open_ports(self, handle: CloudVmRayResourceHandle) -> None:\n        cloud = handle.launched_resources.cloud\n        logger.debug(\n            f'Opening ports {handle.launched_resources.ports} for {cloud}')\n        config = global_user_state.get_cluster_yaml_dict(handle.cluster_yaml)\n        provider_config = config['provider']\n        provision_lib.open_ports(repr(cloud), handle.cluster_name_on_cloud,\n                                 handle.launched_resources.ports,\n                                 provider_config)\n\n    def _update_after_cluster_provisioned(\n            self, handle: CloudVmRayResourceHandle,\n            prev_handle: Optional[CloudVmRayResourceHandle],\n            task: task_lib.Task,\n            prev_cluster_status: Optional[status_lib.ClusterStatus],\n            config_hash: str) -> None:\n        usage_lib.messages.usage.update_cluster_resources(\n            handle.launched_nodes, handle.launched_resources)\n        usage_lib.messages.usage.update_final_cluster_status(\n            status_lib.ClusterStatus.UP)\n\n        # Update job queue to avoid stale jobs (when restarted), before\n        # setting the cluster to be ready.\n        if prev_cluster_status == status_lib.ClusterStatus.INIT:\n            # update_status will query the ray job status for all INIT /\n            # PENDING / RUNNING jobs for the real status, since we do not\n            # know the actual previous status of the cluster.\n            logger.debug('Update job queue on remote cluster.')\n            with rich_utils.safe_status(\n                    ux_utils.spinner_message('Preparing SkyPilot runtime')):\n                use_legacy = not handle.is_grpc_enabled_with_flag\n\n                if not use_legacy:\n                    try:\n                        request = jobsv1_pb2.UpdateStatusRequest()\n                        backend_utils.invoke_skylet_with_retries(\n                            lambda: SkyletClient(handle.get_grpc_channel()\n                                                ).update_status(request))\n                    except exceptions.SkyletMethodNotImplementedError:\n                        use_legacy = True\n\n                if use_legacy:\n                    cmd = job_lib.JobLibCodeGen.update_status()\n                    returncode, _, stderr = self.run_on_head(\n                        handle, cmd, require_outputs=True)\n                    subprocess_utils.handle_returncode(\n                        returncode, cmd, 'Failed to update job status.', stderr)\n        if prev_cluster_status == status_lib.ClusterStatus.STOPPED:\n            # Safely set all the previous jobs to FAILED since the cluster\n            # is restarted\n            # An edge case here due to racing:\n            # 1. A job finishes RUNNING, but right before it update itself\n            # to SUCCEEDED, the cluster is STOPPED by `sky stop`.\n            # 2. On next `sky start`, it gets reset to FAILED.\n            use_legacy = not handle.is_grpc_enabled_with_flag\n\n            if not use_legacy:\n                try:\n                    fail_request = jobsv1_pb2.FailAllInProgressJobsRequest()\n                    backend_utils.invoke_skylet_with_retries(\n                        lambda: SkyletClient(handle.get_grpc_channel(\n                        )).fail_all_in_progress_jobs(fail_request))\n                except exceptions.SkyletMethodNotImplementedError:\n                    use_legacy = True\n\n            if use_legacy:\n                cmd = job_lib.JobLibCodeGen.fail_all_jobs_in_progress()\n                returncode, stdout, stderr = self.run_on_head(\n                    handle, cmd, require_outputs=True)\n                subprocess_utils.handle_returncode(\n                    returncode, cmd,\n                    'Failed to set previously in-progress jobs to FAILED',\n                    stdout + stderr)\n\n        prev_ports = None\n        if prev_handle is not None:\n            prev_ports = prev_handle.launched_resources.ports\n        current_ports = handle.launched_resources.ports\n        open_new_ports = bool(\n            resources_utils.port_ranges_to_set(current_ports) -\n            resources_utils.port_ranges_to_set(prev_ports))\n        if open_new_ports:\n            launched_resources = handle.launched_resources.assert_launchable()\n            if not (launched_resources.cloud.OPEN_PORTS_VERSION <=\n                    clouds.OpenPortsVersion.LAUNCH_ONLY):\n                with rich_utils.safe_status(\n                        ux_utils.spinner_message(\n                            'Launching - Opening new ports')):\n                    self._open_ports(handle)\n\n        # Capture task YAML and command\n        user_specified_task_config = None\n        if task is not None:\n            user_specified_task_config = task.to_yaml_config(\n                use_user_specified_yaml=True)\n\n        with timeline.Event('backend.provision.post_process'):\n            global_user_state.add_or_update_cluster(\n                handle.cluster_name,\n                handle,\n                set(task.resources),\n                ready=True,\n                config_hash=config_hash,\n                task_config=user_specified_task_config,\n            )\n\n            # Add cluster event for successful provisioning.\n            global_user_state.add_cluster_event(\n                handle.cluster_name, status_lib.ClusterStatus.UP,\n                'Cluster successfully provisioned with ' +\n                f'{handle.launched_nodes} nodes',\n                global_user_state.ClusterEventType.STATUS_CHANGE)\n\n            usage_lib.messages.usage.update_final_cluster_status(\n                status_lib.ClusterStatus.UP)\n            # We still add the cluster to ssh config file on API server, this\n            # is helpful for people trying to use `sky launch`'ed cluster for\n            # ssh proxy jump.\n            auth_config = backend_utils.ssh_credential_from_yaml(\n                handle.cluster_yaml,\n                ssh_user=handle.ssh_user,\n                docker_user=handle.docker_user)\n            cluster_utils.SSHConfigHelper.add_cluster(\n                handle.cluster_name, handle.cluster_name_on_cloud,\n                handle.cached_external_ips, auth_config,\n                handle.cached_external_ssh_ports, handle.docker_user,\n                handle.ssh_user)\n\n    def _sync_workdir(self, handle: CloudVmRayResourceHandle,\n                      workdir: Union[Path, Dict[str, Any]],\n                      envs_and_secrets: Dict[str, str]) -> None:\n        # Even though provision() takes care of it, there may be cases where\n        # this function is called in isolation, without calling provision(),\n        # e.g., in CLI.  So we should rerun rsync_up.\n        if isinstance(workdir, dict):\n            self._sync_git_workdir(handle, envs_and_secrets)\n        else:\n            self._sync_path_workdir(handle, workdir)\n\n    def _sync_git_workdir(self, handle: CloudVmRayResourceHandle,\n                          envs_and_secrets: Dict[str, str]) -> None:\n        style = colorama.Style\n        ip_list = handle.external_ips()\n        assert ip_list is not None, 'external_ips is not cached in handle'\n\n        log_path = os.path.join(self.log_dir, 'workdir_sync.log')\n\n        # TODO(zhwu): refactor this with backend_utils.parallel_cmd_with_rsync\n        runners = handle.get_command_runners()\n\n        def _sync_git_workdir_node(\n                runner: command_runner.CommandRunner) -> None:\n            # Type assertion to help mypy understand the type\n            assert hasattr(\n                runner, 'git_clone'\n            ), f'CommandRunner should have git_clone method, ' \\\n                f'got {type(runner)}'\n            runner.git_clone(\n                target_dir=SKY_REMOTE_WORKDIR,\n                log_path=log_path,\n                stream_logs=False,\n                max_retry=3,\n                envs_and_secrets=envs_and_secrets,\n            )\n\n        num_nodes = handle.launched_nodes\n        plural = 's' if num_nodes > 1 else ''\n        logger.info(\n            f'  {style.DIM}Syncing workdir (to {num_nodes} node{plural}): '\n            f'{SKY_REMOTE_WORKDIR}{style.RESET_ALL}')\n        os.makedirs(os.path.expanduser(self.log_dir), exist_ok=True)\n        os.system(f'touch {log_path}')\n        num_threads = subprocess_utils.get_parallel_threads(\n            str(handle.launched_resources.cloud))\n        with rich_utils.safe_status(\n                ux_utils.spinner_message('Syncing workdir', log_path)):\n            subprocess_utils.run_in_parallel(_sync_git_workdir_node, runners,\n                                             num_threads)\n        logger.info(ux_utils.finishing_message('Synced workdir.', log_path))\n\n    def _sync_path_workdir(self, handle: CloudVmRayResourceHandle,\n                           workdir: Path) -> None:\n        fore = colorama.Fore\n        style = colorama.Style\n        ip_list = handle.external_ips()\n        assert ip_list is not None, 'external_ips is not cached in handle'\n        full_workdir = os.path.abspath(os.path.expanduser(workdir))\n\n        # These asserts have been validated at Task construction time.\n        assert os.path.exists(full_workdir), f'{full_workdir} does not exist'\n        if os.path.islink(full_workdir):\n            logger.warning(\n                f'{fore.YELLOW}Workdir {workdir!r} is a symlink. '\n                f'Symlink contents are not uploaded.{style.RESET_ALL}')\n        else:\n            assert os.path.isdir(\n                full_workdir), f'{full_workdir} should be a directory.'\n\n        # Raise warning if directory is too large\n        dir_size = backend_utils.path_size_megabytes(full_workdir)\n        if dir_size >= _PATH_SIZE_MEGABYTES_WARN_THRESHOLD:\n            logger.warning(\n                f'  {fore.YELLOW}The size of workdir {workdir!r} '\n                f'is {dir_size} MB. Try to keep workdir small or use '\n                '.skyignore to exclude large files, as large sizes will slow '\n                f'down rsync.{style.RESET_ALL}')\n\n        log_path = os.path.join(self.log_dir, 'workdir_sync.log')\n\n        # TODO(zhwu): refactor this with backend_utils.parallel_cmd_with_rsync\n        runners = handle.get_command_runners()\n\n        def _sync_workdir_node(runner: command_runner.CommandRunner) -> None:\n            runner.rsync(\n                source=workdir,\n                target=SKY_REMOTE_WORKDIR,\n                up=True,\n                log_path=log_path,\n                stream_logs=False,\n            )\n\n        num_nodes = handle.launched_nodes\n        plural = 's' if num_nodes > 1 else ''\n        logger.info(\n            f'  {style.DIM}Syncing workdir (to {num_nodes} node{plural}): '\n            f'{workdir} -> {SKY_REMOTE_WORKDIR}{style.RESET_ALL}')\n        os.makedirs(os.path.expanduser(self.log_dir), exist_ok=True)\n        os.system(f'touch {log_path}')\n        num_threads = subprocess_utils.get_parallel_threads(\n            str(handle.launched_resources.cloud))\n        with rich_utils.safe_status(\n                ux_utils.spinner_message('Syncing workdir', log_path)):\n            subprocess_utils.run_in_parallel(_sync_workdir_node, runners,\n                                             num_threads)\n        logger.info(ux_utils.finishing_message('Synced workdir.', log_path))\n\n    def _sync_file_mounts(\n        self,\n        handle: CloudVmRayResourceHandle,\n        all_file_mounts: Optional[Dict[Path, Path]],\n        storage_mounts: Optional[Dict[Path, storage_lib.Storage]],\n    ) -> None:\n        \"\"\"Mounts all user files to the remote nodes.\n\n        Note: This does not handle COPY storage_mounts. These should have\n        already been translated into file_mounts by task.sync_storage_mounts().\n\n        TODO: Delete COPY storage_mounts in task.sync_storage_mounts(), and\n        assert here that all storage_mounts are MOUNT mode.\n        \"\"\"\n        launched_resources = handle.launched_resources.assert_launchable()\n        with rich_utils.safe_status(ux_utils.spinner_message('Syncing files')):\n            controller_utils.replace_skypilot_config_path_in_file_mounts(\n                launched_resources.cloud, all_file_mounts)\n            self._execute_file_mounts(handle, all_file_mounts)\n            self._execute_storage_mounts(handle, storage_mounts)\n            self._set_storage_mounts_metadata(handle.cluster_name,\n                                              storage_mounts)\n\n    def _get_num_gpus(self, task: task_lib.Task) -> int:\n        if task.resources is not None:\n            for resource in task.resources:\n                if (resource.accelerators is not None and\n                        isinstance(resource.accelerators, dict)):\n                    if len(resource.accelerators) > 0:\n                        return math.ceil(\n                            list(resource.accelerators.values())[0])\n        return 0\n\n    def _setup(self, handle: CloudVmRayResourceHandle, task: task_lib.Task,\n               detach_setup: bool) -> None:\n\n        start = time.time()\n\n        if task.setup is None:\n            return\n        setup = task.setup\n        # Sync the setup script up and run it.\n        internal_ips = handle.internal_ips()\n        remote_setup_file_name = f'/tmp/sky_setup_{self.run_timestamp}'\n        # Need this `-i` option to make sure `source ~/.bashrc` work\n        setup_cmd = f'/bin/bash -i {remote_setup_file_name} 2>&1'\n        unset_ray_env_vars = ' && '.join(\n            [f'unset {var}' for var in task_codegen.UNSET_RAY_ENV_VARS])\n        setup_cmd = f'{unset_ray_env_vars}; {setup_cmd}'\n        runners = handle.get_command_runners(avoid_ssh_control=True)\n\n        def _setup_node(node_id: int) -> None:\n            setup_envs = task_lib.get_plaintext_envs_and_secrets(\n                task.envs_and_secrets)\n            setup_envs.update(self._skypilot_predefined_env_vars(handle))\n            setup_envs['SKYPILOT_SETUP_NODE_IPS'] = '\\n'.join(internal_ips)\n            setup_envs['SKYPILOT_SETUP_NODE_RANK'] = str(node_id)\n            setup_envs[constants.SKYPILOT_SETUP_NUM_GPUS_PER_NODE] = (str(\n                self._get_num_gpus(task)))\n\n            runner = runners[node_id]\n            setup_script = log_lib.make_task_bash_script(setup,\n                                                         env_vars=setup_envs)\n            encoded_script = shlex.quote(setup_script)\n\n            def _dump_final_script(\n                    setup_script: str,\n                    target_dir: str = remote_setup_file_name) -> None:\n                with tempfile.NamedTemporaryFile('w', prefix='sky_setup_') as f:\n                    f.write(setup_script)\n                    f.flush()\n                    setup_sh_path = f.name\n                    runner.rsync(source=setup_sh_path,\n                                 target=target_dir,\n                                 up=True,\n                                 stream_logs=False)\n\n            # Always dump the full setup script to the persistent path first\n            # In high availability mode, we need to dump the full setup script\n            # to a persistent path BEFORE any other operations. This ensures\n            # that if the pod restarts, it can find and execute the complete\n            # setup script, rather than a reference to a temporary file that\n            # would no longer exist after restart.\n            if self._dump_final_script:\n                _dump_final_script(setup_script,\n                                   constants.PERSISTENT_SETUP_SCRIPT_PATH)\n\n            if (detach_setup or\n                    backend_utils.is_command_length_over_limit(encoded_script)):\n                _dump_final_script(setup_script)\n                create_script_code = 'true'\n            else:\n                create_script_code = (f'{{ echo {encoded_script} > '\n                                      f'{remote_setup_file_name}; }}')\n\n            if detach_setup:\n                return\n\n            setup_log_path = os.path.join(self.log_dir,\n                                          f'setup-{runner.node_id}.log')\n\n            def _run_setup(setup_cmd: str) -> int:\n                returncode = runner.run(\n                    setup_cmd,\n                    log_path=setup_log_path,\n                    process_stream=False,\n                    # We do not source bashrc for setup, since bashrc is sourced\n                    # in the script already.\n                    # Skip an empty line and two lines due to the /bin/bash -i\n                    # and source ~/.bashrc in the setup_cmd.\n                    #   bash: cannot set terminal process group (7398): Inappropriate ioctl for device # pylint: disable=line-too-long\n                    #   bash: no job control in this shell\n                    skip_num_lines=3)\n                return returncode\n\n            returncode = _run_setup(f'{create_script_code} && {setup_cmd}',)\n\n            if _is_message_too_long(returncode, file_path=setup_log_path):\n                # If the setup script is too long, we need to retry it\n                # with dumping the script to a file and running it the script\n                # on remote cluster instead.\n                logger.debug('Failed to run setup command inline due to '\n                             'command length limit. Dumping setup script to '\n                             'file and running it with SSH.')\n                _dump_final_script(setup_script)\n                returncode = _run_setup(setup_cmd)\n\n            def error_message() -> str:\n                # Use the function to avoid tailing the file in success case\n                try:\n                    last_10_lines = subprocess.run(\n                        ['tail', '-n10',\n                         os.path.expanduser(setup_log_path)],\n                        stdout=subprocess.PIPE,\n                        check=True).stdout.decode('utf-8')\n                except subprocess.CalledProcessError:\n                    last_10_lines = None\n\n                err_msg = (f'Failed to setup with return code {returncode}. '\n                           f'Check the details in log: {setup_log_path}')\n                if last_10_lines:\n                    err_msg += (f'\\n\\n{colorama.Fore.RED}'\n                                '****** START Last lines of setup output ******'\n                                f'{colorama.Style.RESET_ALL}\\n'\n                                f'{last_10_lines}'\n                                f'{colorama.Fore.RED}'\n                                '******* END Last lines of setup output *******'\n                                f'{colorama.Style.RESET_ALL}')\n                return err_msg\n\n            subprocess_utils.handle_returncode(returncode=returncode,\n                                               command=setup_cmd,\n                                               error_msg=error_message)\n\n        num_nodes = len(runners)\n        plural = 's' if num_nodes > 1 else ''\n        node_str = f'{num_nodes} VM{plural}'\n        if isinstance(handle.launched_resources.cloud, clouds.Kubernetes):\n            node_str = f'{num_nodes} pod{plural}'\n        controller = controller_utils.Controllers.from_name(handle.cluster_name)\n        if controller is not None:\n            node_str = controller.value.name\n        if not detach_setup:\n            logger.info(\n                ux_utils.starting_message(f'Running setup on {node_str}.'))\n        # TODO(zhwu): run_in_parallel uses multi-thread to run the commands,\n        # which can cause the program waiting for all the threads to finish,\n        # even if some of them raise exceptions. We should replace it with\n        # multi-process.\n        rich_utils.stop_safe_status()\n        subprocess_utils.run_in_parallel(_setup_node, list(range(num_nodes)))\n\n        if detach_setup:\n            # Only set this when setup needs to be run outside the self._setup()\n            # as part of a job (detach_setup, default).\n            self._setup_cmd = setup_cmd\n            logger.info(ux_utils.finishing_message('Setup detached.'))\n            return\n        end = time.time()\n        logger.debug(f'Setup took {end - start} seconds.')\n        setup_log_path = os.path.join(self.log_dir, 'setup-*.log')\n        logger.info(\n            ux_utils.finishing_message('Setup completed.', setup_log_path))\n\n    def _download_file(self, handle: CloudVmRayResourceHandle,\n                       local_file_path: str, remote_file_path: str) -> None:\n        \"\"\"Syncs file from remote to local.\"\"\"\n        runners = handle.get_command_runners()\n        head_runner = runners[0]\n        head_runner.rsync(\n            source=local_file_path,\n            target=remote_file_path,\n            up=False,\n            stream_logs=False,\n        )\n\n    def _exec_code_on_head(\n        self,\n        handle: CloudVmRayResourceHandle,\n        codegen: str,\n        job_id: int,\n        managed_job_dag: Optional['dag.Dag'] = None,\n        managed_job_user_id: Optional[str] = None,\n        remote_log_dir: Optional[str] = None,\n    ) -> None:\n        \"\"\"Executes generated code on the head node.\"\"\"\n        use_legacy = not handle.is_grpc_enabled_with_flag\n        file_name = f'sky_job_{job_id}'\n        script_path = os.path.join(SKY_REMOTE_APP_DIR, file_name)\n        if remote_log_dir is None:\n            remote_log_dir = self.log_dir\n        remote_log_path = os.path.join(remote_log_dir, 'run.log')\n\n        def _dump_code_to_file(codegen: str,\n                               target_dir: str = SKY_REMOTE_APP_DIR) -> None:\n            runners = handle.get_command_runners()\n            head_runner = runners[0]\n            with tempfile.NamedTemporaryFile('w', prefix='sky_app_') as fp:\n                fp.write(codegen)\n                fp.flush()\n                script_path = os.path.join(target_dir, file_name)\n                # We choose to sync code + exec, because the alternative of\n                # 'ray submit' may not work as it may use system python\n                # (python2) to execute the script. Happens for AWS.\n                head_runner.rsync(source=fp.name,\n                                  target=script_path,\n                                  up=True,\n                                  stream_logs=False)\n\n        mkdir_code = f'mkdir -p {remote_log_dir} && touch {remote_log_path}'\n        encoded_script = shlex.quote(codegen)\n        create_script_code = f'{{ echo {encoded_script} > {script_path}; }}'\n        job_submit_cmd = (\n            # JOB_CMD_IDENTIFIER is used for identifying the process\n            # retrieved with pid is the same driver process.\n            f'{job_lib.JOB_CMD_IDENTIFIER.format(job_id)} && '\n            f'{constants.SKY_PYTHON_CMD} -u {script_path}'\n            # Do not use &>, which is not POSIX and may not work.\n            # Note that the order of \">filename 2>&1\" matters.\n            f'> {remote_log_path} 2>&1')\n        code = job_lib.JobLibCodeGen.queue_job(job_id, job_submit_cmd)\n\n        # For Slurm, we need to wait for the job to complete before exiting,\n        # because Slurm's proctrack/cgroup kills all processes when the srun\n        # job step ends, including child processes launched as a separate\n        # process group.\n        # So this keeps srun alive so the job driver process that was spawned\n        # (and runs in the background) by job_lib.JobScheduler.schedule_step()\n        # does not get killed.\n        # Note: proctrack/cgroup is enabled by default on Nebius' Managed\n        # Soperator.\n        is_slurm = isinstance(handle.launched_resources.cloud, clouds.Slurm)\n        if is_slurm:\n            wait_code = job_lib.JobLibCodeGen.wait_for_job(job_id)\n            code = code + ' && ' + wait_code\n\n        job_submit_cmd = ' && '.join([mkdir_code, create_script_code, code])\n\n        # Should also be ealier than is_command_length_over_limit\n        # Same reason as in _setup\n        if self._dump_final_script:\n            _dump_code_to_file(job_submit_cmd,\n                               constants.PERSISTENT_RUN_SCRIPT_DIR)\n\n        if not use_legacy:\n            try:\n                managed_job_info: Optional[jobsv1_pb2.ManagedJobInfo] = None\n                if managed_job_dag is not None:\n                    workspace = skypilot_config.get_active_workspace(\n                        force_user_workspace=True)\n                    entrypoint = common_utils.get_current_command()\n\n                    managed_job_tasks: List[jobsv1_pb2.ManagedJobTask] = []\n                    for task_id, task in enumerate(managed_job_dag.tasks):\n                        resources_str = backend_utils.get_task_resources_str(\n                            task, is_managed_job=True)\n                        managed_job_tasks.append(\n                            jobsv1_pb2.ManagedJobTask(\n                                task_id=task_id,\n                                name=task.name,\n                                resources_str=resources_str,\n                                metadata_json=task.metadata_json))\n\n                    managed_job_info = jobsv1_pb2.ManagedJobInfo(\n                        name=managed_job_dag.name,\n                        pool=managed_job_dag.pool,\n                        workspace=workspace,\n                        entrypoint=entrypoint,\n                        tasks=managed_job_tasks,\n                        user_id=managed_job_user_id)\n\n                if backend_utils.is_command_length_over_limit(codegen):\n                    _dump_code_to_file(codegen)\n                    queue_job_request = jobsv1_pb2.QueueJobRequest(\n                        job_id=job_id,\n                        # codegen not set - server assumes script uploaded\n                        remote_log_dir=remote_log_dir,\n                        managed_job=managed_job_info,\n                        script_path=script_path)\n                else:\n                    queue_job_request = jobsv1_pb2.QueueJobRequest(\n                        job_id=job_id,\n                        codegen=codegen,\n                        remote_log_dir=remote_log_dir,\n                        managed_job=managed_job_info,\n                        script_path=script_path)\n\n                backend_utils.invoke_skylet_with_retries(lambda: SkyletClient(\n                    handle.get_grpc_channel()).queue_job(queue_job_request))\n            except exceptions.SkyletMethodNotImplementedError:\n                use_legacy = True\n\n        if use_legacy:\n            if backend_utils.is_command_length_over_limit(job_submit_cmd):\n                _dump_code_to_file(codegen)\n                job_submit_cmd = f'{mkdir_code} && {code}'\n\n            def _maybe_add_managed_job_code(job_submit_cmd: str) -> str:\n                if managed_job_dag is not None:\n                    # Add the managed job to job queue database.\n                    managed_job_codegen = managed_jobs.ManagedJobCodeGen()\n                    managed_job_code = managed_job_codegen.set_pending(\n                        job_id,\n                        managed_job_dag,\n                        skypilot_config.get_active_workspace(\n                            force_user_workspace=True),\n                        entrypoint=common_utils.get_current_command(),\n                        user_hash=managed_job_user_id)\n                    # Set the managed job to PENDING state to make sure that\n                    # this managed job appears in the `sky jobs queue`, even\n                    # if it needs to wait to be submitted.\n                    # We cannot set the managed job to PENDING state in the\n                    # job template (jobs-controller.yaml.j2), as it may need\n                    # to wait for the run commands to be scheduled on the job\n                    # controller in high-load cases.\n                    job_submit_cmd += ' && ' + managed_job_code\n                return job_submit_cmd\n\n            job_submit_cmd = _maybe_add_managed_job_code(job_submit_cmd)\n\n            # For Slurm, run in background so that SSH returns immediately.\n            # This is needed because we add the wait_for_job code above which\n            # makes the command block until the job completes.\n            returncode, stdout, stderr = self.run_on_head(\n                handle,\n                job_submit_cmd,\n                stream_logs=False,\n                require_outputs=True,\n                run_in_background=is_slurm)\n            # Happens when someone calls `sky exec` but remote is outdated for\n            # running a job. Necessitating calling `sky launch`.\n            backend_utils.check_stale_runtime_on_remote(returncode, stderr,\n                                                        handle.cluster_name)\n            output = stdout + stderr\n            if _is_message_too_long(returncode, output=output):\n                # If the job submit script is too long, we need to retry it\n                # with dumping the script to a file and running it the script\n                # on remote cluster instead.\n                logger.debug(\n                    'Failed to submit job due to command length limit. '\n                    'Dumping job to file and running it with SSH. '\n                    f'Output: {output}')\n                _dump_code_to_file(codegen)\n                job_submit_cmd = f'{mkdir_code} && {code}'\n                job_submit_cmd = _maybe_add_managed_job_code(job_submit_cmd)\n                # See comment above for why run_in_background=is_slurm.\n                returncode, stdout, stderr = self.run_on_head(\n                    handle,\n                    job_submit_cmd,\n                    stream_logs=False,\n                    require_outputs=True,\n                    run_in_background=is_slurm)\n\n            subprocess_utils.handle_returncode(\n                returncode,\n                job_submit_cmd,\n                f'Failed to submit job {job_id}.',\n                stderr=stdout + stderr)\n\n        controller = controller_utils.Controllers.from_name(handle.cluster_name)\n        if controller == controller_utils.Controllers.SKY_SERVE_CONTROLLER:\n            logger.info(ux_utils.starting_message('Service registered.'))\n        else:\n            logger.info(\n                ux_utils.starting_message(f'Job submitted, ID: {job_id}'))\n        rich_utils.stop_safe_status()\n\n    def _add_job(self, handle: CloudVmRayResourceHandle,\n                 job_name: Optional[str], resources_str: str,\n                 metadata: str) -> Tuple[int, str]:\n        use_legacy = not handle.is_grpc_enabled_with_flag\n\n        if not use_legacy:\n            try:\n                request = jobsv1_pb2.AddJobRequest(\n                    job_name=job_name,\n                    username=common_utils.get_user_hash(),\n                    run_timestamp=self.run_timestamp,\n                    resources_str=resources_str,\n                    metadata=metadata)\n                response = backend_utils.invoke_skylet_with_retries(\n                    lambda: SkyletClient(handle.get_grpc_channel()).add_job(\n                        request))\n                job_id = response.job_id\n                log_dir = response.log_dir\n                return job_id, log_dir\n            except exceptions.SkyletMethodNotImplementedError:\n                use_legacy = True\n\n        if use_legacy:\n            code = job_lib.JobLibCodeGen.add_job(\n                job_name=job_name,\n                username=common_utils.get_user_hash(),\n                run_timestamp=self.run_timestamp,\n                resources_str=resources_str,\n                metadata=metadata)\n            returncode, result_str, stderr = self.run_on_head(\n                handle,\n                code,\n                stream_logs=False,\n                require_outputs=True,\n                separate_stderr=True)\n            # Happens when someone calls `sky exec` but remote is outdated for\n            # adding a job. Necessitating calling `sky launch`.\n            backend_utils.check_stale_runtime_on_remote(returncode, stderr,\n                                                        handle.cluster_name)\n            # TODO(zhwu): this sometimes will unexpectedly fail, we can add\n            # retry for this, after we figure out the reason.\n            subprocess_utils.handle_returncode(returncode, code,\n                                               'Failed to fetch job id.',\n                                               stderr)\n            try:\n                job_id_match = _JOB_ID_PATTERN.search(result_str)\n                if job_id_match is not None:\n                    job_id = int(job_id_match.group(1))\n                else:\n                    # For backward compatibility.\n                    job_id = int(result_str)\n                log_dir_match = _LOG_DIR_PATTERN.search(result_str)\n                if log_dir_match is not None:\n                    log_dir = log_dir_match.group(1).strip()\n                else:\n                    # For backward compatibility, use the same log dir as local.\n                    log_dir = self.log_dir\n            except ValueError as e:\n                logger.error(stderr)\n                raise ValueError(f'Failed to parse job id: {result_str}; '\n                                 f'Returncode: {returncode}') from e\n        return job_id, log_dir\n\n    def _execute(\n        self,\n        handle: CloudVmRayResourceHandle,\n        task: task_lib.Task,\n        dryrun: bool = False,\n    ) -> Optional[int]:\n        \"\"\"Executes the task on the cluster.\n\n        Returns:\n            Job id if the task is submitted to the cluster, None otherwise.\n        \"\"\"\n        if task.run is None and self._setup_cmd is None:\n            # This message is fine without mentioning setup, as there are two\n            # cases when run section is empty:\n            # 1. setup specified: setup is executed in detached mode and this\n            #    message will not be shown.\n            # 2. no setup specified: this message is fine as a user is likely\n            #    creating a cluster only, and ok with the empty run command.\n            logger.info('Run commands not specified or empty.')\n            return None\n        if task.run is None:\n            # If the task has no run command, we still need to execute the\n            # generated ray driver program to run the setup command in detached\n            # mode.\n            # In this case, we reset the resources for the task, so that the\n            # detached setup does not need to wait for the task resources to be\n            # ready (which is not used for setup anyway).\n            valid_resource = resources_lib.Resources()\n        else:\n            # Check the task resources vs the cluster resources. Since\n            # `sky exec` will not run the provision and _check_existing_cluster\n            # We need to check ports here since sky.exec shouldn't change\n            # resources.\n            valid_resource = self.check_resources_fit_cluster(handle,\n                                                              task,\n                                                              check_ports=True)\n        task_copy = copy.copy(task)\n        # Handle multiple resources exec case.\n        task_copy.set_resources(valid_resource)\n        if len(task.resources) > 1:\n            logger.info('Multiple resources are specified '\n                        f'for the task, using: {valid_resource}')\n        task_copy.best_resources = None\n        resources_str = backend_utils.get_task_resources_str(task_copy)\n\n        if dryrun:\n            logger.info(f'Dryrun complete. Would have run:\\n{task}')\n            return None\n\n        job_id, log_dir = self._add_job(handle, task_copy.name, resources_str,\n                                        task.metadata_json)\n\n        num_actual_nodes = task.num_nodes * handle.num_ips_per_node\n        # Case: task_lib.Task(run, num_nodes=N) or TPU VM Pods\n        if num_actual_nodes > 1:\n            self._execute_task_n_nodes(handle, task_copy, job_id, log_dir)\n        else:\n            # Case: task_lib.Task(run, num_nodes=1)\n            self._execute_task_one_node(handle, task_copy, job_id, log_dir)\n\n        return job_id\n\n    def _post_execute(self, handle: CloudVmRayResourceHandle,\n                      down: bool) -> None:\n        \"\"\"Post-execute cleanup.\"\"\"\n        del handle, down  # Unused.\n        # All logic is handled in previous stages, no-op.\n\n    def _teardown_ephemeral_storage(self, task: task_lib.Task) -> None:\n        storage_mounts = task.storage_mounts\n        if storage_mounts is not None:\n            for _, storage in storage_mounts.items():\n                if not storage.persistent:\n                    storage.delete()\n\n    def _teardown(self,\n                  handle: CloudVmRayResourceHandle,\n                  terminate: bool,\n                  purge: bool = False):\n        \"\"\"Tear down or stop the cluster.\n\n        Args:\n            handle: The handle to the cluster.\n            terminate: Terminate or stop the cluster.\n            purge: Purge the cluster record from the cluster table, even if\n                the teardown fails.\n        Raises:\n            exceptions.ClusterOwnerIdentityMismatchError: If the cluster is\n                owned by another user.\n            exceptions.CloudUserIdentityError: if we fail to get the current\n                user identity.\n            RuntimeError: If the cluster fails to be terminated/stopped.\n        \"\"\"\n        cluster_name = handle.cluster_name\n        # Check if the cluster is owned by the current user. Raise\n        # exceptions.ClusterOwnerIdentityMismatchError\n        yellow = colorama.Fore.YELLOW\n        reset = colorama.Style.RESET_ALL\n        is_identity_mismatch_and_purge = False\n        try:\n            backend_utils.check_owner_identity(cluster_name)\n        except (exceptions.ClusterOwnerIdentityMismatchError,\n                exceptions.CloudUserIdentityError) as e:\n            if purge:\n                logger.error(e)\n                verbed = 'terminated' if terminate else 'stopped'\n                logger.warning(\n                    f'{yellow}Purge (-p/--purge) is set, ignoring the '\n                    f'identity mismatch error and removing '\n                    f'the cluster record from cluster table.{reset}\\n{yellow}It'\n                    ' is the user\\'s responsibility to ensure that this '\n                    f'cluster is actually {verbed} on the cloud.{reset}')\n                is_identity_mismatch_and_purge = True\n            else:\n                raise\n        lock_id = backend_utils.cluster_status_lock_id(cluster_name)\n        lock = locks.get_lock(lock_id, timeout=1)\n        # Retry in case new cluster operation comes in and holds the lock\n        # right after the lock is removed.\n        n_attempts = 2\n        while True:\n            n_attempts -= 1\n            # We have to kill the cluster requests, because `down` and `stop`\n            # should be higher priority than the cluster requests, and we should\n            # release the lock from other requests.\n            exclude_request_to_kill = 'sky.down' if terminate else 'sky.stop'\n            try:\n                # TODO(zhwu): we should get rid of this when it is being called\n                # internally without involving an API server, e.g., when a\n                # controller is trying to terminate a cluster.\n                requests_lib.kill_cluster_requests(handle.cluster_name,\n                                                   exclude_request_to_kill)\n            except Exception as e:  # pylint: disable=broad-except\n                # We allow the failure to kill other launch requests, because\n                # it is not critical to the cluster teardown.\n                logger.warning(\n                    'Failed to kill other launch requests for the '\n                    f'cluster {handle.cluster_name}: '\n                    f'{common_utils.format_exception(e, use_bracket=True)}')\n            # In case other running cluster operations are still holding the\n            # lock.\n            lock.force_unlock()\n            try:\n                with lock:\n                    self.teardown_no_lock(\n                        handle,\n                        terminate,\n                        purge,\n                        # When --purge is set and we already see an ID mismatch\n                        # error, we skip the refresh codepath. This is because\n                        # refresh checks current user identity can throw\n                        # ClusterOwnerIdentityMismatchError. The argument/flag\n                        # `purge` should bypass such ID mismatch errors.\n                        refresh_cluster_status=(\n                            not is_identity_mismatch_and_purge))\n                if terminate:\n                    lock.force_unlock()\n                break\n            except locks.LockTimeout as e:\n                logger.debug(f'Failed to acquire lock for {cluster_name}, '\n                             f'retrying...')\n                if n_attempts <= 0:\n                    raise RuntimeError(\n                        f'Cluster {cluster_name!r} is locked by {lock_id}. '\n                        'Check to see if it is still being launched') from e\n\n    # --- CloudVMRayBackend Specific APIs ---\n\n    def get_job_status(\n        self,\n        handle: CloudVmRayResourceHandle,\n        job_ids: Optional[List[int]] = None,\n        stream_logs: bool = True\n    ) -> Dict[Optional[int], Optional[job_lib.JobStatus]]:\n        if handle.is_grpc_enabled_with_flag:\n            try:\n                request = jobsv1_pb2.GetJobStatusRequest(job_ids=job_ids)\n                response = backend_utils.invoke_skylet_with_retries(\n                    lambda: SkyletClient(handle.get_grpc_channel()\n                                        ).get_job_status(request))\n                statuses: Dict[Optional[int], Optional[job_lib.JobStatus]] = {\n                    job_id: job_lib.JobStatus.from_protobuf(proto_status)\n                    for job_id, proto_status in response.job_statuses.items()\n                }\n                return statuses\n            except exceptions.SkyletMethodNotImplementedError:\n                pass\n\n        code = job_lib.JobLibCodeGen.get_job_status(job_ids)\n        returncode, stdout, stderr = self.run_on_head(handle,\n                                                      code,\n                                                      stream_logs=stream_logs,\n                                                      require_outputs=True,\n                                                      separate_stderr=True)\n        subprocess_utils.handle_returncode(returncode, code,\n                                           'Failed to get job status.', stderr)\n        statuses = job_lib.load_statuses_payload(stdout)\n        return statuses\n\n    def cancel_jobs(self,\n                    handle: CloudVmRayResourceHandle,\n                    jobs: Optional[List[int]],\n                    cancel_all: bool = False,\n                    user_hash: Optional[str] = None) -> None:\n        \"\"\"Cancels jobs.\n\n        See `skylet.job_lib.cancel_jobs_encoded_results` for more details.\n        \"\"\"\n        use_legacy = not handle.is_grpc_enabled_with_flag\n\n        if not use_legacy:\n            try:\n                request = jobsv1_pb2.CancelJobsRequest(job_ids=jobs,\n                                                       cancel_all=cancel_all,\n                                                       user_hash=user_hash)\n                response = backend_utils.invoke_skylet_with_retries(\n                    lambda: SkyletClient(handle.get_grpc_channel()).cancel_jobs(\n                        request))\n                cancelled_ids = response.cancelled_job_ids\n            except exceptions.SkyletMethodNotImplementedError:\n                use_legacy = True\n\n        if use_legacy:\n            code = job_lib.JobLibCodeGen.cancel_jobs(jobs, cancel_all,\n                                                     user_hash)\n            returncode, stdout, _ = self.run_on_head(handle,\n                                                     code,\n                                                     stream_logs=False,\n                                                     require_outputs=True)\n            subprocess_utils.handle_returncode(\n                returncode, code,\n                f'Failed to cancel jobs on cluster {handle.cluster_name}.',\n                stdout)\n            cancelled_ids = message_utils.decode_payload(stdout)\n        if cancelled_ids:\n            logger.info(\n                f'Cancelled job ID(s): {\", \".join(map(str, cancelled_ids))}')\n        else:\n            logger.info('No jobs cancelled. They may be in terminal states.')\n\n    def sync_down_logs(\n            self,\n            handle: CloudVmRayResourceHandle,\n            job_ids: Optional[List[str]],\n            local_dir: str = constants.SKY_LOGS_DIRECTORY) -> Dict[str, str]:\n        \"\"\"Sync down logs for the given job_ids.\n\n        Returns:\n            A dictionary mapping job_id to log path.\n        \"\"\"\n        job_to_dir: Dict[str, str] = {}\n        use_legacy = not handle.is_grpc_enabled_with_flag\n\n        if not use_legacy:\n            try:\n                int_job_ids = []\n                if job_ids:\n                    for str_job_id in job_ids:\n                        if str_job_id.isdigit():\n                            int_job_ids.append(int(str_job_id))\n                request = jobsv1_pb2.GetLogDirsForJobsRequest(\n                    job_ids=int_job_ids)\n                response = backend_utils.invoke_skylet_with_retries(\n                    lambda: SkyletClient(handle.get_grpc_channel()\n                                        ).get_log_dirs_for_jobs(request))\n                job_log_dirs = response.job_log_dirs\n                if not job_log_dirs:\n                    logger.info(f'{colorama.Fore.YELLOW}'\n                                'No matching log directories found'\n                                f'{colorama.Style.RESET_ALL}')\n                    return {}\n                for job_id, log_dir in job_log_dirs.items():\n                    # Convert to string for backwards compatibility\n                    job_to_dir[str(job_id)] = log_dir\n            except exceptions.SkyletMethodNotImplementedError:\n                use_legacy = True\n\n        if use_legacy:\n            code = job_lib.JobLibCodeGen.get_log_dirs_for_jobs(job_ids)\n            returncode, stdout, stderr = self.run_on_head(handle,\n                                                          code,\n                                                          stream_logs=False,\n                                                          require_outputs=True,\n                                                          separate_stderr=True)\n            subprocess_utils.handle_returncode(returncode, code,\n                                               'Failed to sync logs.', stderr)\n            job_to_dir = message_utils.decode_payload(stdout)\n            if not job_to_dir:\n                logger.info(f'{colorama.Fore.YELLOW}'\n                            'No matching log directories found'\n                            f'{colorama.Style.RESET_ALL}')\n                return {}\n\n        job_ids = list(job_to_dir.keys())\n        dirs = list(job_to_dir.values())\n        remote_log_dirs = [\n            # TODO(aylei): backward compatibility for legacy runtime that\n            # returns run_timestamp only, remove after 0.12.0\n            (dir if constants.SKY_LOGS_DIRECTORY in dir else os.path.join(\n                constants.SKY_LOGS_DIRECTORY, dir)) for dir in dirs\n        ]\n        # Include cluster name in local log directory path to avoid conflicts\n        # when the same job_id exists on different clusters\n        cluster_name = handle.cluster_name\n        local_log_dirs = []\n        for remote_log_dir in dirs:\n            if constants.SKY_LOGS_DIRECTORY in remote_log_dir:\n                # Extract the job-specific directory name from the full path\n                # e.g., ~/sky_logs/1-job_name -> 1-job_name\n                job_dir = remote_log_dir.replace(constants.SKY_LOGS_DIRECTORY,\n                                                 '').lstrip('/')\n                local_log_dir = os.path.join(local_dir, cluster_name, job_dir)\n            else:\n                # remote_log_dir is already just the job directory name (e.g.,\n                # \"1-job_name\")\n                local_log_dir = os.path.join(local_dir, cluster_name,\n                                             remote_log_dir)\n            local_log_dirs.append(local_log_dir)\n\n        runners = handle.get_command_runners()\n\n        def _rsync_down(args) -> None:\n            \"\"\"Rsync down logs from remote nodes.\n\n            Args:\n                args: A tuple of (runner, local_log_dir, remote_log_dir)\n            \"\"\"\n            (runner, local_log_dir, remote_log_dir) = args\n            try:\n                os.makedirs(os.path.expanduser(local_log_dir), exist_ok=True)\n                runner.rsync(\n                    # Require a `/` at the end to make sure the parent dir\n                    # are not created locally. We do not add additional '*' as\n                    # kubernetes's rsync does not work with an ending '*'.\n                    source=f'{remote_log_dir}/',\n                    target=os.path.expanduser(local_log_dir),\n                    up=False,\n                    stream_logs=False,\n                )\n            except exceptions.CommandError as e:\n                if e.returncode == exceptions.RSYNC_FILE_NOT_FOUND_CODE:\n                    # Raised by rsync_down. Remote log dir may not exist, since\n                    # the job can be run on some part of the nodes.\n                    logger.debug(f'{runner.node_id} does not have the tasks/*.')\n                else:\n                    raise\n\n        parallel_args = [[runner, *item]\n                         for item in zip(local_log_dirs, remote_log_dirs)\n                         for runner in runners]\n        subprocess_utils.run_in_parallel(_rsync_down, parallel_args)\n        return dict(zip(job_ids, local_log_dirs))\n\n    @context_utils.cancellation_guard\n    def tail_logs(\n            self,\n            handle: CloudVmRayResourceHandle,\n            job_id: Optional[int],\n            managed_job_id: Optional[int] = None,\n            follow: bool = True,\n            tail: int = 0,\n            require_outputs: bool = False,\n            stream_logs: bool = True,\n            process_stream: bool = False) -> Union[int, Tuple[int, str, str]]:\n        \"\"\"Tail the logs of a job.\n\n        Args:\n            handle: The handle to the cluster.\n            job_id: The job ID to tail the logs of.\n            managed_job_id: The managed job ID for display purpose only.\n            follow: Whether to follow the logs.\n            tail: The number of lines to display from the end of the\n                log file. If 0, print all lines.\n            require_outputs: Whether to return the stdout/stderr of the command.\n            stream_logs: Whether to stream the logs to stdout/stderr.\n            process_stream: Whether to process the stream.\n\n        Returns:\n            The exit code of the tail command. Returns code 100 if the job has\n            failed. See exceptions.JobExitCode for possible return codes.\n        \"\"\"\n        if handle.is_grpc_enabled_with_flag:\n            last_exit_code = 0\n            try:\n                request = jobsv1_pb2.TailLogsRequest(\n                    job_id=job_id,\n                    managed_job_id=managed_job_id,\n                    follow=follow,\n                    tail=tail)\n                for resp in backend_utils.invoke_skylet_streaming_with_retries(\n                        lambda: SkyletClient(handle.get_grpc_channel()\n                                            ).tail_logs(request, timeout=None)):\n                    if resp.log_line:\n                        print(resp.log_line, end='', flush=True)\n                    last_exit_code = resp.exit_code\n                return last_exit_code\n            except exceptions.SkyletMethodNotImplementedError:\n                pass\n            except grpc.RpcError as e:\n                if e.code() == grpc.StatusCode.CANCELLED:\n                    return last_exit_code\n                raise e\n\n        code = job_lib.JobLibCodeGen.tail_logs(job_id,\n                                               managed_job_id=managed_job_id,\n                                               follow=follow,\n                                               tail=tail)\n        if job_id is None and managed_job_id is None:\n            logger.info(\n                'Job ID not provided. Streaming the logs of the latest job.')\n\n        # With the stdin=subprocess.DEVNULL, the ctrl-c will not directly\n        # kill the process, so we need to handle it manually here.\n        if threading.current_thread() is threading.main_thread():\n            signal.signal(signal.SIGINT, backend_utils.interrupt_handler)\n            signal.signal(signal.SIGTSTP, backend_utils.stop_handler)\n        try:\n            final = self.run_on_head(\n                handle,\n                code,\n                stream_logs=stream_logs,\n                process_stream=process_stream,\n                require_outputs=require_outputs,\n                # Allocate a pseudo-terminal to disable output buffering.\n                # Otherwise, there may be 5 minutes delay in logging.\n                ssh_mode=command_runner.SshMode.INTERACTIVE,\n            )\n        except SystemExit as e:\n            final = e.code\n        return final\n\n    def tail_managed_job_logs(self,\n                              handle: CloudVmRayResourceHandle,\n                              job_id: Optional[int] = None,\n                              job_name: Optional[str] = None,\n                              controller: bool = False,\n                              follow: bool = True,\n                              tail: Optional[int] = None) -> int:\n        # if job_name is not None, job_id should be None\n        assert job_name is None or job_id is None, (job_name, job_id)\n        # TODO(kevin): Migrate stream_logs to gRPC\n        code = managed_jobs.ManagedJobCodeGen.stream_logs(\n            job_name, job_id, follow, controller, tail)\n\n        # With the stdin=subprocess.DEVNULL, the ctrl-c will not directly\n        # kill the process, so we need to handle it manually here.\n        if threading.current_thread() is threading.main_thread():\n            signal.signal(signal.SIGINT, backend_utils.interrupt_handler)\n            signal.signal(signal.SIGTSTP, backend_utils.stop_handler)\n\n        # Refer to the notes in tail_logs.\n        try:\n            returncode = self.run_on_head(\n                handle,\n                code,\n                stream_logs=True,\n                process_stream=False,\n                ssh_mode=command_runner.SshMode.INTERACTIVE,\n            )\n        except SystemExit as e:\n            returncode = e.code\n        return returncode\n\n    def sync_down_managed_job_logs(\n            self,\n            handle: CloudVmRayResourceHandle,\n            job_id: Optional[int] = None,\n            job_name: Optional[str] = None,\n            controller: bool = False,\n            local_dir: str = constants.SKY_LOGS_DIRECTORY) -> Dict[str, str]:\n        \"\"\"Sync down logs for a managed job.\n\n        Args:\n            handle: The handle to the cluster.\n            job_id: The job ID to sync down logs for.\n            job_name: The job name to sync down logs for.\n            controller: Whether to sync down logs for the controller.\n            local_dir: The local directory to sync down logs to.\n\n        Returns:\n            A dictionary mapping job_id to log path.\n        \"\"\"\n        # if job_name and job_id should not both be specified\n        assert job_name is None or job_id is None, (job_name, job_id)\n\n        if job_id is None:\n            # get the job_id\n            # if job_name is None, get all job_ids\n            # TODO: Only get the latest job_id, since that's the only one we use\n\n            use_legacy = not handle.is_grpc_enabled_with_flag\n            logger.info(f'handle.is_grpc_enabled_with_flag: '\n                        f'{handle.is_grpc_enabled_with_flag}')\n            if not use_legacy:\n                try:\n                    request = managed_jobsv1_pb2.GetAllJobIdsByNameRequest(\n                        job_name=job_name)\n                    response = backend_utils.invoke_skylet_with_retries(\n                        lambda: SkyletClient(handle.get_grpc_channel(\n                        )).get_all_managed_job_ids_by_name(request))\n                    job_ids = list(response.job_ids)\n                except exceptions.SkyletMethodNotImplementedError:\n                    use_legacy = True\n\n            if use_legacy:\n                code = managed_jobs.ManagedJobCodeGen.get_all_job_ids_by_name(\n                    job_name=job_name)\n                returncode, job_ids_payload, stderr = self.run_on_head(\n                    handle,\n                    code,\n                    stream_logs=False,\n                    require_outputs=True,\n                    separate_stderr=True)\n                subprocess_utils.handle_returncode(returncode, code,\n                                                   'Failed to sync down logs.',\n                                                   stderr)\n                job_ids = message_utils.decode_payload(job_ids_payload)\n            if not job_ids:\n                logger.info(f'{colorama.Fore.YELLOW}'\n                            'No matching job found'\n                            f'{colorama.Style.RESET_ALL}')\n                return {}\n            elif len(job_ids) > 1:\n                name_str = ''\n                if job_name is not None:\n                    name_str = ('Multiple jobs IDs found under the name '\n                                f'{job_name}. ')\n                controller_str = ' (controller)' if controller else ''\n                logger.info(f'{colorama.Fore.YELLOW}'\n                            f'{name_str}'\n                            f'Downloading the latest job logs{controller_str}.'\n                            f'{colorama.Style.RESET_ALL}')\n            # list should aready be in descending order\n            job_id = job_ids[0]\n\n        if isinstance(handle, LocalResourcesHandle):\n            # In consolidation mode, we don't submit a ray job, therefore no\n            # run_timestamp is available. We use a dummy run_timestamp here.\n            run_timestamps = {\n                job_id: f'managed-jobs-consolidation-mode-{job_id}'\n            }\n        else:\n            # get the run_timestamp\n            # the function takes in [job_id]\n            use_legacy = not handle.is_grpc_enabled_with_flag\n            if not use_legacy:\n                try:\n                    log_dirs_request = jobsv1_pb2.GetLogDirsForJobsRequest(\n                        job_ids=[job_id])\n                    log_dirs_response = (\n                        backend_utils.invoke_skylet_with_retries(\n                            lambda: SkyletClient(handle.get_grpc_channel(\n                            )).get_log_dirs_for_jobs(log_dirs_request)))\n                    job_log_dirs = log_dirs_response.job_log_dirs\n                    # Convert back to the expected format\n                    # {job_id: run_timestamp}\n                    run_timestamps = {}\n                    for jid, log_dir in job_log_dirs.items():\n                        run_timestamps[int(jid)] = log_dir\n                except exceptions.SkyletMethodNotImplementedError:\n                    use_legacy = True\n\n            if use_legacy:\n                code = job_lib.JobLibCodeGen.get_log_dirs_for_jobs(\n                    [str(job_id)])\n                returncode, run_timestamps_payload, stderr = self.run_on_head(\n                    handle,\n                    code,\n                    stream_logs=False,\n                    require_outputs=True,\n                    separate_stderr=True)\n                subprocess_utils.handle_returncode(returncode, code,\n                                                   'Failed to sync logs.',\n                                                   stderr)\n                # returns with a dict of {job_id: run_timestamp}\n                run_timestamps = message_utils.decode_payload(\n                    run_timestamps_payload)\n        if not run_timestamps:\n            logger.info(f'{colorama.Fore.YELLOW}'\n                        'No matching log directories found'\n                        f'{colorama.Style.RESET_ALL}')\n            return {}\n\n        run_timestamp = list(run_timestamps.values())[0]\n        job_id = list(run_timestamps.keys())[0]\n\n        # If run_timestamp contains the full path with SKY_LOGS_DIRECTORY,\n        # strip the prefix to get just the relative part to avoid duplication\n        # when constructing local paths.\n        if run_timestamp.startswith(constants.SKY_LOGS_DIRECTORY):\n            run_timestamp = run_timestamp[len(constants.SKY_LOGS_DIRECTORY\n                                             ):].lstrip('/')\n        local_log_dir = ''\n        if controller:  # download controller logs\n            remote_log = os.path.join(managed_jobs.JOBS_CONTROLLER_LOGS_DIR,\n                                      f'{job_id}.log')\n            local_log_dir = os.path.join(local_dir, 'managed_jobs',\n                                         run_timestamp)\n            os.makedirs(os.path.dirname(os.path.expanduser(local_log_dir)),\n                        exist_ok=True)\n\n            logger.debug(f'{colorama.Fore.CYAN}'\n                         f'Job {job_id} local logs: {local_log_dir}'\n                         f'{colorama.Style.RESET_ALL}')\n\n            runners = handle.get_command_runners()\n\n            def _rsync_down(args) -> None:\n                \"\"\"Rsync down logs from remote nodes.\n\n                Args:\n                    args: A tuple of (runner, local_log_dir, remote_log_dir)\n                \"\"\"\n                (runner, local_log_dir, remote_log) = args\n                try:\n                    os.makedirs(os.path.expanduser(local_log_dir),\n                                exist_ok=True)\n                    runner.rsync(\n                        source=remote_log,\n                        target=f'{local_log_dir}/controller.log',\n                        up=False,\n                        stream_logs=False,\n                    )\n                except exceptions.CommandError as e:\n                    if e.returncode == exceptions.RSYNC_FILE_NOT_FOUND_CODE:\n                        # Raised by rsync_down. Remote log dir may not exist\n                        # since the job can be run on some part of the nodes.\n                        logger.debug(\n                            f'{runner.node_id} does not have the tasks/*.')\n                    else:\n                        raise\n\n            parallel_args = [\n                (runner, local_log_dir, remote_log) for runner in runners\n            ]\n            subprocess_utils.run_in_parallel(_rsync_down, parallel_args)\n        else:  # download job logs\n            local_log_dir = os.path.join(local_dir, 'managed_jobs',\n                                         run_timestamp)\n            os.makedirs(os.path.dirname(os.path.expanduser(local_log_dir)),\n                        exist_ok=True)\n            log_file = os.path.join(local_log_dir, 'run.log')\n\n            # TODO(kevin): Migrate stream_logs to gRPC\n            code = managed_jobs.ManagedJobCodeGen.stream_logs(\n                job_name=None,\n                job_id=int(job_id),\n                follow=False,\n                controller=False)\n            # With the stdin=subprocess.DEVNULL, the ctrl-c will not\n            # kill the process, so we need to handle it manually here.\n            if threading.current_thread() is threading.main_thread():\n                signal.signal(signal.SIGINT, backend_utils.interrupt_handler)\n                signal.signal(signal.SIGTSTP, backend_utils.stop_handler)\n\n            # We redirect the output to the log file\n            # and disable the STDOUT and STDERR\n            self.run_on_head(\n                handle,\n                code,\n                log_path=os.path.expanduser(log_file),\n                stream_logs=False,\n                process_stream=False,\n                ssh_mode=command_runner.SshMode.INTERACTIVE,\n            )\n\n        logger.debug(f'{colorama.Fore.CYAN}'\n                     f'Job {job_id} logs: {local_log_dir}'\n                     f'{colorama.Style.RESET_ALL}')\n        return {str(job_id): local_log_dir}\n\n    def teardown_no_lock(self,\n                         handle: CloudVmRayResourceHandle,\n                         terminate: bool,\n                         purge: bool = False,\n                         post_teardown_cleanup: bool = True,\n                         refresh_cluster_status: bool = True,\n                         remove_from_db: bool = True) -> None:\n        \"\"\"Teardown the cluster without acquiring the cluster status lock.\n\n        NOTE: This method should not be called without holding the cluster\n        status lock already.\n\n        refresh_cluster_status is only used internally in the status refresh\n        process, and should not be set to False in other cases.\n\n        Raises:\n            RuntimeError: If the cluster fails to be terminated/stopped.\n        \"\"\"\n        try:\n            handle.close_skylet_ssh_tunnel()\n        except Exception as e:  # pylint: disable=broad-except\n            # Not critical to the cluster teardown, just log a warning.\n            logger.warning(\n                'Failed to close Skylet SSH tunnel for cluster '\n                f'{handle.cluster_name}: '\n                f'{common_utils.format_exception(e, use_bracket=True)}')\n\n        exclude_request_to_kill = 'sky.down' if terminate else 'sky.stop'\n        # We have to kill the cluster requests again within the lock, because\n        # any pending requests on the same cluster should be cancelled after\n        # the cluster is terminated/stopped. Otherwise, it will be quite\n        # confusing to see the cluster restarted immediately after it is\n        # terminated/stopped, when there is a pending launch request.\n        try:\n            # TODO(zhwu): we should get rid of this when it is being called\n            # internally without involving an API server, e.g., when a\n            # controller is trying to terminate a cluster.\n            requests_lib.kill_cluster_requests(handle.cluster_name,\n                                               exclude_request_to_kill)\n        except Exception as e:  # pylint: disable=broad-except\n            # We allow the failure to kill other launch requests, because\n            # it is not critical to the cluster teardown.\n            logger.warning(\n                'Failed to kill other launch requests for the '\n                f'cluster {handle.cluster_name}: '\n                f'{common_utils.format_exception(e, use_bracket=True)}')\n        cluster_status_fetched = False\n        if refresh_cluster_status:\n            try:\n                prev_cluster_status, _ = (\n                    backend_utils.refresh_cluster_status_handle(\n                        handle.cluster_name,\n                        # There is a case where\n                        # 1. The cluster was interrupted during provisioning.\n                        # 2. The API request to create the cluster instances was\n                        #    sent to the cloud, but hasn't been processed yet.\n                        # In this case, the cluster will be INIT. We should do a\n                        # hard status refresh to see if the instances are\n                        # actually there or not. Otherwise, teardown may not\n                        # find the instances, leading to a leak. This was\n                        # observed in AWS. See also\n                        # _LAUNCH_DOUBLE_CHECK_WINDOW in backend_utils.py.\n                        force_refresh_statuses={status_lib.ClusterStatus.INIT},\n                        cluster_lock_already_held=True,\n                        retry_if_missing=False))\n                cluster_status_fetched = True\n            except exceptions.ClusterStatusFetchingError:\n                logger.warning(\n                    'Failed to fetch cluster status for '\n                    f'{handle.cluster_name!r}. Assuming the cluster is still '\n                    'up.')\n        if not cluster_status_fetched:\n            status = global_user_state.get_status_from_cluster_name(\n                handle.cluster_name)\n            prev_cluster_status = status if status is not None else None\n        if prev_cluster_status is None:\n            # When the cluster is not in the cluster table, we guarantee that\n            # all related resources / cache / config are cleaned up, i.e. it\n            # is safe to skip and return True.\n            ux_utils.console_newline()\n            logger.warning(\n                f'Cluster {handle.cluster_name!r} is already terminated. '\n                'Skipped.')\n            return\n\n        if handle.cluster_yaml is None:\n            logger.warning(f'Cluster {handle.cluster_name!r} has no '\n                           f'provision yaml so it '\n                           'has not been provisioned. Skipped.')\n            global_user_state.remove_cluster(handle.cluster_name,\n                                             terminate=terminate)\n            return\n        log_path = os.path.join(os.path.expanduser(self.log_dir),\n                                'teardown.log')\n        log_abs_path = os.path.abspath(log_path)\n        launched_resources = handle.launched_resources.assert_launchable()\n        cloud = launched_resources.cloud\n        config = global_user_state.get_cluster_yaml_dict(handle.cluster_yaml)\n        cluster_name = handle.cluster_name\n        cluster_name_on_cloud = handle.cluster_name_on_cloud\n\n        # Avoid possibly unbound warnings. Code below must overwrite these vars:\n        returncode = 0\n        stdout = ''\n        stderr = ''\n\n        if (cloud.PROVISIONER_VERSION >=\n                clouds.ProvisionerVersion.RAY_PROVISIONER_SKYPILOT_TERMINATOR):\n            logger.debug(f'Provisioner version: {cloud.PROVISIONER_VERSION} '\n                         'using new provisioner for teardown.')\n            # Stop the ray autoscaler first to avoid the head node trying to\n            # re-launch the worker nodes, during the termination of the\n            # cluster.\n            try:\n                # We do not check the return code, since Ray returns\n                # non-zero return code when calling Ray stop,\n                # even when the command was executed successfully.\n                self.run_on_head(handle,\n                                 f'{constants.SKY_RAY_CMD} stop --force')\n            except exceptions.FetchClusterInfoError:\n                # This error is expected if the previous cluster IP is\n                # failed to be found,\n                # i.e., the cluster is already stopped/terminated.\n                if prev_cluster_status == status_lib.ClusterStatus.UP:\n                    logger.warning(\n                        'Failed to take down Ray autoscaler on the head node. '\n                        'It might be because the cluster\\'s head node has '\n                        'already been terminated. It is fine to skip this.')\n\n            try:\n                provisioner.teardown_cluster(repr(cloud),\n                                             resources_utils.ClusterName(\n                                                 cluster_name,\n                                                 cluster_name_on_cloud),\n                                             terminate=terminate,\n                                             provider_config=config['provider'])\n            except Exception as e:  # pylint: disable=broad-except\n                if purge:\n                    logger.warning(\n                        _TEARDOWN_PURGE_WARNING.format(\n                            reason='stopping/terminating cluster nodes',\n                            details=common_utils.format_exception(\n                                e, use_bracket=True)))\n                else:\n                    raise\n\n            if post_teardown_cleanup:\n                self.post_teardown_cleanup(handle, terminate, purge,\n                                           remove_from_db)\n            return\n\n        if (isinstance(cloud, clouds.IBM) and terminate and\n                prev_cluster_status == status_lib.ClusterStatus.STOPPED):\n            # pylint: disable= W0622 W0703 C0415\n            from sky.adaptors import ibm\n            from sky.skylet.providers.ibm.vpc_provider import IBMVPCProvider\n\n            config_provider = global_user_state.get_cluster_yaml_dict(\n                handle.cluster_yaml)['provider']\n            region = config_provider['region']\n            search_client = ibm.search_client()\n            vpc_found = False\n            # pylint: disable=unsubscriptable-object\n            vpcs_filtered_by_tags_and_region = search_client.search(\n                query=(f'type:vpc AND tags:{cluster_name_on_cloud} '\n                       f'AND region:{region}'),\n                fields=['tags', 'region', 'type'],\n                limit=1000).get_result()['items']\n            vpc_id = None\n            try:\n                vpc_id = vpcs_filtered_by_tags_and_region[0]['crn'].rsplit(\n                    ':', 1)[-1]\n                vpc_found = True\n            except Exception:\n                logger.critical('failed to locate vpc for ibm cloud')\n                returncode = -1\n\n            if vpc_found:\n                # Delete VPC and it's associated resources\n                vpc_provider = IBMVPCProvider(\n                    config_provider['resource_group_id'], region,\n                    cluster_name_on_cloud)\n                vpc_provider.delete_vpc(vpc_id, region)\n                # successfully removed cluster as no exception was raised\n                returncode = 0\n\n        else:\n            config['provider']['cache_stopped_nodes'] = not terminate\n            with tempfile.NamedTemporaryFile('w',\n                                             prefix='sky_',\n                                             delete=False,\n                                             suffix='.yml') as f:\n                yaml_utils.dump_yaml(f.name, config)\n                f.flush()\n\n                teardown_verb = 'Terminating' if terminate else 'Stopping'\n                with rich_utils.safe_status(\n                        ux_utils.spinner_message(\n                            f'{teardown_verb}: {cluster_name}', log_path)):\n                    # FIXME(zongheng): support retries. This call can fail for\n                    # example due to GCP returning list requests per limit\n                    # exceeded.\n                    returncode, stdout, stderr = log_lib.run_with_log(\n                        ['ray', 'down', '-y', f.name],\n                        log_abs_path,\n                        stream_logs=False,\n                        require_outputs=True,\n                        # Disable stdin to avoid ray outputs mess up the\n                        # terminal with misaligned output when multithreading/\n                        # multiprocessing are used.\n                        # Refer to: https://github.com/ray-project/ray/blob/d462172be7c5779abf37609aed08af112a533e1e/python/ray/autoscaler/_private/subprocess_output_util.py#L264 # pylint: disable=line-too-long\n                        stdin=subprocess.DEVNULL)\n        if returncode != 0:\n            if purge:\n                logger.warning(\n                    _TEARDOWN_PURGE_WARNING.format(\n                        reason='stopping/terminating cluster nodes',\n                        details=stderr))\n            # 'TPU must be specified.': This error returns when we call \"gcloud\n            #   delete\" with an empty VM list where no instance exists. Safe to\n            #   ignore it and do cleanup locally. TODO(wei-lin): refactor error\n            #   handling mechanism.\n            #\n            # 'SKYPILOT_ERROR_NO_NODES_LAUNCHED': this indicates nodes are\n            #   never launched and the errors are related to pre-launch\n            #   configurations (such as VPC not found). So it's safe & good UX\n            #   to not print a failure message.\n            elif ('TPU must be specified.' not in stderr and\n                  'SKYPILOT_ERROR_NO_NODES_LAUNCHED: ' not in stderr):\n                raise RuntimeError(\n                    _TEARDOWN_FAILURE_MESSAGE.format(\n                        extra_reason='',\n                        cluster_name=common_utils.cluster_name_in_hint(\n                            cluster_name, cluster_name_on_cloud),\n                        stdout=stdout,\n                        stderr=stderr))\n\n        # No need to clean up if the cluster is already terminated\n        # (i.e., prev_status is None), as the cleanup has already been done\n        # if the cluster is removed from the status table.\n        if post_teardown_cleanup:\n            self.post_teardown_cleanup(handle, terminate, purge)\n\n    def post_teardown_cleanup(self,\n                              handle: CloudVmRayResourceHandle,\n                              terminate: bool,\n                              purge: bool = False,\n                              remove_from_db: bool = True,\n                              failover: bool = False) -> None:\n        \"\"\"Cleanup local configs/caches and delete TPUs after teardown.\n\n        This method will handle the following cleanup steps:\n        * Deleting the TPUs;\n        * Removing ssh configs for the cluster;\n        * Deleting the open ports;\n        * Deleting the custom multi network infrastructure based on the\n          failover flag (e.g. delete firewalls, subnets, and VPCs for GPU\n          Direct if failover is False, otherwise, only delete the subnets);\n        * Updating the local state of the cluster;\n        * Removing the terminated cluster's scripts and ray yaml files.\n        \"\"\"\n        cluster_name_on_cloud = handle.cluster_name_on_cloud\n        cloud = handle.launched_resources.cloud\n\n        if terminate and handle.launched_resources.is_image_managed is True:\n            # Delete the image when terminating a \"cloned\" cluster, i.e.,\n            # whose image is created by SkyPilot (--clone-disk-from)\n            logger.debug(f'Deleting image {handle.launched_resources.image_id}')\n            cluster_resources = handle.launched_resources\n            cluster_cloud = cluster_resources.cloud\n            image_dict = cluster_resources.image_id\n            assert cluster_cloud is not None, cluster_resources\n            assert image_dict is not None and len(image_dict) == 1\n            image_id = list(image_dict.values())[0]\n            try:\n                cluster_cloud.delete_image(image_id,\n                                           handle.launched_resources.region)\n            except exceptions.CommandError as e:\n                logger.warning(\n                    f'Failed to delete cloned image {image_id}. Please '\n                    'remove it manually to avoid image leakage. Details: '\n                    f'{common_utils.format_exception(e, use_bracket=True)}')\n        if terminate:\n            # This function could be directly called from status refresh,\n            # where we need to cleanup the cluster profile.\n            metadata_utils.remove_cluster_metadata(handle.cluster_name)\n            # The cluster yaml does not exist when skypilot has not found\n            # the right resource to provision the cluster.\n            if handle.cluster_yaml is not None:\n                launched_resources = (\n                    handle.launched_resources.assert_launchable())\n                cloud = launched_resources.cloud\n                config = global_user_state.get_cluster_yaml_dict(\n                    handle.cluster_yaml)\n                ports_cleaned_up = False\n                custom_multi_network_cleaned_up = False\n                try:\n                    cloud.check_features_are_supported(\n                        launched_resources,\n                        {clouds.CloudImplementationFeatures.OPEN_PORTS})\n                    provision_lib.cleanup_ports(repr(cloud),\n                                                cluster_name_on_cloud,\n                                                handle.launched_resources.ports,\n                                                config['provider'])\n                    ports_cleaned_up = True\n                except exceptions.NotSupportedError:\n                    ports_cleaned_up = True\n                except exceptions.PortDoesNotExistError:\n                    logger.debug('Ports do not exist. Skipping cleanup.')\n                    ports_cleaned_up = True\n                except Exception as e:  # pylint: disable=broad-except\n                    if purge:\n                        msg = common_utils.format_exception(e, use_bracket=True)\n                        logger.warning(\n                            f'Failed to cleanup ports. Skipping since purge is '\n                            f'set. Details: {msg}')\n                    else:\n                        raise\n\n                # Clean up custom multi networks, e.g. the subnets, firewalls,\n                # and VPCs created for GCP GPUDirect TCPX\n                try:\n                    cloud.check_features_are_supported(\n                        handle.launched_resources, {\n                            clouds.CloudImplementationFeatures.\n                            CUSTOM_MULTI_NETWORK\n                        })\n                    provision_lib.cleanup_custom_multi_network(\n                        repr(cloud), cluster_name_on_cloud, config['provider'],\n                        failover)\n                    custom_multi_network_cleaned_up = True\n                except exceptions.NotSupportedError:\n                    custom_multi_network_cleaned_up = True\n                except Exception as e:  # pylint: disable=broad-except\n                    if purge:\n                        msg = common_utils.format_exception(e, use_bracket=True)\n                        logger.warning(\n                            f'Failed to cleanup custom multi network. Skipping '\n                            f'since purge is set. Details: {msg}')\n                    else:\n                        raise\n\n                if ports_cleaned_up and custom_multi_network_cleaned_up:\n                    try:\n                        self.remove_cluster_config(handle)\n                    except Exception as e:  # pylint: disable=broad-except\n                        if purge:\n                            msg = common_utils.format_exception(\n                                e, use_bracket=True)\n                            logger.warning(\n                                f'Failed to remove cluster config. Skipping '\n                                f'since purge is set. Details: {msg}')\n                        else:\n                            raise\n\n        cluster_utils.SSHConfigHelper.remove_cluster(handle.cluster_name)\n\n        def _detect_abnormal_non_terminated_nodes(\n                handle: CloudVmRayResourceHandle) -> None:\n            # Confirm that instances have actually transitioned state before\n            # updating the state database. We do this immediately before\n            # removing the state from the database, so that we can guarantee\n            # that this is always called before the state is removed. We\n            # considered running this check as part of\n            # provisioner.teardown_cluster or provision.terminate_instances, but\n            # it would open the door to code paths that successfully call this\n            # function but do not first call teardown_cluster or\n            # terminate_instances. See\n            # https://github.com/skypilot-org/skypilot/pull/4443#discussion_r1872798032\n            attempts = 0\n            while True:\n                config = global_user_state.get_cluster_yaml_dict(\n                    handle.cluster_yaml)\n\n                logger.debug(f'instance statuses attempt {attempts + 1}')\n                node_status_dict = provision_lib.query_instances(\n                    repr(cloud),\n                    handle.cluster_name,\n                    cluster_name_on_cloud,\n                    config['provider'],\n                    non_terminated_only=False)\n\n                unexpected_nodes = []\n                for node_id, node_status_tuple in node_status_dict.items():\n                    node_status, reason = node_status_tuple\n                    reason_str = '' if reason is None else f' ({reason})'\n                    logger.debug(f'{node_id} status: {node_status}{reason_str}')\n                    # FIXME(cooperc): Some clouds (e.g. GCP) do not distinguish\n                    # between \"stopping/stopped\" and \"terminating/terminated\",\n                    # so we allow for either status instead of casing on\n                    # `terminate`.\n                    if node_status not in [\n                            None, status_lib.ClusterStatus.STOPPED\n                    ]:\n                        unexpected_nodes.append((node_id, node_status, reason))\n\n                if not unexpected_nodes:\n                    break\n\n                attempts += 1\n                if attempts < _TEARDOWN_WAIT_MAX_ATTEMPTS:\n                    time.sleep(_TEARDOWN_WAIT_BETWEEN_ATTEMPS_SECONDS)\n                else:\n                    unexpected_nodes_str = '\\n'.join([\n                        f'  - {node_id}: {node_status}' +\n                        (f' ({reason})' if reason else '')\n                        for node_id, node_status, reason in unexpected_nodes\n                    ])\n                    raise RuntimeError(f'Instances in unexpected state:\\n'\n                                       f'{unexpected_nodes_str}')\n\n        # If cluster_yaml is None, the cluster should ensured to be terminated,\n        # so we don't need to do the double check.\n        if handle.cluster_yaml is not None:\n            try:\n                _detect_abnormal_non_terminated_nodes(handle)\n            except exceptions.ClusterStatusFetchingError as e:\n                if purge:\n                    msg = common_utils.format_exception(e, use_bracket=True)\n                    logger.warning(\n                        'Failed abnormal non-terminated nodes cleanup. '\n                        'Skipping and cleaning up as purge is set. '\n                        f'Details: {msg}')\n                    logger.debug(f'Full exception details: {msg}',\n                                 exc_info=True)\n                else:\n                    raise\n\n        if not terminate or remove_from_db:\n            global_user_state.remove_cluster(handle.cluster_name,\n                                             terminate=terminate)\n\n    def remove_cluster_config(self, handle: CloudVmRayResourceHandle) -> None:\n        \"\"\"Remove the YAML config of a cluster.\"\"\"\n        cluster_yaml_path = handle.cluster_yaml\n        handle.cluster_yaml = None\n        global_user_state.update_cluster_handle(handle.cluster_name, handle)\n        # Removing the cluster YAML can cause some unexpected stability issues.\n        # See #5011.\n        # global_user_state.remove_cluster_yaml(handle.cluster_name)\n        common_utils.remove_file_if_exists(cluster_yaml_path)\n\n    def set_autostop(self,\n                     handle: CloudVmRayResourceHandle,\n                     idle_minutes_to_autostop: Optional[int],\n                     wait_for: Optional[autostop_lib.AutostopWaitFor],\n                     down: bool = False,\n                     stream_logs: bool = True) -> None:\n        # The core.autostop() function should have already checked that the\n        # cloud and resources support requested autostop.\n        if idle_minutes_to_autostop is not None:\n            # Skip auto-stop for Kubernetes and RunPod clusters.\n            if (isinstance(handle.launched_resources.cloud,\n                           (clouds.Kubernetes, clouds.RunPod)) and not down and\n                    idle_minutes_to_autostop >= 0):\n                # We should hit this code path only for the controllers on\n                # Kubernetes and RunPod clusters, because autostop() will\n                # skip the supported feature check. Non-controller k8s/runpod\n                # clusters will have already errored out.\n                controller = controller_utils.Controllers.from_name(\n                    handle.cluster_name)\n                assert (controller is not None), handle.cluster_name\n                if (controller\n                        == controller_utils.Controllers.SKY_SERVE_CONTROLLER and\n                        isinstance(handle.launched_resources.cloud,\n                                   clouds.Kubernetes)):\n                    # For SkyServe controllers on Kubernetes: override autostop\n                    # behavior to force autodown (instead of no-op)\n                    # to avoid dangling controllers.\n\n                    # down = False is the default, but warn the user in case\n                    # they have explicitly specified it.\n                    # TODO(cooperc): Fix for new autostop stuff.\n                    config_override_down = skypilot_config.get_nested(\n                        (controller.value.controller_type, 'controller',\n                         'autostop', 'down'), None)\n                    if config_override_down is False:  # will not match None\n                        logger.warning(\n                            'SkyServe controller autodown is disabled in the '\n                            '~/.sky/config.yaml configuration file '\n                            '(serve.controller.autostop.down_when_idle), but '\n                            'it is force enabled for Kubernetes clusters.')\n\n                    down = True\n                else:\n                    logger.info('Auto-stop is not supported for Kubernetes '\n                                'and RunPod clusters. Skipping.')\n                    return\n\n            # Check if we're stopping spot\n            assert (handle.launched_resources is not None and\n                    handle.launched_resources.cloud is not None), handle\n            if handle.is_grpc_enabled_with_flag:\n                request = autostopv1_pb2.SetAutostopRequest(\n                    idle_minutes=idle_minutes_to_autostop,\n                    backend=self.NAME,\n                    wait_for=wait_for.to_protobuf() if wait_for is not None else\n                    autostopv1_pb2.AUTOSTOP_WAIT_FOR_UNSPECIFIED,\n                    down=down,\n                )\n                backend_utils.invoke_skylet_with_retries(lambda: SkyletClient(\n                    handle.get_grpc_channel()).set_autostop(request))\n            else:\n                code = autostop_lib.AutostopCodeGen.set_autostop(\n                    idle_minutes_to_autostop, self.NAME, wait_for, down)\n                returncode, _, stderr = self.run_on_head(\n                    handle, code, require_outputs=True, stream_logs=stream_logs)\n                subprocess_utils.handle_returncode(returncode,\n                                                   code,\n                                                   'Failed to set autostop',\n                                                   stderr=stderr,\n                                                   stream_logs=stream_logs)\n            global_user_state.set_cluster_autostop_value(\n                handle.cluster_name, idle_minutes_to_autostop, down)\n\n        # Add/Remove autodown annotations to/from Kubernetes pods.\n        if isinstance(handle.launched_resources.cloud, clouds.Kubernetes):\n            kubernetes_utils.set_autodown_annotations(\n                handle=handle,\n                idle_minutes_to_autostop=idle_minutes_to_autostop,\n                down=down)\n\n    def is_definitely_autostopping(self,\n                                   handle: CloudVmRayResourceHandle,\n                                   stream_logs: bool = True) -> bool:\n        \"\"\"Check if the cluster is autostopping.\n\n        Returns:\n            True if the cluster is definitely autostopping. It is possible\n            that the cluster is still autostopping when False is returned,\n            due to errors like transient network issues.\n        \"\"\"\n        if handle.head_ip is None:\n            # The head node of the cluster is not UP or in an abnormal state.\n            # We cannot check if the cluster is autostopping.\n            return False\n        if handle.is_grpc_enabled_with_flag:\n            try:\n                request = autostopv1_pb2.IsAutostoppingRequest()\n                response = backend_utils.invoke_skylet_with_retries(\n                    lambda: SkyletClient(handle.get_grpc_channel()\n                                        ).is_autostopping(request))\n                return response.is_autostopping\n            except Exception as e:  # pylint: disable=broad-except\n                # The cluster may have been terminated, causing the gRPC call\n                # to timeout and fail.\n                logger.debug(f'Failed to check if cluster is autostopping: {e}')\n                return False\n        else:\n            code = autostop_lib.AutostopCodeGen.is_autostopping()\n            returncode, stdout, stderr = self.run_on_head(\n                handle, code, require_outputs=True, stream_logs=stream_logs)\n            if returncode == 0:\n                return message_utils.decode_payload(stdout)\n            logger.debug('Failed to check if cluster is autostopping with '\n                         f'{returncode}: {stdout+stderr}\\n'\n                         f'Command: {code}')\n            return False\n\n    # TODO(zhwu): Refactor this to a CommandRunner class, so different backends\n    # can support its own command runner.\n    @timeline.event\n    @context_utils.cancellation_guard\n    def run_on_head(\n        self,\n        handle: CloudVmRayResourceHandle,\n        cmd: str,\n        *,\n        port_forward: Optional[List[int]] = None,\n        log_path: str = '/dev/null',\n        stream_logs: bool = False,\n        ssh_mode: command_runner.SshMode = command_runner.SshMode.\n        NON_INTERACTIVE,\n        under_remote_workdir: bool = False,\n        require_outputs: bool = False,\n        separate_stderr: bool = False,\n        process_stream: bool = True,\n        source_bashrc: bool = False,\n        **kwargs,\n    ) -> Union[int, Tuple[int, str, str]]:\n        \"\"\"Runs 'cmd' on the cluster's head node.\n\n        It will try to fetch the head node IP if it is not cached.\n\n        Args:\n            handle: The ResourceHandle to the cluster.\n            cmd: The command to run.\n\n            Advanced options:\n\n            port_forward: A list of ports to forward.\n            log_path: The path to the log file.\n            stream_logs: Whether to stream the logs to stdout/stderr.\n            ssh_mode: The mode to use for ssh.\n                See command_runner.SSHCommandRunner.SSHMode for more details.\n            under_remote_workdir: Whether to run the command under the remote\n                workdir ~/sky_workdir.\n            require_outputs: Whether to return the stdout and stderr of the\n                command.\n            separate_stderr: Whether to separate stderr from stdout.\n            process_stream: Whether to post-process the stdout/stderr of the\n                command, such as replacing or skipping lines on the fly. If\n                enabled, lines are printed only when '\\r' or '\\n' is found.\n            source_bashrc: Whether to source bashrc when running on the command\n                on the VM. If it is a user-related commands, it would always be\n                good to source bashrc to make sure the env vars are set.\n\n        Returns:\n            returncode\n            or\n            A tuple of (returncode, stdout, stderr).\n\n        Raises:\n            exceptions.FetchClusterInfoError: If the cluster info cannot be\n                fetched.\n        \"\"\"\n        # This will try to fetch the head node IP if it is not cached.\n\n        runners = handle.get_command_runners()\n        head_runner = runners[0]\n        if under_remote_workdir:\n            cmd = f'cd {SKY_REMOTE_WORKDIR} && {cmd}'\n\n        return head_runner.run(\n            cmd,\n            port_forward=port_forward,\n            log_path=log_path,\n            process_stream=process_stream,\n            stream_logs=stream_logs,\n            ssh_mode=ssh_mode,\n            require_outputs=require_outputs,\n            separate_stderr=separate_stderr,\n            source_bashrc=source_bashrc,\n            **kwargs,\n        )\n\n    # --- Utilities ---\n\n    @timeline.event\n    def _check_existing_cluster(\n            self,\n            task: task_lib.Task,\n            to_provision: Optional[resources_lib.Resources],\n            cluster_name: str,\n            dryrun: bool = False) -> RetryingVmProvisioner.ToProvisionConfig:\n        \"\"\"Checks if the cluster exists and returns the provision config.\n\n        Raises:\n            exceptions.ResourcesMismatchError: If the resources in the task\n                does not match the existing cluster.\n            exceptions.InvalidClusterNameError: If the cluster name is invalid.\n            # TODO(zhwu): complete the list of exceptions.\n        \"\"\"\n        record = global_user_state.get_cluster_from_name(\n            cluster_name, include_user_info=False, summary_response=True)\n        if record is None:\n            handle_before_refresh = None\n            status_before_refresh = None\n        else:\n            handle_before_refresh = record['handle']\n            status_before_refresh = record['status']\n\n        handle: Optional[CloudVmRayResourceHandle]\n        prev_cluster_status, handle = (status_before_refresh,\n                                       handle_before_refresh)\n\n        if not dryrun:\n            # We force refresh any cluster (1) with INIT status, or (2) has\n            # autostop set. This is to determine the actual state of such a\n            # cluster and to make the hint that uses prev_cluster_status more\n            # accurate.\n            record = backend_utils.refresh_cluster_record(\n                cluster_name,\n                force_refresh_statuses={status_lib.ClusterStatus.INIT},\n                cluster_lock_already_held=True,\n                include_user_info=False,\n                summary_response=True,\n            )\n            if record is not None:\n                prev_cluster_status = record['status']\n                handle = record['handle']\n            else:\n                prev_cluster_status = None\n                handle = None\n        # We should check the cluster_ever_up after refresh, because if the\n        # cluster is terminated (through console or auto-down), the record will\n        # become None and the cluster_ever_up should be considered as False.\n        cluster_ever_up = record is not None and record['cluster_ever_up']\n        prev_config_hash = record['config_hash'] if record is not None else None\n        logger.debug(f'cluster_ever_up: {cluster_ever_up}')\n        logger.debug(f'record: {record}')\n\n        if prev_cluster_status is not None:\n            assert handle is not None\n            # Cluster already exists.\n            self.check_resources_fit_cluster(handle, task)\n\n            # Use the existing cluster.\n            assert handle.launched_resources is not None, (cluster_name, handle)\n            # Take a random resource in order to get resource info that applies\n            # to all resources.\n            one_task_resource = list(task.resources)[0]\n\n            # Assume resources share the same ports.\n            for resource in task.resources:\n                assert resource.ports == one_task_resource.ports\n            requested_ports_set = resources_utils.port_ranges_to_set(\n                one_task_resource.ports)\n            current_ports_set = resources_utils.port_ranges_to_set(\n                handle.launched_resources.ports)\n            all_ports = resources_utils.port_set_to_ranges(current_ports_set |\n                                                           requested_ports_set)\n            to_provision = handle.launched_resources\n            assert to_provision is not None\n            to_provision = to_provision.assert_launchable()\n            if (to_provision.cloud.OPEN_PORTS_VERSION <=\n                    clouds.OpenPortsVersion.LAUNCH_ONLY):\n                if not requested_ports_set <= current_ports_set:\n                    current_cloud = to_provision.cloud\n                    with ux_utils.print_exception_no_traceback():\n                        raise exceptions.NotSupportedError(\n                            'Failed to open new ports on an existing cluster '\n                            f'with the current cloud {current_cloud} as it only'\n                            ' supports opening ports on launch of the cluster. '\n                            'Please terminate the existing cluster and launch '\n                            'a new cluster with the desired ports open.')\n            if all_ports:\n                to_provision = to_provision.copy(ports=all_ports)\n            # Docker login should always be the same for all resources, since\n            # it's set from envs.\n            for resource in task.resources:\n                assert (resource.docker_login_config ==\n                        one_task_resource.docker_login_config), (\n                            resource.docker_login_config,\n                            one_task_resource.docker_login_config)\n            # If we have docker login config in the new task, override the\n            # existing resources to pick up new credentials. This allows the\n            # user to specify new or fixed credentials if the existing\n            # credentials are not working. If we don't do this, the credentials\n            # from the existing resources will always be reused.\n            if one_task_resource.docker_login_config is not None:\n                to_provision = to_provision.copy(\n                    _docker_login_config=one_task_resource.docker_login_config)\n\n            # cluster_config_overrides should be the same for all resources.\n            for resource in task.resources:\n                assert (resource.cluster_config_overrides ==\n                        one_task_resource.cluster_config_overrides)\n\n            cluster_yaml_str = global_user_state.get_cluster_yaml_str(\n                cluster_name)\n            cluster_yaml_obj = (yaml_utils.safe_load(cluster_yaml_str)\n                                if cluster_yaml_str is not None else None)\n\n            def _get_pod_config(yaml_obj: Dict[str, Any]) -> Dict[str, Any]:\n                return (yaml_obj.get('available_node_types',\n                                     {}).get('ray_head_default',\n                                             {}).get('node_config', {}))\n\n            if isinstance(to_provision.cloud,\n                          clouds.Kubernetes) and cluster_yaml_obj is not None:\n                # Warn users if the Kubernetes pod config is different\n                # from the existing cluster.\n                desired_cluster_yaml_obj = (\n                    kubernetes_utils.combine_pod_config_fields_and_metadata(\n                        cluster_yaml_obj,\n                        cluster_config_overrides=one_task_resource.\n                        cluster_config_overrides,\n                        cloud=to_provision.cloud,\n                        context=to_provision.region))\n\n                if _get_pod_config(desired_cluster_yaml_obj) != _get_pod_config(\n                        cluster_yaml_obj):\n                    # pylint: disable=line-too-long\n                    logger.warning(\n                        f'{colorama.Fore.YELLOW}WARNING: Kubernetes pod config mismatch detected. Task requires different '\n                        f'pod config than the existing cluster. The existing '\n                        f'cluster will be used with its current pod config.'\n                        f'To apply use your task\\'s new pod config:\\n'\n                        f'   Use a new cluster'\n                        f'   Or restart this cluster: sky down {cluster_name}; sky launch -c {cluster_name} ...'\n                        f'{colorama.Style.RESET_ALL}')\n\n            # Check for volume mount warnings\n            if task.volume_mounts:\n                # Get existing cluster's volume mounts from cluster yaml\n                existing_volume_names = set()\n                try:\n                    if cluster_yaml_obj is not None:\n                        # Extract volume names from existing cluster\n                        node_config = _get_pod_config(cluster_yaml_obj)\n\n                        if isinstance(to_provision.cloud, clouds.Kubernetes):\n                            # Check for K8s-style persistent volumes\n                            # (spec.volumes)\n                            # See sky/templates/kubernetes-ray.yml.j2.\n                            volumes = node_config.get('spec',\n                                                      {}).get('volumes', [])\n                            for vol in volumes:\n                                # Volume from PVC has structure:\n                                # - name: <volume_name>\n                                #   persistentVolumeClaim:\n                                #     claimName: <volume_name_on_cloud>\n                                if 'persistentVolumeClaim' in vol:\n                                    pvc = vol.get('persistentVolumeClaim', {})\n                                    # Use claimName (volume_name_on_cloud) to\n                                    # be consistent with RunPod.\n                                    vol_name_on_cloud = pvc.get('claimName')\n                                    if vol_name_on_cloud:\n                                        existing_volume_names.add(\n                                            vol_name_on_cloud)\n\n                            # Check for K8s ephemeral volumes\n                            # See sky/templates/kubernetes-ray.yml.j2.\n                            provider_config = cluster_yaml_obj.get(\n                                'provider', {})\n                            ephemeral_specs = provider_config.get(\n                                'ephemeral_volume_specs', [])\n                            for spec in ephemeral_specs:\n                                # For ephemeral volumes, we check the mount\n                                # path.\n                                mount_path = spec.get('path')\n                                if mount_path:\n                                    existing_volume_names.add(mount_path)\n\n                        elif isinstance(to_provision.cloud, clouds.RunPod):\n                            # Check for custom VolumeMounts config\n                            # (e.g. RunPod)\n                            # See sky/templates/runpod-ray.yml.j2.\n                            volume_mounts_config = node_config.get(\n                                'VolumeMounts', [])\n                            for vol_mount in volume_mounts_config:\n                                vol_name = vol_mount.get('VolumeNameOnCloud')\n                                if vol_name:\n                                    existing_volume_names.add(vol_name)\n                except Exception as e:  # pylint: disable=broad-except\n                    # If we can't get the existing volume mounts, log debug\n                    # and skip the warning check\n                    logger.debug(f'Failed to check existing volume mounts: {e}',\n                                 exc_info=True)\n\n                # Check if task has new volumes not in existing cluster\n                new_ephemeral_volumes = []\n                new_persistent_volumes = []\n                for volume_mount in task.volume_mounts:\n                    # Compare using volume_name for user-facing name\n                    if volume_mount.is_ephemeral:\n                        if volume_mount.path not in existing_volume_names:\n                            new_ephemeral_volumes.append(volume_mount.path)\n                    elif (volume_mount.volume_name not in existing_volume_names\n                          and volume_mount.volume_config.name_on_cloud\n                          not in existing_volume_names):\n                        new_persistent_volumes.append(volume_mount.volume_name)\n\n                if new_ephemeral_volumes or new_persistent_volumes:\n                    msg_parts = []\n                    if new_ephemeral_volumes:\n                        msg_parts.append(f'new ephemeral volume(s) with path '\n                                         f'{\", \".join(new_ephemeral_volumes)}')\n                    if new_persistent_volumes:\n                        msg_parts.append(\n                            f'new volume(s) {\", \".join(new_persistent_volumes)}'\n                        )\n\n                    volume_msg = ' and '.join(msg_parts)\n                    # Capitalize the first letter of the message\n                    volume_msg = volume_msg[0].upper() + volume_msg[1:]\n\n                    logger.warning(\n                        f'{colorama.Fore.YELLOW}WARNING: {volume_msg} '\n                        f'specified in task but not '\n                        f'mounted to existing cluster \"{cluster_name}\". '\n                        f'These volumes will not be mounted to the cluster. '\n                        f'To mount new volumes, either:\\n'\n                        f'   Use a new cluster, or\\n'\n                        f'   Terminate and recreate this cluster'\n                        f'{colorama.Style.RESET_ALL}')\n\n            return RetryingVmProvisioner.ToProvisionConfig(\n                cluster_name,\n                to_provision,\n                handle.launched_nodes,\n                prev_cluster_status=prev_cluster_status,\n                prev_handle=handle,\n                prev_cluster_ever_up=cluster_ever_up,\n                prev_config_hash=prev_config_hash)\n        usage_lib.messages.usage.set_new_cluster()\n        # Use the task_cloud, because the cloud in `to_provision` can be changed\n        # later during the retry.\n        common_utils.check_cluster_name_is_valid(cluster_name)\n\n        if to_provision is None:\n            # Recently terminated after refresh. OPTIMIZE usually ran outside\n            # the lock, so that decision may be stale by now. Under the lock,\n            # ensure we always have a concrete plan via the following order:\n            #   1) Reuse last placement snapshot (if available);\n            #   2) Else, call injected planner for a fresh plan.\n            # If we still have a pre-refresh handle snapshot with a concrete\n            # placement, prefer reusing it.\n            if (isinstance(handle_before_refresh, CloudVmRayResourceHandle) and\n                    handle_before_refresh.launched_resources is not None):\n                to_provision = handle_before_refresh.launched_resources\n                # Ensure the requested task fits the previous placement.\n                self.check_resources_fit_cluster(handle_before_refresh, task)\n                # Mirror the original message for reuse path.\n                status_before_refresh_str = None\n                if status_before_refresh is not None:\n                    status_before_refresh_str = status_before_refresh.value\n                logger.info(\n                    f'The cluster {cluster_name!r} (status: '\n                    f'{status_before_refresh_str}) was not found on the cloud: '\n                    'it may be autodowned, manually terminated, or its launch '\n                    'never succeeded. Provisioning a new cluster by using the '\n                    'same resources as its original launch.')\n            elif self._planner is not None:\n                to_provision = self._planner(task)\n                logger.info(\n                    'Previous placement snapshot missing; computing a fresh '\n                    'plan for provisioning.')\n            else:\n                # Without a snapshot or planner, we cannot proceed safely.\n                # Surface a user-friendly error without a long traceback.\n                with ux_utils.print_exception_no_traceback():\n                    raise RuntimeError(\n                        'No concrete launch plan available after recent cloud '\n                        f'termination of cluster {cluster_name!r}. Ensure the '\n                        'OPTIMIZE stage runs or provide concrete resources.')\n\n        return RetryingVmProvisioner.ToProvisionConfig(\n            cluster_name,\n            to_provision,\n            task.num_nodes,\n            prev_cluster_status=None,\n            prev_handle=None,\n            prev_cluster_ever_up=False,\n            prev_config_hash=prev_config_hash)\n\n    def _execute_file_mounts(self, handle: CloudVmRayResourceHandle,\n                             file_mounts: Optional[Dict[Path, Path]]):\n        \"\"\"Executes file mounts.\n\n        Rsyncing local files and copying from remote stores.\n        \"\"\"\n        # File mounts handling for remote paths possibly without write access:\n        #  (1) in 'file_mounts' sections, add <prefix> to these target paths.\n        #  (2) then, create symlinks from '/.../file' to '<prefix>/.../file'.\n        if file_mounts is None or not file_mounts:\n            return\n        symlink_commands = []\n        fore = colorama.Fore\n        style = colorama.Style\n        start = time.time()\n        runners = handle.get_command_runners()\n        log_path = os.path.join(self.log_dir, 'file_mounts.log')\n        num_threads = subprocess_utils.get_max_workers_for_file_mounts(\n            file_mounts, str(handle.launched_resources.cloud))\n\n        # Check the files and warn\n        for dst, src in file_mounts.items():\n            if not data_utils.is_cloud_store_url(src):\n                full_src = os.path.abspath(os.path.expanduser(src))\n                # Checked during Task.set_file_mounts().\n                assert os.path.exists(\n                    full_src), f'{full_src} does not exist. {file_mounts}'\n                src_size = backend_utils.path_size_megabytes(full_src)\n                if src_size >= _PATH_SIZE_MEGABYTES_WARN_THRESHOLD:\n                    logger.warning(\n                        f'  {fore.YELLOW}The size of file mount src {src!r} '\n                        f'is {src_size} MB. Try to keep src small or use '\n                        '.skyignore to exclude large files, as large sizes '\n                        f'will slow down rsync. {style.RESET_ALL}')\n                if os.path.islink(full_src):\n                    logger.warning(\n                        f'  {fore.YELLOW}Source path {src!r} is a symlink. '\n                        f'Symlink contents are not uploaded.{style.RESET_ALL}')\n\n        os.makedirs(os.path.expanduser(self.log_dir), exist_ok=True)\n        os.system(f'touch {log_path}')\n\n        rich_utils.force_update_status(\n            ux_utils.spinner_message('Syncing file mounts', log_path))\n\n        for dst, src in file_mounts.items():\n            # TODO: room for improvement.  Here there are many moving parts\n            # (download gsutil on remote, run gsutil on remote).  Consider\n            # alternatives (smart_open, each provider's own sdk), a\n            # data-transfer container etc.\n            if not os.path.isabs(dst) and not dst.startswith('~/'):\n                dst = f'{SKY_REMOTE_WORKDIR}/{dst}'\n            # Sync 'src' to 'wrapped_dst', a safe-to-write \"wrapped\" path.\n            wrapped_dst = dst\n            if not dst.startswith('~/') and not dst.startswith('/tmp/'):\n                # Handles the remote paths possibly without write access.\n                # (1) add <prefix> to these target paths.\n                wrapped_dst = backend_utils.FileMountHelper.wrap_file_mount(dst)\n                cmd = backend_utils.FileMountHelper.make_safe_symlink_command(\n                    source=dst, target=wrapped_dst)\n                symlink_commands.append(cmd)\n\n            if not data_utils.is_cloud_store_url(src):\n                full_src = os.path.abspath(os.path.expanduser(src))\n\n                if os.path.isfile(full_src):\n                    mkdir_for_wrapped_dst = (\n                        f'mkdir -p {os.path.dirname(wrapped_dst)}')\n                else:\n                    mkdir_for_wrapped_dst = f'mkdir -p {wrapped_dst}'\n\n                # TODO(mluo): Fix method so that mkdir and rsync run together\n                backend_utils.parallel_data_transfer_to_nodes(\n                    runners,\n                    source=src,\n                    target=wrapped_dst,\n                    cmd=mkdir_for_wrapped_dst,\n                    run_rsync=True,\n                    action_message='Syncing',\n                    log_path=log_path,\n                    stream_logs=False,\n                    num_threads=num_threads,\n                )\n                continue\n\n            storage = cloud_stores.get_storage_from_path(src)\n            if storage.is_directory(src):\n                sync_cmd = (storage.make_sync_dir_command(\n                    source=src, destination=wrapped_dst))\n                # It is a directory so make sure it exists.\n                mkdir_for_wrapped_dst = f'mkdir -p {wrapped_dst}'\n            else:\n                sync_cmd = (storage.make_sync_file_command(\n                    source=src, destination=wrapped_dst))\n                # It is a file so make sure *its parent dir* exists.\n                mkdir_for_wrapped_dst = (\n                    f'mkdir -p {os.path.dirname(wrapped_dst)}')\n\n            download_target_commands = [\n                # Ensure sync can write to wrapped_dst (e.g., '/data/').\n                mkdir_for_wrapped_dst,\n                # Both the wrapped and the symlink dir exist; sync.\n                sync_cmd,\n            ]\n            command = ' && '.join(download_target_commands)\n            # dst is only used for message printing.\n            backend_utils.parallel_data_transfer_to_nodes(\n                runners,\n                source=src,\n                target=dst,\n                cmd=command,\n                run_rsync=False,\n                action_message='Syncing',\n                log_path=log_path,\n                stream_logs=False,\n                # Need to source bashrc, as the cloud specific CLI or SDK may\n                # require PATH in bashrc.\n                source_bashrc=True,\n                num_threads=num_threads,\n            )\n        # (2) Run the commands to create symlinks on all the nodes.\n        symlink_command = ' && '.join(symlink_commands)\n        if symlink_command:\n            # ALIAS_SUDO_TO_EMPTY_FOR_ROOT_CMD sets sudo to empty string for\n            # root. We need this as we do not source bashrc for the command for\n            # better performance, and our sudo handling is only in bashrc.\n            symlink_command = (\n                f'{command_runner.ALIAS_SUDO_TO_EMPTY_FOR_ROOT_CMD} && '\n                f'{symlink_command}')\n\n            def _symlink_node(runner: command_runner.CommandRunner):\n                returncode = runner.run(symlink_command, log_path=log_path)\n                subprocess_utils.handle_returncode(\n                    returncode, symlink_command,\n                    'Failed to create symlinks. The target destination '\n                    f'may already exist. Log: {log_path}')\n\n            subprocess_utils.run_in_parallel(_symlink_node, runners,\n                                             num_threads)\n        end = time.time()\n        logger.debug(f'File mount sync took {end - start} seconds.')\n        logger.info(ux_utils.finishing_message('Synced file_mounts.', log_path))\n\n    def _execute_storage_mounts(\n            self, handle: CloudVmRayResourceHandle,\n            storage_mounts: Optional[Dict[Path, storage_lib.Storage]]):\n        \"\"\"Executes storage mounts: installing mounting tools and mounting.\"\"\"\n        # Handle cases where `storage_mounts` is None. This occurs when users\n        # initiate a 'sky start' command from a Skypilot version that predates\n        # the introduction of the `storage_mounts_metadata` feature.\n        if storage_mounts is None:\n            return\n\n        # Process only mount mode objects here. COPY mode objects have been\n        # converted to regular copy file mounts and thus have been handled\n        # in the '_execute_file_mounts' method.\n        storage_mounts = {\n            path: storage_mount\n            for path, storage_mount in storage_mounts.items()\n            if storage_mount.mode in storage_lib.MOUNTABLE_STORAGE_MODES\n        }\n\n        # Handle cases when there aren't any Storages with either MOUNT or\n        # MOUNT_CACHED mode.\n        if not storage_mounts:\n            return\n        start = time.time()\n        runners = handle.get_command_runners()\n        num_threads = subprocess_utils.get_parallel_threads(\n            str(handle.launched_resources.cloud))\n        log_path = os.path.join(self.log_dir, 'storage_mounts.log')\n\n        plural = 's' if len(storage_mounts) > 1 else ''\n        rich_utils.force_update_status(\n            ux_utils.spinner_message(\n                f'Mounting {len(storage_mounts)} storage{plural}', log_path))\n\n        for dst, storage_obj in storage_mounts.items():\n            storage_obj.construct()\n            if not os.path.isabs(dst) and not dst.startswith('~/'):\n                dst = f'{SKY_REMOTE_WORKDIR}/{dst}'\n            # Raised when the bucket is externall removed before re-mounting\n            # with sky start.\n            if not storage_obj.stores:\n                with ux_utils.print_exception_no_traceback():\n                    raise exceptions.StorageExternalDeletionError(\n                        f'The bucket, {storage_obj.name!r}, could not be '\n                        f'mounted on cluster {handle.cluster_name!r}. Please '\n                        'verify that the bucket exists. The cluster started '\n                        'successfully without mounting the bucket.')\n            # Get the first store and use it to mount\n            store = list(storage_obj.stores.values())[0]\n            assert store is not None, storage_obj\n            if storage_obj.mode == storage_lib.StorageMode.MOUNT:\n                mount_cmd = store.mount_command(dst)\n                action_message = 'Mounting'\n            else:\n                assert storage_obj.mode == storage_lib.StorageMode.MOUNT_CACHED\n                mount_cmd = store.mount_cached_command(dst)\n                action_message = 'Mounting cached mode'\n            src_print = (storage_obj.source\n                         if storage_obj.source else storage_obj.name)\n            if isinstance(src_print, list):\n                src_print = ', '.join(src_print)\n            try:\n                backend_utils.parallel_data_transfer_to_nodes(\n                    runners,\n                    source=src_print,\n                    target=dst,\n                    cmd=mount_cmd,\n                    run_rsync=False,\n                    action_message=action_message,\n                    log_path=log_path,\n                    # Need to source bashrc, as the cloud specific CLI or SDK\n                    # may require PATH in bashrc.\n                    source_bashrc=True,\n                    num_threads=num_threads,\n                )\n            except exceptions.CommandError as e:\n                if e.returncode == exceptions.MOUNT_PATH_NON_EMPTY_CODE:\n                    mount_path = (f'{colorama.Fore.RED}'\n                                  f'{colorama.Style.BRIGHT}{dst}'\n                                  f'{colorama.Style.RESET_ALL}')\n                    error_msg = (f'Mount path {mount_path} is non-empty.'\n                                 f' {mount_path} may be a standard unix '\n                                 f'path or may contain files from a previous'\n                                 f' task. To fix, change the mount path'\n                                 f' to an empty or non-existent path.')\n                    raise RuntimeError(error_msg) from None\n                else:\n                    # By default, raising an error caused from mounting_utils\n                    # shows a big heredoc as part of it. Here, we want to\n                    # conditionally show the heredoc only if SKYPILOT_DEBUG\n                    # is set\n                    if env_options.Options.SHOW_DEBUG_INFO.get():\n                        raise exceptions.CommandError(\n                            e.returncode,\n                            command='to mount',\n                            error_msg=e.error_msg,\n                            detailed_reason=e.detailed_reason)\n                    else:\n                        # Strip the command (a big heredoc) from the exception\n                        raise exceptions.CommandError(\n                            e.returncode,\n                            command='to mount',\n                            error_msg=e.error_msg,\n                            detailed_reason=e.detailed_reason) from None\n\n        end = time.time()\n        logger.debug(f'Storage mount sync took {end - start} seconds.')\n        logger.info(ux_utils.finishing_message('Storage mounted.', log_path))\n\n    def _set_storage_mounts_metadata(\n            self, cluster_name: str,\n            storage_mounts: Optional[Dict[Path, storage_lib.Storage]]) -> None:\n        \"\"\"Sets 'storage_mounts' object in cluster's storage_mounts_metadata.\n\n        After converting Storage objects in 'storage_mounts' to metadata,\n        it stores {PATH: StorageMetadata} into the table.\n        \"\"\"\n        if not storage_mounts:\n            return\n        storage_mounts_metadata = {}\n        for dst, storage_obj in storage_mounts.items():\n            if storage_obj.mode not in storage_lib.MOUNTABLE_STORAGE_MODES:\n                # Skip non-mount storage objects, as there is no need to\n                # reconstruct them during cluster restart.\n                continue\n            storage_mounts_metadata[dst] = storage_obj.handle\n        lock_id = backend_utils.cluster_file_mounts_lock_id(cluster_name)\n        lock_timeout = backend_utils.CLUSTER_FILE_MOUNTS_LOCK_TIMEOUT_SECONDS\n        try:\n            with locks.get_lock(lock_id, lock_timeout):\n                global_user_state.set_cluster_storage_mounts_metadata(\n                    cluster_name, storage_mounts_metadata)\n        except locks.LockTimeout as e:\n            raise RuntimeError(\n                f'Failed to store metadata for cluster {cluster_name!r} due to '\n                'a timeout when trying to access local database. Please '\n                f'try again or manually remove the lock at {lock_id}. '\n                f'{common_utils.format_exception(e)}') from None\n\n    def get_storage_mounts_metadata(\n            self,\n            cluster_name: str) -> Optional[Dict[Path, storage_lib.Storage]]:\n        \"\"\"Gets 'storage_mounts' object from cluster's storage_mounts_metadata.\n\n        After retrieving storage_mounts_metadata, it converts back the\n        StorageMetadata to Storage object and restores 'storage_mounts.'\n        \"\"\"\n        lock_id = backend_utils.cluster_file_mounts_lock_id(cluster_name)\n        lock_timeout = backend_utils.CLUSTER_FILE_MOUNTS_LOCK_TIMEOUT_SECONDS\n        try:\n            with locks.get_lock(lock_id, lock_timeout):\n                storage_mounts_metadata = (\n                    global_user_state.get_cluster_storage_mounts_metadata(\n                        cluster_name))\n        except locks.LockTimeout as e:\n            raise RuntimeError(\n                f'Failed to retrieve metadata for cluster {cluster_name!r} '\n                'due to a timeout when trying to access local database. '\n                f'Please try again or manually remove the lock at {lock_id}.'\n                f' {common_utils.format_exception(e)}') from None\n\n        if storage_mounts_metadata is None:\n            return None\n        storage_mounts = {}\n        for dst, storage_metadata in storage_mounts_metadata.items():\n            # Setting 'sync_on_reconstruction' to False prevents from Storage\n            # object creation to sync local source syncing to the bucket. Local\n            # source specified in Storage object is synced to the bucket only\n            # when it is created with 'sky launch'.\n            storage_mounts[dst] = storage_lib.Storage.from_metadata(\n                storage_metadata, sync_on_reconstruction=False)\n        return storage_mounts\n\n    def _skypilot_predefined_env_vars(\n            self, handle: CloudVmRayResourceHandle) -> Dict[str, str]:\n        \"\"\"Returns the SkyPilot predefined environment variables.\n\n        TODO(zhwu): Check if a single variable for all the cluster info is more\n        desirable or separate variables for each piece of info.\n        NOTE: In order to avoid complication in a potential future separation\n        of the info into multiple env vars, we should not treat this json format\n        as a sink for all the cluster info.\n        \"\"\"\n        return {\n            'SKYPILOT_CLUSTER_INFO': json.dumps({\n                'cluster_name': handle.cluster_name,\n                'cloud': str(handle.launched_resources.cloud),\n                'region': handle.launched_resources.region,\n                'zone': handle.launched_resources.zone,\n            })\n        }\n\n    def _get_task_env_vars(self, task: task_lib.Task, job_id: int,\n                           handle: CloudVmRayResourceHandle) -> Dict[str, str]:\n        \"\"\"Returns the environment variables for the task.\"\"\"\n        env_vars = task_lib.get_plaintext_envs_and_secrets(\n            task.envs_and_secrets)\n        # If it is a managed job, the TASK_ID_ENV_VAR will have been already set\n        # by the controller.\n        if constants.TASK_ID_ENV_VAR not in env_vars:\n            env_vars[\n                constants.TASK_ID_ENV_VAR] = common_utils.get_global_job_id(\n                    self.run_timestamp,\n                    cluster_name=handle.cluster_name,\n                    job_id=str(job_id))\n        env_vars.update(self._skypilot_predefined_env_vars(handle))\n        return env_vars\n\n    def _get_managed_job_user_id(self, task: task_lib.Task) -> Optional[str]:\n        \"\"\"Returns the user id for the managed job.\"\"\"\n        if task.managed_job_dag is not None:\n            return task.envs[constants.USER_ID_ENV_VAR]\n        return None\n\n    def _get_task_codegen_class(\n            self, handle: CloudVmRayResourceHandle) -> task_codegen.TaskCodeGen:\n        \"\"\"Returns the appropriate TaskCodeGen for the given handle.\"\"\"\n        if isinstance(handle.launched_resources.cloud, clouds.Slurm):\n            assert (handle.cached_cluster_info\n                    is not None), ('cached_cluster_info must be set')\n            head_instance = handle.cached_cluster_info.get_head_instance()\n            assert (head_instance is not None), (\n                'Head instance not found in cached cluster info')\n            slurm_job_id = head_instance.tags.get('job_id')\n            assert (slurm_job_id\n                    is not None), ('job_id tag not found in head instance')\n            return task_codegen.SlurmCodeGen(slurm_job_id=slurm_job_id)\n        else:\n            return task_codegen.RayCodeGen()\n\n    def _execute_task_one_node(self, handle: CloudVmRayResourceHandle,\n                               task: task_lib.Task, job_id: int,\n                               remote_log_dir: str) -> None:\n        # Launch the command as a Ray task.\n        log_dir = os.path.join(remote_log_dir, 'tasks')\n\n        resources_dict = backend_utils.get_task_demands_dict(task)\n        internal_ips = handle.internal_ips()\n        assert internal_ips is not None, 'internal_ips is not cached in handle'\n\n        task_env_vars = self._get_task_env_vars(task, job_id, handle)\n\n        codegen = self._get_task_codegen_class(handle)\n\n        codegen.add_prologue(job_id)\n        codegen.add_setup(\n            1,\n            resources_dict,\n            stable_cluster_internal_ips=internal_ips,\n            env_vars=task_env_vars,\n            log_dir=log_dir,\n            setup_cmd=self._setup_cmd,\n        )\n\n        codegen.add_task(\n            1,\n            bash_script=task.run,\n            env_vars=task_env_vars,\n            task_name=task.name,\n            resources_dict=backend_utils.get_task_demands_dict(task),\n            log_dir=log_dir)\n\n        codegen.add_epilogue()\n\n        self._exec_code_on_head(\n            handle,\n            codegen.build(),\n            job_id,\n            managed_job_dag=task.managed_job_dag,\n            managed_job_user_id=self._get_managed_job_user_id(task),\n            remote_log_dir=remote_log_dir)\n\n    def _execute_task_n_nodes(self, handle: CloudVmRayResourceHandle,\n                              task: task_lib.Task, job_id: int,\n                              remote_log_dir: str) -> None:\n        # Strategy:\n        #   ray.init(...)\n        #   for node:\n        #     submit _run_cmd(cmd) with resource {node_i: 1}\n        log_dir = os.path.join(remote_log_dir, 'tasks')\n        resources_dict = backend_utils.get_task_demands_dict(task)\n        internal_ips = handle.internal_ips()\n        assert internal_ips is not None, 'internal_ips is not cached in handle'\n\n        # If TPU VM Pods is used, #num_nodes should be num_nodes * num_node_ips\n        num_actual_nodes = task.num_nodes * handle.num_ips_per_node\n        task_env_vars = self._get_task_env_vars(task, job_id, handle)\n\n        codegen = self._get_task_codegen_class(handle)\n\n        codegen.add_prologue(job_id)\n        codegen.add_setup(\n            num_actual_nodes,\n            resources_dict,\n            stable_cluster_internal_ips=internal_ips,\n            env_vars=task_env_vars,\n            log_dir=log_dir,\n            setup_cmd=self._setup_cmd,\n        )\n\n        codegen.add_task(\n            num_actual_nodes,\n            bash_script=task.run,\n            env_vars=task_env_vars,\n            task_name=task.name,\n            resources_dict=backend_utils.get_task_demands_dict(task),\n            log_dir=log_dir)\n\n        codegen.add_epilogue()\n        # TODO(zhanghao): Add help info for downloading logs.\n        self._exec_code_on_head(\n            handle,\n            codegen.build(),\n            job_id,\n            managed_job_dag=task.managed_job_dag,\n            managed_job_user_id=self._get_managed_job_user_id(task),\n            remote_log_dir=remote_log_dir)\n"
        }
    ]
}